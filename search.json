[
  {
    "objectID": "home_assignments.html",
    "href": "home_assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Below is a list of all assignments for this course. Click on the links to access the .qmd files for each assignment. Ensure that you have the necessary software (e.g., Quarto, R, RStudio) to render and complete the assignments.\n\n\n\nAssignment 1: Introduction to Experimental Design\nDue Date: February 6, 2025\nAssignment 2: Completely Randomized Design (CRD)\nDue Date: February 13, 2025\nAssignment 3: Multiple Comparisons and Contrasts\nDue Date: February 20, 2025\nAssignment 4: Checking Model Assumptions\nDue Date: February 27, 2025\nAssignment 5: Two-Factor ANOVA\nDue Date: March 6, 2025\nAssignment 6: Higher-Order Factorial Designs\nDue Date: March 25, 2025\nAssignment 7: Analysis of Covariance (ANCOVA)\nDue Date: April 3, 2025\nAssignment 8: Randomized Complete Block Design (RCBD)\nDue Date: April 10, 2025\nAssignment 9: Complete Block Designs\nDue Date: April 17, 2025\nAssignment 10: Row-Column (Latin Square) Designs\nDue Date: April 24, 2025\nAssignment 11: Random and Mixed Effects Models\nDue Date: May 1, 2025\nAssignment 12: Nested Models\nDue Date: May 6, 2025\nAssignment 13: Split-Plot Designs\nDue Date: May 13, 2025\n\n\n\n\n\n\nDownload the .qmd File: Access the assignment file by clicking the links above.\nOpen in Quarto IDE: Use your preferred Quarto-supported IDE (e.g., RStudio, VS Code).\nComplete the Assignment: Follow the instructions provided in each .qmd file.\nRender to PDF: Render the .qmd file to PDF format.\nSubmit on Canvas: Upload the rendered PDF on Canvas under the corresponding assignment section.\n\n\n\n\n\nRefer to the Course GitHub Repository for additional resources and examples.\nPost questions on the Discussion Forum on Canvas.\nSchedule a meeting with the instructor for further clarification."
  },
  {
    "objectID": "home_assignments.html#assignment-list",
    "href": "home_assignments.html#assignment-list",
    "title": "Assignments",
    "section": "",
    "text": "Assignment 1: Introduction to Experimental Design\nDue Date: February 6, 2025\nAssignment 2: Completely Randomized Design (CRD)\nDue Date: February 13, 2025\nAssignment 3: Multiple Comparisons and Contrasts\nDue Date: February 20, 2025\nAssignment 4: Checking Model Assumptions\nDue Date: February 27, 2025\nAssignment 5: Two-Factor ANOVA\nDue Date: March 6, 2025\nAssignment 6: Higher-Order Factorial Designs\nDue Date: March 25, 2025\nAssignment 7: Analysis of Covariance (ANCOVA)\nDue Date: April 3, 2025\nAssignment 8: Randomized Complete Block Design (RCBD)\nDue Date: April 10, 2025\nAssignment 9: Complete Block Designs\nDue Date: April 17, 2025\nAssignment 10: Row-Column (Latin Square) Designs\nDue Date: April 24, 2025\nAssignment 11: Random and Mixed Effects Models\nDue Date: May 1, 2025\nAssignment 12: Nested Models\nDue Date: May 6, 2025\nAssignment 13: Split-Plot Designs\nDue Date: May 13, 2025"
  },
  {
    "objectID": "home_assignments.html#instructions",
    "href": "home_assignments.html#instructions",
    "title": "Assignments",
    "section": "",
    "text": "Download the .qmd File: Access the assignment file by clicking the links above.\nOpen in Quarto IDE: Use your preferred Quarto-supported IDE (e.g., RStudio, VS Code).\nComplete the Assignment: Follow the instructions provided in each .qmd file.\nRender to PDF: Render the .qmd file to PDF format.\nSubmit on Canvas: Upload the rendered PDF on Canvas under the corresponding assignment section."
  },
  {
    "objectID": "home_assignments.html#need-help",
    "href": "home_assignments.html#need-help",
    "title": "Assignments",
    "section": "",
    "text": "Refer to the Course GitHub Repository for additional resources and examples.\nPost questions on the Discussion Forum on Canvas.\nSchedule a meeting with the instructor for further clarification."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html",
    "href": "lectures/week-04_model-diagnostics.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "When using ANOVA or regression models, the reliability and validity of inferences depend on certain assumptions:\n\nIndependence of Errors: Observations should not be correlated.\nConstant Variance (Homoscedasticity): The variability of errors should remain constant across treatments or predicted values.\nNormality of Errors: Residuals should follow a normal distribution.\n\nIf these assumptions fail, confidence intervals and hypothesis tests may yield misleading conclusions. This lecture focuses on systematic ways to verify assumptions, diagnose problems, and apply corrective measures. By conducting proper diagnostic checks, we can maintain rigor and integrity in our statistical analyses.\n\n\n\nUnderstand the importance of verifying model assumptions in ANOVA and regression.\nLearn diagnostic tools (residual plots, normal probability plots) to detect assumption violations.\nExplore transformations and alternative modeling strategies to address violations.\nDevelop proficiency in R for performing diagnostics and interpreting results.\nChallenge understanding through exercises involving medium to complex scenarios and proofs."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#objectives",
    "href": "lectures/week-04_model-diagnostics.html#objectives",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Understand the importance of verifying model assumptions in ANOVA and regression.\nLearn diagnostic tools (residual plots, normal probability plots) to detect assumption violations.\nExplore transformations and alternative modeling strategies to address violations.\nDevelop proficiency in R for performing diagnostics and interpreting results.\nChallenge understanding through exercises involving medium to complex scenarios and proofs."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-independence",
    "href": "lectures/week-04_model-diagnostics.html#checking-independence",
    "title": "Model Diagnostics",
    "section": "Checking Independence",
    "text": "Checking Independence\nContext: Consider a balloon inflation experiment where each time a subject inflates a balloon, they learn and get faster. Residuals vs. observation order might show a trend, indicating non-independence (learning effect).\nR Implementation:\n# Suppose 'Time' is order of observation\nplot(resid(model) ~ Time, data = dataset)\nabline(h=0, col=\"red\")\n\n# Look for random scatter. Patterns suggest non-independence.\nIf non-independence arises, consider mixed-effects models or time-series methods."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-equal-variance-homoscedasticity",
    "href": "lectures/week-04_model-diagnostics.html#checking-equal-variance-homoscedasticity",
    "title": "Model Diagnostics",
    "section": "Checking Equal Variance (Homoscedasticity)",
    "text": "Checking Equal Variance (Homoscedasticity)\nVisual Clue: A “megaphone” shape in residuals vs. fitted values suggests variance grows with the mean. Rule of Thumb: If ratio of largest to smallest sample variance &gt; 3, consider a transformation.\nR Implementation:\nplot(fitted(model), resid(model))\nabline(h=0, col=\"red\")\nIf variance is not constant, transformations (like log or square-root) can stabilize it. The Box-Cox method in R helps find a suitable lambda for transforming responses."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#checking-normality",
    "href": "lectures/week-04_model-diagnostics.html#checking-normality",
    "title": "Model Diagnostics",
    "section": "Checking Normality",
    "text": "Checking Normality\nQQ-Plot: If residuals follow a normal distribution, they align closely with the QQ-line.\nR Implementation:\nqqnorm(resid(model))\nqqline(resid(model), col=\"blue\")\nIf deviations are severe, consider transformations or nonparametric methods."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#transformations",
    "href": "lectures/week-04_model-diagnostics.html#transformations",
    "title": "Model Diagnostics",
    "section": "Transformations",
    "text": "Transformations\n\nLog Transform: Useful if variance increases roughly with the mean.\nSquare-root Transform: Often stabilizes variance when data are counts or positive and moderately skewed.\nBox-Cox Transform: Systematically searches for a suitable power transform:\n\nlibrary(MASS)\nboxcox(lm(Y ~ X, data=dataset), lambda=seq(-2,2,0.1))"
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#alternative-models",
    "href": "lectures/week-04_model-diagnostics.html#alternative-models",
    "title": "Model Diagnostics",
    "section": "Alternative Models",
    "text": "Alternative Models\n\nMixed-Effects Models: Address correlation among observations.\nNonparametric Methods: For severe non-normality or other intractable issues, methods not reliant on normality assumptions (e.g., Kruskal-Wallis test for ANOVA replacement) may be suitable."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#medium-difficulty",
    "href": "lectures/week-04_model-diagnostics.html#medium-difficulty",
    "title": "Model Diagnostics",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nResidual Diagnostics (Basic): Simulate a dataset with four treatments. Perform an ANOVA and create residual vs. fitted plots and QQ-plots. Interpret patterns and identify if any assumptions fail.\nApplying a Transformation: Given a dataset where variance increases with the mean, apply a log transform. Show before/after residual diagnostics and discuss improvements."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#challenging-problems",
    "href": "lectures/week-04_model-diagnostics.html#challenging-problems",
    "title": "Model Diagnostics",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nBox-Cox Method Analysis: Use the boxcox function on a real or complex dataset. Determine the best lambda and re-check assumptions. Provide a detailed interpretation of why this lambda helps and what it implies theoretically.\nNon-Constant Variance Remedies: Suppose you have a dataset with strong heteroscedasticity. Try square-root, log, and Box-Cox transforms. Compare their effectiveness using diagnostic plots and normality tests. Write a short report on which transform works best and why.\nNon-Independent Errors: Generate a dataset with serial correlation (e.g., an AR(1) process). Fit a standard ANOVA model, show that assumptions are violated, and discuss how a mixed model or generalized least squares approach could rectify the issue. Provide code snippets and interpret results."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#references",
    "href": "lectures/week-04_model-diagnostics.html#references",
    "title": "Model Diagnostics",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#derivation-of-standardized-residuals",
    "href": "lectures/week-04_model-diagnostics.html#derivation-of-standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Derivation of Standardized Residuals",
    "text": "Derivation of Standardized Residuals\nGiven a model \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) with homoscedastic errors \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\):\n\nResidual: \\(e_{ij} = Y_{ij} - \\hat{Y}_{ij}\\).\nMSE: \\(\\hat{\\sigma}^2 = \\text{SSE}/(N-v)\\).\nStandardized Residual: \\(z_{ij} = e_{ij}/\\sqrt{\\hat{\\sigma}^2}\\).\n\nUnder ideal conditions, \\(z_{ij} \\sim N(0,1)\\) approximately, allowing outlier detection via thresholding (e.g., |z| &gt; 3)."
  },
  {
    "objectID": "lectures/week-04_model-diagnostics.html#box-cox-transformation",
    "href": "lectures/week-04_model-diagnostics.html#box-cox-transformation",
    "title": "Model Diagnostics",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nThe Box-Cox family:\n\\[\nY'(\\lambda) =\n\\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda} & \\lambda \\neq 0 \\\\\n\\ln(Y) & \\lambda = 0\n\\end{cases}\n\\]\nFor each \\(\\lambda\\), fit the model and compute the log-likelihood. Choose \\(\\lambda\\) that maximizes the likelihood, indicating a best variance-stabilizing and normalizing transform."
  },
  {
    "objectID": "lectures/week-14_split-plot.html",
    "href": "lectures/week-14_split-plot.html",
    "title": "Split-Plot ANOVA",
    "section": "",
    "text": "Split-plot designs are a powerful class of experimental designs that originated in agricultural settings but now see extensive use across engineering and scientific fields where certain factors are hard to change and others are easy to change. First formalized by R. A. Fisher at Rothamsted Experimental Station, these designs address practical limitations in experimentation—for instance, when adjusting equipment temperature is costly or time-consuming (Kempthorne, 1977; Scheffé, 1959).\n\n\n\nEarly Development: Split-plot methodology emerged from agricultural research, where entire fields (whole plots) were assigned to certain treatments (e.g., irrigation methods), and sub-portions of each field (split plots) received additional treatments (e.g., fertilizer types).\nModern Applications: Today, split-plot designs are prevalent in industrial experiments (Dean et al., 2017), such as semiconductor manufacturing or paint-curing processes. They enable investigators to manipulate “hard-to-change” variables at the whole-plot level while fine-tuning other variables at the split-plot level.\n\n\n\n\nAfter working through this material, you will be able to:\n\nRecognize when split-plot designs are appropriate.\nFormulate the statistical models underpinning split-plot experiments.\nAnalyze split-plot data using R and interpret the output.\nUnderstand the hierarchical (two-error) structure inherent in split-plot designs.\nConduct diagnostic checks and manage potential pitfalls such as unbalanced data or pseudo-replication."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#historical-context",
    "href": "lectures/week-14_split-plot.html#historical-context",
    "title": "Split-Plot ANOVA",
    "section": "",
    "text": "Early Development: Split-plot methodology emerged from agricultural research, where entire fields (whole plots) were assigned to certain treatments (e.g., irrigation methods), and sub-portions of each field (split plots) received additional treatments (e.g., fertilizer types).\nModern Applications: Today, split-plot designs are prevalent in industrial experiments (Dean et al., 2017), such as semiconductor manufacturing or paint-curing processes. They enable investigators to manipulate “hard-to-change” variables at the whole-plot level while fine-tuning other variables at the split-plot level."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#learning-objectives",
    "href": "lectures/week-14_split-plot.html#learning-objectives",
    "title": "Split-Plot ANOVA",
    "section": "",
    "text": "After working through this material, you will be able to:\n\nRecognize when split-plot designs are appropriate.\nFormulate the statistical models underpinning split-plot experiments.\nAnalyze split-plot data using R and interpret the output.\nUnderstand the hierarchical (two-error) structure inherent in split-plot designs.\nConduct diagnostic checks and manage potential pitfalls such as unbalanced data or pseudo-replication."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#two-error-structure",
    "href": "lectures/week-14_split-plot.html#two-error-structure",
    "title": "Split-Plot ANOVA",
    "section": "2.1 Two-Error Structure",
    "text": "2.1 Two-Error Structure\nA defining feature of split-plot designs is the presence of two distinct experimental error terms (Dean et al., 2017; Montgomery & Runger, 2010; Christensen, 2018):\n\nWhole-Plot Error (\\(\\sigma^2_W\\))\n\nArises from variation among the “hard-to-change” factors (e.g., temperature, irrigation).\nTypically larger due to the cost and complexity of changing these factors.\nExample: Oven-to-oven variability when testing different oven temperatures.\n\nSplit-Plot Error (\\(\\sigma^2_S\\))\n\nCaptures variability within each whole plot for the “easy-to-change” factors.\nUsually smaller since these factors are more straightforward to manipulate.\nExample: Variation among parts painted in the same oven run.\n\n\nThis nested structure means that each whole plot is subdivided into several split plots, and different analysis methods are needed to properly separate the two sources of variability."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#intuitive-analogy-paint-curing-process",
    "href": "lectures/week-14_split-plot.html#intuitive-analogy-paint-curing-process",
    "title": "Split-Plot ANOVA",
    "section": "2.2 Intuitive Analogy: Paint Curing Process",
    "text": "2.2 Intuitive Analogy: Paint Curing Process\nConsider an industrial paint-curing process:\n\nWhole-Plot Factor: Oven Temperature (Low, Medium, High)\nSplit-Plot Factor: Paint Type (A, B, C)\nResponse: Coating Durability\n\nRunning the experiment:\n\nSet the oven to a specific temperature (hard to change).\nPaint several parts (easy to change) in that oven run.\nMeasure the durability of each part’s coating.\n\nBecause the oven temperature remains fixed for a batch of painted parts, any variation among parts at the same temperature is captured by \\(\\sigma^2_S\\) (split-plot error), whereas variation across different oven temperatures is captured by \\(\\sigma^2_W\\) (whole-plot error)."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#manufacturing-example",
    "href": "lectures/week-14_split-plot.html#manufacturing-example",
    "title": "Split-Plot ANOVA",
    "section": "6.1 Manufacturing Example",
    "text": "6.1 Manufacturing Example\nSuppose you have:\n\nWhole-Plot Factor: Furnace Temperature (\\(800^\\circ\\)C vs. \\(900^\\circ\\)C)\nSplit-Plot Factor: Gas Flow Rate (10, 20, 30 sccm)\nBlocks: 4 furnace runs\nResponse: Film Thickness Uniformity\n\n\nset.seed(123)\nsemi_data &lt;- data.frame(\n  run = rep(1:4, each = 6),\n  temp = rep(rep(c(\"800C\", \"900C\"), each = 3), 4),\n  flow = rep(c(\"10\", \"20\", \"30\"), times = 8),\n  uniformity = rnorm(24,\n    mean = rep(c(85, 90), each = 12) +\n      rep(c(0, 2, 4), times = 8),\n    sd = 2\n  )\n)\n\nsemi_model &lt;- lmer(uniformity ~ temp * flow + (1 | run) + (1 | run:temp),\n  data = semi_data\n)\n\nsummary(semi_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: uniformity ~ temp * flow + (1 | run) + (1 | run:temp)\n   Data: semi_data\n\nREML criterion at convergence: 92.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.61892 -0.60661  0.00919  0.54843  1.50653 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n run:temp (Intercept) 0.000    0.000   \n run      (Intercept) 5.452    2.335   \n Residual             4.459    2.112   \nNumber of obs: 24, groups:  run:temp, 8; run, 4\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)     88.00128    1.57408  55.906\ntemp900C         0.09561    1.49316   0.064\nflow20           0.57004    1.49316   0.382\nflow30           3.12281    1.49316   2.091\ntemp900C:flow20  1.24567    2.11164   0.590\ntemp900C:flow30 -0.03002    2.11164  -0.014\n\nCorrelation of Fixed Effects:\n            (Intr) tm900C flow20 flow30 t900C:2\ntemp900C    -0.474                             \nflow20      -0.474  0.500                      \nflow30      -0.474  0.500  0.500               \ntmp900C:f20  0.335 -0.707 -0.707 -0.354        \ntmp900C:f30  0.335 -0.707 -0.354 -0.707  0.500 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nanova(semi_model)\n\n\n  \n\n\n\n\nPlotting:\n\n\nlibrary(ggplot2)\nggplot(semi_data, aes(x = flow, y = uniformity, color = temp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~run) +\n  theme_bw() +\n  labs(\n    title = \"Film Uniformity vs. Flow Rate by Temperature\",\n    x = \"Flow Rate (sccm)\",\n    y = \"Uniformity (%)\"\n  )"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#agricultural-example",
    "href": "lectures/week-14_split-plot.html#agricultural-example",
    "title": "Split-Plot ANOVA",
    "section": "6.2 Agricultural Example",
    "text": "6.2 Agricultural Example\n\nWhole-Plot Factor: Irrigation Method (Drip, Sprinkler)\nSplit-Plot Factor: Fertilizer (None, Organic, Chemical)\nBlocks: 6 fields\nResponse: Crop Yield\n\n\nset.seed(456)\nag_data &lt;- data.frame(\n  field = rep(1:6, each = 6),\n  irrigation = rep(rep(c(\"Drip\", \"Sprinkler\"), each = 3), 6),\n  fertilizer = rep(c(\"None\", \"Organic\", \"Chemical\"), times = 12),\n  yield = rnorm(36,\n    mean = rep(c(40, 45), each = 18) +\n      rep(c(0, 5, 8), times = 12),\n    sd = 3\n  )\n)\n\nag_model &lt;- lmer(\n  yield ~ irrigation * fertilizer +\n    (1 | field) + (1 | field:irrigation),\n  data = ag_data\n)\n\nanova(ag_model)\n\n\n  \n\n\nemmeans(ag_model, ~ irrigation | fertilizer)\n\nfertilizer = Chemical:\n irrigation emmean  SE df lower.CL upper.CL\n Drip         49.2 1.7 21     45.7     52.8\n Sprinkler    51.7 1.7 21     48.1     55.2\n\nfertilizer = None:\n irrigation emmean  SE df lower.CL upper.CL\n Drip         43.5 1.7 21     40.0     47.1\n Sprinkler    42.2 1.7 21     38.7     45.7\n\nfertilizer = Organic:\n irrigation emmean  SE df lower.CL upper.CL\n Drip         49.8 1.7 21     46.3     53.3\n Sprinkler    47.7 1.7 21     44.1     51.2\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#power-analysis",
    "href": "lectures/week-14_split-plot.html#power-analysis",
    "title": "Split-Plot ANOVA",
    "section": "7.1 Power Analysis",
    "text": "7.1 Power Analysis\nIn split-plot designs, we have distinct power calculations for the whole-plot factor and the split-plot factor (Dean et al., 2017). Each factor is tested against a different error term.\n\\[\n\\text{Power}^*_{\\text{whole-plot}} \\quad=\\quad P\\bigl(F^*_{a-1,(b-1)(a-1)} &gt; F_{\\alpha;a-1,(b-1)(a-1)} \\;\\bigm|\\; \\delta\\bigr),\n\\]\n\\[\n\\text{Power}^*_{\\text{split-plot}} \\quad=\\quad P\\bigl(F^*_{s-1,a(b-1)(s-1)} &gt; F_{\\alpha;s-1,a(b-1)(s-1)} \\;\\bigm|\\; \\delta\\bigr).\n\\]\n\n\\(\\delta\\) represents the noncentrality parameter linked to the effect size.\nSoftware such as pwr, simr, or custom code can be used to approximate power."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#optimal-design-considerations",
    "href": "lectures/week-14_split-plot.html#optimal-design-considerations",
    "title": "Split-Plot ANOVA",
    "section": "7.2 Optimal Design Considerations",
    "text": "7.2 Optimal Design Considerations\nFor large-scale experiments, cost constraints and logistical feasibility often dictate the chosen design structure (Kempthorne, 1977; Dean et al., 2017). Key elements include:\n\nBalance: Achieving uniform replication across factor levels.\nResource Allocation: The ratio of whole-plot replicates to split-plot replicates.\nVariance Component Estimates: Anticipating or pilot-testing \\(\\sigma_W^2\\) and \\(\\sigma_S^2\\) to inform design decisions."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#pseudo-replication",
    "href": "lectures/week-14_split-plot.html#pseudo-replication",
    "title": "Split-Plot ANOVA",
    "section": "8.1 Pseudo-Replication",
    "text": "8.1 Pseudo-Replication\nA critical error is to treat measurements within a single whole plot as fully independent replicates for the whole-plot factor. This leads to inflated Type I error rates and overly optimistic standard errors.\nIncorrect:\n# Failing to account for hierarchical structure\nlm_model &lt;- lm(yield ~ temp * flow, data = semi_data)\nCorrect:\n# Using a mixed-effects model to capture nested variability\nlmer_model &lt;- lmer(uniformity ~ temp * flow + (1|run) + (1|run:temp),\n                   data = semi_data)"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#unbalanced-data",
    "href": "lectures/week-14_split-plot.html#unbalanced-data",
    "title": "Split-Plot ANOVA",
    "section": "8.2 Unbalanced Data",
    "text": "8.2 Unbalanced Data\nReal-world settings often produce unbalanced data (missing cells, unequal replicates):\n\nUse lmerTest for Satterthwaite or Kenward-Roger df approximations.\nExamine residuals and leverage robust inference methods if necessary.\n\n\nlibrary(lmerTest)\nunbalanced_data &lt;- subset(semi_data, !(run == 4 & flow == \"10\")) # remove some observations\nmodel_unbalanced &lt;- lmer(\n  uniformity ~ temp * flow +\n    (1 | run) + (1 | run:temp),\n  data = unbalanced_data\n)\nanova(model_unbalanced, ddf = \"Satterthwaite\")"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#medium-difficulty-problems",
    "href": "lectures/week-14_split-plot.html#medium-difficulty-problems",
    "title": "Split-Plot ANOVA",
    "section": "9.1 Medium-Difficulty Problems",
    "text": "9.1 Medium-Difficulty Problems\n\nSplit-Plot Degrees of Freedom\n\nA two-factor split-plot experiment has 3 whole-plot factor levels, 4 split-plot factor levels, and 3 blocks.\n\nCalculate the df for the whole-plot factor.\nCalculate the df for the split-plot error.\nExplain why whole-plot and split-plot errors differ.\n\n\nModel Formulation\n\nWrite down the full split-plot model with two factors (A, B) and one blocking factor (r blocks). Identify which terms belong to the whole-plot portion vs. the split-plot portion.\n\nInterpreting lmer Output\n\nFit a split-plot model using the ag_data dataset.\nReport which random effects are estimated and how they impact the F-tests for the main factors."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#challenging-problems",
    "href": "lectures/week-14_split-plot.html#challenging-problems",
    "title": "Split-Plot ANOVA",
    "section": "9.2 Challenging Problems",
    "text": "9.2 Challenging Problems\n\nDeriving Expected Mean Squares\n\nTask: Show that \\(E(MS_{whole}) = \\sigma^2_S + s\\sigma^2_W\\).\nHint: Partition the sum of squares for the whole plot and use properties of the normal distribution.\nReference: See derivations in Scheffé (1959) and Appendix A below.\n\nPower Calculation\n\nSuppose \\(\\sigma_W^2 = 15\\) and \\(\\sigma_S^2 = 5\\). You have 3 levels of a whole-plot factor (A) and 4 levels of a split-plot factor (B), with 3 replicates (blocks).\nTask: Outline how you would simulate data in R to estimate the power to detect a particular effect size \\(\\delta\\) on factor A.\n\nDesign Optimization\n\nYou have a cost function \\(C = c_w n_w + c_s n_s\\), and you want to keep the variance of \\(\\hat{\\tau}*i - \\hat{\\tau}*{i'}\\) below a threshold \\(V_0\\). Show how to derive the ratio \\(n_w^* / n_s^*\\) that minimizes cost subject to the variance constraint (Kempthorne, 1977)."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#a.1-expected-mean-squares",
    "href": "lectures/week-14_split-plot.html#a.1-expected-mean-squares",
    "title": "Split-Plot ANOVA",
    "section": "A.1 Expected Mean Squares",
    "text": "A.1 Expected Mean Squares\nUsing the notation in Dean et al. (2017) and Scheffé (1959):\n\\[\n\\text{For a balanced split-plot design:}\n\\]\n\nWhole-plot error:\n\n\\[\nE(MS_{\\text{whole}}) = \\sigma^2_S + s\\,\\sigma^2_W.\n\\]\n\nSplit-plot error:\n\n\\[\nE(MS_{\\text{split}}) = \\sigma^2_S.\n\\]\nThe derivation partitions sums of squares and leverages properties of normal distributions (see Christensen, 2018, for a modern ANOVA treatment)."
  },
  {
    "objectID": "lectures/week-14_split-plot.html#a.2-power-analysis-for-whole-plot-factor",
    "href": "lectures/week-14_split-plot.html#a.2-power-analysis-for-whole-plot-factor",
    "title": "Split-Plot ANOVA",
    "section": "A.2 Power Analysis for Whole-Plot Factor",
    "text": "A.2 Power Analysis for Whole-Plot Factor\n\\[\n\\lambda \\;=\\; \\frac{n_w \\sum_{i=1}^a \\tau_i^2}{a\\,\\sigma_W^2},\n\\]\nwhere \\(n_w\\) is the number of whole-plot replicates, \\(\\tau_i\\) are the treatment contrasts, and \\(\\sigma_W^2\\) is the whole-plot variance. The power to detect a nonzero \\(\\tau_i\\) at significance level \\(\\alpha\\) becomes:\n\\[\n1 - \\beta \\;=\\; P\\!\\bigl(F_{a-1,(b-1)(a-1)} &gt; F_{\\alpha;a-1,(b-1)(a-1)} \\;\\bigm|\\; \\lambda\\bigr).\n\\]"
  },
  {
    "objectID": "lectures/week-14_split-plot.html#a.3-optimal-design-theory",
    "href": "lectures/week-14_split-plot.html#a.3-optimal-design-theory",
    "title": "Split-Plot ANOVA",
    "section": "A.3 Optimal Design Theory",
    "text": "A.3 Optimal Design Theory\nFollowing Kempthorne (1977):\n\\[\n\\text{Minimize } C = c_w n_w + c_s n_s \\quad \\text{subject to }\nVar(\\hat{\\tau}^*_i - \\hat{\\tau}^*_{i'}) \\leq V_0.\n\\]\nIf \\(\\sigma_W^2\\) and \\(\\sigma_S^2\\) are known (or estimated from pilot data):\n\\[\nn_w^* = \\sqrt{\\frac{c_s}{c_w}} \\;\\frac{\\sigma_W}{\\sigma_S}\\;n_s^*.\n\\]\nThis balance ensures cost efficiency while meeting variance constraints."
  },
  {
    "objectID": "lectures/week-10_ancova.html",
    "href": "lectures/week-10_ancova.html",
    "title": "ANCOVA",
    "section": "",
    "text": "In statistical analysis, Analysis of Covariance (ANCOVA) serves as a powerful tool that combines the principles of ANOVA (Analysis of Variance) and regression. ANCOVA allows researchers to adjust the response variable for one or more covariates (also known as nuisance variables) that are related to the response but are not influenced by the treatments. This adjustment enhances the precision of treatment effect estimates and controls for potential confounding factors.\nIntuition: Imagine you are a teacher assessing the effectiveness of different teaching methods (treatments) on students’ final exam scores (response). However, students come with varying levels of prior knowledge (covariate). By using ANCOVA, you can adjust the final scores based on their initial knowledge, providing a clearer picture of how each teaching method truly affects performance.\nThis lecture delves into the theoretical foundations, assumptions, and applications of ANCOVA. We will explore practical implementations using R, ensuring a balance between theoretical depth and applied understanding. References to classical texts will support and deepen the content, providing a rigorous academic framework.\n\n\n\nUnderstand the purpose and application of ANCOVA.\nExplore the mathematical model and assumptions underlying ANCOVA.\nLearn methods to test and interpret treatment effects adjusted for covariates.\nUse R to perform ANCOVA and evaluate its assumptions.\nExtend the model to unbalanced data and multiple covariates.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-10_ancova.html#objectives",
    "href": "lectures/week-10_ancova.html#objectives",
    "title": "ANCOVA",
    "section": "",
    "text": "Understand the purpose and application of ANCOVA.\nExplore the mathematical model and assumptions underlying ANCOVA.\nLearn methods to test and interpret treatment effects adjusted for covariates.\nUse R to perform ANCOVA and evaluate its assumptions.\nExtend the model to unbalanced data and multiple covariates.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-10_ancova.html#why-ancova",
    "href": "lectures/week-10_ancova.html#why-ancova",
    "title": "ANCOVA",
    "section": "Why ANCOVA?",
    "text": "Why ANCOVA?\nANCOVA addresses several key scenarios in experimental and observational studies:\n\nAdjusting for Covariates: It accounts for variability due to covariates, improving the precision of estimating treatment effects.\nControlling for Confounding Variables: By adjusting for variables related to the response but not influenced by treatments, ANCOVA enhances causal interpretations.\nIncreasing Statistical Power: Reducing error variance by accounting for covariates can lead to more powerful tests of treatment effects.\n\nExample: Consider comparing the effects of three different fertilizers (A, B, C) on crop yield. Soil quality before treatment is a covariate that affects yield but is not influenced by the fertilizer used. ANCOVA can adjust yield differences based on initial soil quality, providing a clearer assessment of fertilizer effectiveness.\nReference: Dean, Voss & Draguljić (2017) discuss the integration of covariates in experimental designs, highlighting the benefits of ANCOVA in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-10_ancova.html#the-ancova-model",
    "href": "lectures/week-10_ancova.html#the-ancova-model",
    "title": "ANCOVA",
    "section": "The ANCOVA Model",
    "text": "The ANCOVA Model\nFor \\(v\\) treatments and \\(n\\) total observations, the ANCOVA model is defined as:\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij},\n\\]\nwhere:\n\n\\(Y_{ij}\\): Response variable for the \\(j\\)-th unit under treatment \\(i\\).\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for the \\(i\\)-th treatment (\\(\\sum_{i} \\tau_i = 0\\)).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(X_{ij}\\): Covariate value for the \\(j\\)-th unit under treatment \\(i\\).\n\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\): Error term.\n\n\nCentered Form of the Model\nTo simplify computations, the covariate is often centered around its mean:\n\\[\nY_{ij} = \\mu^* + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij},\n\\]\nwhere \\(\\mu^* = \\mu - \\beta \\bar{X}\\).\nIntuition: Centering the covariate reduces multicollinearity between the intercept and the covariate, enhancing the interpretability of the model parameters.\nReference: Montgomery & Runger (2010) provide a detailed explanation of model centering and its benefits in Applied Statistics and Probability for Engineers."
  },
  {
    "objectID": "lectures/week-10_ancova.html#example-balloon-inflation-times",
    "href": "lectures/week-10_ancova.html#example-balloon-inflation-times",
    "title": "ANCOVA",
    "section": "Example: Balloon Inflation Times",
    "text": "Example: Balloon Inflation Times\n\nDataset Description\n\nResponse Variable: Inflation time of balloons (in seconds).\nCovariate: Run order (time of day).\nTreatments: Balloon colors (pink, yellow, orange, blue).\n\nObjective: Compare inflation times across different balloon colors while adjusting for the run order to account for time-related variability.\n\n\nStep-by-Step R Code\n\n1. Load Data and Prepare Covariate\n# Load necessary packages\nlibrary(car)\nlibrary(emmeans)\n\n# Read the dataset\nballoon.data &lt;- read.table(\"data/balloon.txt\", header = TRUE)\n\n# Prepare the data: center the covariate and convert treatment to factor\nballoon.data &lt;- within(balloon.data, { \n  x &lt;- RunOrder - mean(RunOrder)  # Center the covariate\n  fColor &lt;- factor(Color)         # Convert treatment to factor\n})\n\n\n2. Fit the ANCOVA Model\n# Fit the ANCOVA model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Summary of the model\nsummary(model)\nInterpretation: - Covariate Effect (\\(\\beta\\)): Indicates how inflation time changes with run order. - Treatment Effects (\\(\\tau_i\\)): Compare balloon colors after adjusting for run order.\n\n\n3. Test the Assumptions\n\nA. Homogeneity of Slopes\nTo verify if the assumption of equal slopes holds, include an interaction term between treatment and covariate.\n# Fit the model with interaction\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare models\nanova(model, model_interaction)\nInterpretation: A significant interaction term suggests that slopes are not equal across treatments, violating the homogeneity of regression slopes assumption.\n\n\n\n4. Post-Model Diagnostics\n# Plot diagnostic graphs\npar(mfrow = c(2, 2))\nplot(model)\nInterpretation: Examine residual plots for normality, homoscedasticity, and independence. Look for random scatter without patterns in residuals vs. fitted values and Q-Q plots.\n\n\n5. Multiple Comparisons\nUse the emmeans package to perform pairwise comparisons of adjusted means.\n# Estimated marginal means\nemm &lt;- emmeans(model, ~ fColor)\n\n# Pairwise comparisons\npairwise &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise)\nInterpretation: Identify which balloon colors have significantly different inflation times after adjusting for run order."
  },
  {
    "objectID": "lectures/week-10_ancova.html#extensions-to-unbalanced-data",
    "href": "lectures/week-10_ancova.html#extensions-to-unbalanced-data",
    "title": "ANCOVA",
    "section": "Extensions to Unbalanced Data",
    "text": "Extensions to Unbalanced Data\nWhen treatment groups have unequal sample sizes, ANCOVA can still be performed using Type II or Type III sums of squares to handle the imbalance.\n\nHandling Unbalanced Data in R\n# Load necessary package\nlibrary(car)\n\n# Fit ANCOVA model with unbalanced data\n# Assume 'data_unbalanced' is loaded with unequal replicates\nfit_unbalanced &lt;- lm(Response ~ FactorA * FactorB, data = data_unbalanced)\n\n# Type II Sum of Squares\nAnova(fit_unbalanced, type = \"II\")\n\n# Type III Sum of Squares\nAnova(fit_unbalanced, type = \"III\")\nConsiderations: - Type II SS: Suitable when interactions are not of primary interest. - Type III SS: Appropriate when interactions are present or main effects need to be tested conditionally.\nReference: Christensen (2018) discusses handling unbalanced designs and the choice of sums of squares types in Analysis of Variance, Design, and Regression."
  },
  {
    "objectID": "lectures/week-10_ancova.html#multiple-covariates",
    "href": "lectures/week-10_ancova.html#multiple-covariates",
    "title": "ANCOVA",
    "section": "Multiple Covariates",
    "text": "Multiple Covariates\nANCOVA can be extended to include multiple covariates, enhancing the model’s ability to adjust for various sources of variability.\n\nModel with Multiple Covariates\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_1 X_{1ij} + \\beta_2 X_{2ij} + \\ldots + \\epsilon_{ij}\n\\]\nR Implementation:\n# Fit ANCOVA model with multiple covariates\nmodel_multi_cov &lt;- lm(Response ~ FactorA + FactorB + Covariate1 + Covariate2, data = data)\nsummary(model_multi_cov)\nInterpretation: Assess the effect of each covariate and treatment while controlling for other covariates."
  },
  {
    "objectID": "lectures/week-10_ancova.html#nonlinear-relationships",
    "href": "lectures/week-10_ancova.html#nonlinear-relationships",
    "title": "ANCOVA",
    "section": "Nonlinear Relationships",
    "text": "Nonlinear Relationships\nIf the relationship between the response and covariate is not linear, polynomial terms can be included in the model.\n\nModel with Polynomial Terms\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_1 X_{ij} + \\beta_2 X_{ij}^2 + \\epsilon_{ij}\n\\]\nR Implementation:\n# Fit ANCOVA model with quadratic term\nmodel_poly &lt;- lm(Response ~ FactorA + FactorB + x + I(x^2), data = data)\nsummary(model_poly)\nInterpretation: Determine if adding a quadratic term improves model fit and captures the nonlinear relationship.\nReference: Montgomery & Runger (2010) explain handling nonlinear relationships in regression models in Applied Statistics and Probability for Engineers."
  },
  {
    "objectID": "lectures/week-10_ancova.html#medium-difficulty",
    "href": "lectures/week-10_ancova.html#medium-difficulty",
    "title": "ANCOVA",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction:\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate X.\nFit an ANCOVA model adjusting for X.\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\n\nR Code Example:\n\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + \n    ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n    0.5 * (X - mean(X)) + \n    rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\nQuestions: - What are the main effects of Treatment and the covariate X? - Are there significant differences between treatments after adjusting for X? - Do the diagnostic plots suggest any assumption violations?\n\nBalloon Data Analysis:\n\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\n\nR Code Example:\n\nlibrary(car)\nlibrary(emmeans)\n\n# Load and prepare data\nballoon.data &lt;- read.table(\"data/balloon.txt\", header = TRUE)\nballoon.data &lt;- within(balloon.data, { \n x &lt;- RunOrder - mean(RunOrder)\n fColor &lt;- factor(Color)\n})\n\n# Fit ANCOVA model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n# Test homogeneity of slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n# Estimated marginal means\nemm_balloon &lt;- emmeans(model, ~ fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\nQuestions: - Does the interaction term indicate equality of slopes across treatments? - How do the adjusted means compare across balloon colors? - What conclusions can be drawn about the effect of balloon color on inflation time after adjusting for run order?"
  },
  {
    "objectID": "lectures/week-10_ancova.html#challenging-problems",
    "href": "lectures/week-10_ancova.html#challenging-problems",
    "title": "ANCOVA",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor ANCOVA Analysis:\n\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\n\nR Code Example:\n\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + \n    ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n    ifelse(FactorB == \"B1\", 0, 3) + \n    0.5 * (X - mean(X)) + \n    rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points: - How do Type II and Type III sums of squares handle the unbalanced design differently? - Which factors or interactions are significant under each type? - What are the implications for interpreting main effects and interactions in unbalanced designs?\n\nThree-Factor Interaction Interpretation:\n\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\n\nR Code Example:\n\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + \n    ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n    ifelse(FactorB == \"B1\", 0, 3) + \n    ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n    0.5 * (X - mean(X)) + \n    ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n           ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \n    rnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n geom_point() +\n geom_line() +\n facet_wrap(~ FactorC) +\n labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\nInterpretation: - Analyze how the interaction between Factor A and Factor B changes across different levels of Factor C. - Determine specific treatment combinations that significantly differ. - Discuss the impact of the three-way interaction on the interpretation of main and two-way interactions, emphasizing that main effects may not be interpretable in the presence of significant higher-order interactions.\n\nMathematical Proof of Interaction Contrasts:\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy linear algebra properties to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nOutline:\n\nDefine the Three-Factor Model:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)*{ij} + (\\alpha\\gamma)*{ik} + (\\beta\\gamma)*{jk} + (\\alpha\\beta\\gamma)*{ijk} + \\epsilon_{ijk}\n\\]\n  where $\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0$.\n\nConstruct Interaction Contrasts:\n\nDefine contrasts for two-way and three-way interactions.\nExpress interaction contrasts in terms of group means.\n\nProve Deviations from Additivity:\n\nShow that if all three-way interaction contrasts \\((\\alpha\\beta\\gamma)_{ijk} = 0\\), the model reduces to an additive model.\nDemonstrate that non-zero three-way interactions indicate deviations from additivity.\n\nConclusion:\n\nInteraction contrasts quantify how the observed responses deviate from what would be expected under an additive model.\nNon-zero contrasts signify the presence of higher-order interactions affecting the response variable.\n\n\nReference: Dean, Voss & Draguljić (2017) provide comprehensive mathematical derivations and proofs in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-10_ancova.html#medium-difficulty-1",
    "href": "lectures/week-10_ancova.html#medium-difficulty-1",
    "title": "ANCOVA",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction:\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate X.\nFit an ANCOVA model adjusting for X.\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\n\nR Code Example:\n\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + \n    ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n    0.5 * (X - mean(X)) + \n    rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\nQuestions: - What are the main effects of Treatment and the covariate X? - Are there significant differences between treatments after adjusting for X? - Do the diagnostic plots suggest any assumption violations?\n\nBalloon Data Analysis:\n\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\n\nR Code Example:\n\nlibrary(car)\nlibrary(emmeans)\n\n# Load and prepare data\nballoon.data &lt;- read.table(\"data/balloon.txt\", header = TRUE)\nballoon.data &lt;- within(balloon.data, { \n x &lt;- RunOrder - mean(RunOrder)\n fColor &lt;- factor(Color)\n})\n\n# Fit ANCOVA model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n# Test homogeneity of slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n# Estimated marginal means\nemm_balloon &lt;- emmeans(model, ~ fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\nQuestions: - Does the interaction term indicate equality of slopes across treatments? - How do the adjusted means compare across balloon colors? - What conclusions can be drawn about the effect of balloon color on inflation time after adjusting for run order?"
  },
  {
    "objectID": "lectures/week-10_ancova.html#challenging-problems-1",
    "href": "lectures/week-10_ancova.html#challenging-problems-1",
    "title": "ANCOVA",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor ANCOVA Analysis:\n\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\n\nR Code Example:\n\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + \n    ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n    ifelse(FactorB == \"B1\", 0, 3) + \n    0.5 * (X - mean(X)) + \n    rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points: - Type II vs. Type III SS: - Type II: Tests main effects after accounting for other main effects but ignores interactions. - Type III: Tests main effects after accounting for all other factors, including interactions. - Impact of Imbalance: - Imbalanced designs can lead to different interpretations under Type II and Type III SS. - Type I SS is not appropriate here as it is sequential and depends on the order of factors. - Significance of Effects: - Identify which factors and interactions are significant under each type. - Discuss how imbalance might inflate or deflate the significance levels of certain effects. - Implications for Experimental Conclusions: - Choosing the appropriate type of sums of squares is crucial for accurate inference. - Misinterpretation can lead to incorrect conclusions about factor effects and interactions.\nReference: Christensen (2018) provides guidance on handling unbalanced designs and selecting appropriate sums of squares types.\n\nThree-Factor Interaction Interpretation:\n\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\n\nR Code Example:\n\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + \n    ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n    ifelse(FactorB == \"B1\", 0, 3) + \n    ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n    0.5 * (X - mean(X)) + \n    ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n           ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \n    rnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n geom_point() +\n geom_line() +\n facet_wrap(~ FactorC) +\n labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\nInterpretation: - Three-Way Interaction: The interaction plot shows how the interaction between Factor A and Factor B varies across levels of Factor C. Non-parallel lines across different facets (Factor C levels) indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions: Significant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation. The effect of one factor depends on the combination of other factors.\nPractical Implications: Understanding three-way interactions allows for more nuanced insights into how multiple factors jointly influence the response variable, leading to better-informed decisions and strategies.\n\nReference: Dean, Voss & Draguljić (2017) elaborate on interpreting higher-order interactions in Design and Analysis of Experiments.\n\nMathematical Proof of Interaction Contrasts:\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nProof Outline:\n\nDefine the Three-Factor Model:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)*{ij} + (\\alpha\\gamma)*{ik} + (\\beta\\gamma)*{jk} + (\\alpha\\beta\\gamma)*{ijk} + \\epsilon_{ijk}\n\\]\n  where $\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0$.\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)*{ik}\\) and \\((\\beta\\gamma)*{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProof of Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)*{ij} = (\\alpha\\gamma)*{ik} = (\\beta\\gamma)*{jk} = (\\alpha\\beta\\gamma)*{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n    indicating that the response is purely additive with respect to the main effects.\n\n\n  - **Zero Interaction Contrasts:**  \n    If all interaction contrasts are zero, the model exhibits additivity. Thus, interaction contrasts quantify how much the actual model deviates from an additive one.\n\nConclusion:\nInteraction contrasts provide a mathematical measure of the presence and magnitude of interactions. Non-zero contrasts indicate that the combined effect of factors is not merely the sum of their individual effects, highlighting the importance of considering interactions in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017) offer detailed mathematical treatments of interaction contrasts in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-10_ancova.html#adjusted-treatment-means",
    "href": "lectures/week-10_ancova.html#adjusted-treatment-means",
    "title": "ANCOVA",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe adjusted mean for treatment \\(i\\) is given by:\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X}),\n\\]\nwhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate.\n\nDerivation:\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery (2019) and Scheffé (1959) provide detailed derivations of adjusted means in ANCOVA."
  },
  {
    "objectID": "lectures/week-10_ancova.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-10_ancova.html#f-test-for-homogeneity-of-slopes",
    "title": "ANCOVA",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTo test the assumption of homogeneity of regression slopes (i.e., that \\(\\beta\\) is consistent across treatments), an F-test can be conducted by comparing two models:\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta X_{ij} + \\epsilon_{ij}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_i X_{ij} + \\epsilon_{ij}\n\\]\nWhere \\(\\beta_i = \\beta + (\\alpha\\beta)_i\\) allows for different slopes across treatments.\nTest Procedure:\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta_i = \\beta\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta\\) for at least one \\(i\\)).\n\nF-Statistic Calculation:\n\\[\nF = \\frac{\\text{SSR}*{\\text{Interaction}} / \\text{DF}*{\\text{Interaction}}}{\\text{SSE} / \\text{DF}_{\\text{Error}}}\n\\]\nWhere:\n\n\\(\\text{SSR}_{\\text{Interaction}}\\): Sum of squares for the interaction term.\n\\(\\text{DF}_{\\text{Interaction}}\\): Degrees of freedom for the interaction.\n\\(\\text{SSE}\\): Sum of squares for error.\n\\(\\text{DF}_{\\text{Error}}\\): Degrees of freedom for error.\n\nDecision Rule:\n\nCompare the calculated F-statistic to the critical value from the F-distribution.\nReject \\(H_0\\) if \\(F\\) exceeds the critical value, indicating heterogeneous slopes.\n\nReference: Scheffé (1959) elaborates on hypothesis testing for interaction terms in ANCOVA."
  },
  {
    "objectID": "lectures/week-10_ancova.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-10_ancova.html#confidence-intervals-for-higher-order-contrasts",
    "title": "ANCOVA",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts can provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\n\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen (2018) discusses confidence intervals for contrasts in Analysis of Variance, Design, and Regression."
  },
  {
    "objectID": "lectures/week-11_row-column.html",
    "href": "lectures/week-11_row-column.html",
    "title": "Row and Column Designs",
    "section": "",
    "text": "In the realm of experimental design, Row–Column Designs serve as an extension of traditional blocking techniques by incorporating two independent blocking factors: rows and columns. These designs are instrumental in controlling multiple sources of variability, thereby enhancing the precision and efficiency of treatment comparisons. By organizing experimental units into a matrix of rows and columns, Row–Column Designs mitigate the influence of two distinct nuisance factors, enabling clearer insights into the effects of the primary treatments under investigation.\nIntuitive Example: Imagine conducting an agricultural experiment to test different crop varieties. The field is naturally divided into rows and columns due to varying sunlight and soil moisture gradients. By applying each variety once per row and column, you control for these environmental gradients, ensuring that differences in crop performance are attributable to the varieties themselves rather than external factors.\nThis lecture delves into the structure, analysis, and practical applications of Row–Column Designs, with a focus on specific examples like Latin Square Designs and Youden Designs. We will explore theoretical foundations, implement analyses using R, and engage with advanced topics to provide a comprehensive understanding suitable for graduate-level studies.\n\n\n\nUnderstand the principles and utility of Row–Column Designs.\nExplore specific examples such as Latin Square and Youden Designs.\nAnalyze data using ANOVA tailored for Row–Column Designs.\nExtend designs to accommodate factorial experiments.\nImplement Row–Column Designs in R with step-by-step guidance.\nReinforce concepts through exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-11_row-column.html#objectives",
    "href": "lectures/week-11_row-column.html#objectives",
    "title": "Row and Column Designs",
    "section": "",
    "text": "Understand the principles and utility of Row–Column Designs.\nExplore specific examples such as Latin Square and Youden Designs.\nAnalyze data using ANOVA tailored for Row–Column Designs.\nExtend designs to accommodate factorial experiments.\nImplement Row–Column Designs in R with step-by-step guidance.\nReinforce concepts through exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-11_row-column.html#key-concepts",
    "href": "lectures/week-11_row-column.html#key-concepts",
    "title": "Row and Column Designs",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nRow–Column Design Structure\nA Row–Column Design arranges experimental units in a two-dimensional grid comprising rows and columns. Each cell in the grid represents an experimental unit to which a treatment is applied. This design incorporates two independent blocking factors:\n\nRows: Control for variability along the horizontal dimension.\nColumns: Control for variability along the vertical dimension.\n\nDiagram of a 3x3 Latin Square:\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nRow 1\nA\nB\nC\n\n\nRow 2\nB\nC\nA\n\n\nRow 3\nC\nA\nB\n\n\n\n\n\nBlocking Factors\n\nRow Blocks: Address systematic variability across rows, such as gradients in environmental conditions.\nColumn Blocks: Address systematic variability across columns, such as variations in soil composition.\n\nIntuitive Explanation: Consider a greenhouse experiment where temperature gradients run horizontally and light intensity gradients run vertically. By arranging treatments in a row–column grid, you control for these two sources of environmental variability, ensuring that treatment effects are not confounded with spatial gradients.\n\n\nAdvantages\n\nReduced Variability: By controlling two sources of variability, Row–Column Designs offer greater precision in estimating treatment effects.\nEfficiency: These designs make efficient use of experimental units, especially in small-scale experiments with inherent multidimensional variability.\nFlexibility: They can be extended to accommodate factorial experiments and multiple interactions.\nEnhanced Analytical Power: ANOVA tailored for Row–Column Designs can effectively isolate treatment effects from block effects.\n\nReference: Montgomery, D. C., Peck, E. A., & Vining, G. G. (2020). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#latin-square-designs",
    "href": "lectures/week-11_row-column.html#latin-square-designs",
    "title": "Row and Column Designs",
    "section": "Latin Square Designs",
    "text": "Latin Square Designs\n\nDefinition\nA Latin Square Design is a specialized type of Row–Column Design where:\n\nEach treatment appears exactly once in each row.\nEach treatment appears exactly once in each column.\n\nThis ensures that the design is balanced and controls for both row and column effects effectively.\n\n\nConstruction\n\nStandard Latin Squares:\nFor \\(v = 3\\) treatments, a Latin Square can be constructed as follows:\n\n\\[\n\\begin{array}{|c|c|c|}\n   \\hline\n   A & B & C \\\\\n   \\hline\n   B & C & A \\\\\n   \\hline\n   C & A & B \\\\\n   \\hline\n   \\end{array}\n\\]\n\nRows and Columns: Each treatment \\(A\\), \\(B\\), and \\(C\\) appears once per row and column.\n\n\nRandomization:\nTo prevent systematic bias, randomize the order of treatments within rows and columns. This can be achieved by randomly permuting the treatment labels for rows and columns.\n\n\n\nAnalysis of Variance (ANOVA)\nFor a Latin Square Design with \\(v\\) treatments, the ANOVA model includes:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\tau_k + \\epsilon_{ijk},\n\\]\nwhere:\n\n\\(Y_{ijk}\\): Response for treatment \\(k\\) in row \\(i\\) and column \\(j\\).\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i\\): Effect of row \\(i\\).\n\\(\\beta_j\\): Effect of column \\(j\\).\n\\(\\tau_k\\): Effect of treatment \\(k\\).\n\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\): Random error term.\n\nDegrees of Freedom: - Rows: \\(v - 1\\) - Columns: \\(v - 1\\) - Treatments: \\(v - 1\\) - Error: \\(v^2 - 2v + 1\\)\nExample ANOVA Table for Latin Square:\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom\nSum of Squares\nMean Square\nF-ratio\n\n\n\n\nRows\n\\(v - 1\\)\n\\(SS_\\text{Rows}\\)\n\\(MS_\\text{Rows}\\)\n\\(F = \\frac{MS_\\text{Rows}}{MS_\\text{Error}}\\)\n\n\nColumns\n\\(v - 1\\)\n\\(SS_\\text{Columns}\\)\n\\(MS_\\text{Columns}\\)\n\\(F = \\frac{MS_\\text{Columns}}{MS_\\text{Error}}\\)\n\n\nTreatments\n\\(v - 1\\)\n\\(SS_\\text{Treatments}\\)\n\\(MS_\\text{Treatments}\\)\n\\(F = \\frac{MS_\\text{Treatments}}{MS_\\text{Error}}\\)\n\n\nError\n\\(v^2 - 2v + 1\\)\n\\(SS_\\text{Error}\\)\n\\(MS_\\text{Error}\\)\n\n\n\nTotal\n\\(v^2 - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\nHypothesis Testing: - Null Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_k = 0\\) for all \\(k\\)). - Alternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, v^2 - 2v + 1)\\) at the chosen significance level (\\(\\alpha\\)).\n\n\nExample in R\n# Simulated Data for Latin Square Design\nset.seed(123)\ndata &lt;- data.frame(\n  row = factor(rep(1:3, each = 3)),\n  column = factor(rep(1:3, times = 3)),\n  treatment = factor(c(\"A\", \"B\", \"C\", \"B\", \"C\", \"A\", \"C\", \"A\", \"B\")),\n  response = c(10.2, 15.3, 20.1, 14.8, 19.5, 9.7, 19.0, 9.9, 14.2)\n)\n\n# Fit ANOVA Model\nfit &lt;- lm(response ~ row + column + treatment, data = data)\nanova(fit)\nInterpretation: - Rows and Columns: Control for row and column effects, ensuring that treatment comparisons are not confounded by these blocking factors. - Treatments: Assess the main effect of treatments on the response variable after accounting for row and column effects. - Error: Represents variability not explained by rows, columns, or treatments."
  },
  {
    "objectID": "lectures/week-11_row-column.html#youden-designs",
    "href": "lectures/week-11_row-column.html#youden-designs",
    "title": "Row and Column Designs",
    "section": "Youden Designs",
    "text": "Youden Designs\n\nDefinition\nA Youden Design is a type of Row–Column Design that extends the Latin Square by allowing for multiple treatments per row or column. Specifically, in a Youden Design:\n\nRow Blocks: Form a balanced incomplete block design.\nColumn Blocks: Each treatment appears once per column.\n\nThis design is particularly useful when the number of treatments exceeds the number of rows or columns, providing flexibility in experimental layouts.\n\n\nConstruction\nConsider a Youden Design with parameters \\(v = 4\\) treatments, \\(b = 4\\) blocks (rows), and \\(c = 3\\) columns. The design ensures that each treatment appears once in each column and multiple times across rows.\nExample Design:\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\nA & B & C \\\\\n\\hline\nB & C & D \\\\\n\\hline\nC & D & A \\\\\n\\hline\nD & A & B \\\\\n\\hline\n\\end{array}\n\\]\n\n\nApplications\n\nClinical Trials: When treatments need to be tested across different time periods or conditions.\nAgricultural Studies: When testing multiple crop varieties across different soil types and irrigation methods.\nIndustrial Experiments: When assessing the effects of various process parameters across different machines or shifts.\n\n\n\nAnalysis of Youden Designs\nThe ANOVA model for a Youden Design incorporates both row and column effects, along with treatment effects and their interactions.\n\\[\nY_{hit} = \\mu + \\theta_h + \\phi_i + \\tau_t + (\\theta \\tau)^*_{ht} + \\epsilon^*_{hit},\n\\]\nwhere:\n\n\\(Y_{hit}\\): Response for treatment \\(t\\) in row \\(h\\) and column \\(i\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of row \\(h\\).\n\\(\\phi_i\\): Effect of column \\(i\\).\n\\(\\tau_t\\): Effect of treatment \\(t\\).\n\\((\\theta \\tau)_{ht}\\): Interaction effect between row \\(h\\) and treatment \\(t\\).\n\\(\\epsilon_{hit}\\): Random error term.\n\nDegrees of Freedom: - Rows: \\(b - 1\\) - Columns: \\(c - 1\\) - Treatments: \\(v - 1\\) - Error: \\(bc - b - c - v + 2\\)\nHypothesis Testing: - Null Hypothesis (\\(H_0\\)): All treatment means are equal. - Alternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, bc - b - c - v + 2)\\) at the chosen significance level (\\(\\alpha\\)).\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-11_row-column.html#model",
    "href": "lectures/week-11_row-column.html#model",
    "title": "Row and Column Designs",
    "section": "Model",
    "text": "Model\nFor a Row–Column Design with \\(b\\) rows, \\(c\\) columns, and \\(v\\) treatments, the ANOVA model is defined as:\n\\[\nY_{hqi} = \\mu + \\theta_h + \\phi_q + \\tau_i + \\epsilon_{hqi},\n\\]\nwhere:\n\n\\(Y_{hqi}\\): Response for treatment \\(i\\) in row \\(h\\) and column \\(q\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of row \\(h\\).\n\\(\\phi_q\\): Effect of column \\(q\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hqi} \\sim N(0, \\sigma^2)\\): Random error term.\n\n\nKey Assumptions\n\nNormality: The residuals (\\(\\epsilon_{hqi}\\)) are normally distributed.\nHomogeneity of Variance (Homoscedasticity): The variance of residuals is constant across all treatments, rows, and columns.\nIndependence: Observations are independent of each other.\nNo Interaction: There is no interaction between rows and treatments or columns and treatments, i.e., the effects of treatments are consistent across rows and columns.\n\n\n\nANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom (DF)\nSum of Squares (SS)\nMean Square (MS)\nF-ratio\n\n\n\n\nRows\n\\(b - 1\\)\n\\(SS_{\\text{Rows}}\\)\n\\(MS_{\\text{Rows}} = \\frac{SS_{\\text{Rows}}}{b - 1}\\)\n\\(F = \\frac{MS_{\\text{Rows}}}{MS_{\\text{Error}}}\\)\n\n\nColumns\n\\(c - 1\\)\n\\(SS_{\\text{Columns}}\\)\n\\(MS_{\\text{Columns}} = \\frac{SS_{\\text{Columns}}}{c - 1}\\)\n\\(F = \\frac{MS_{\\text{Columns}}}{MS_{\\text{Error}}}\\)\n\n\nTreatments\n\\(v - 1\\)\n\\(SS_{\\text{Treatments}}\\)\n\\(MS_{\\text{Treatments}} = \\frac{SS_{\\text{Treatments}}}{v - 1}\\)\n\\(F = \\frac{MS_{\\text{Treatments}}}{MS_{\\text{Error}}}\\)\n\n\nError\n\\(bc - b - c - v + 2\\)\n\\(SS_{\\text{Error}}\\)\n\\(MS_{\\text{Error}} = \\frac{SS_{\\text{Error}}}{bc - b - c - v + 2}\\)\n-\n\n\nTotal\n\\(b c v - 1\\)\n\\(SS_{\\text{Total}}\\)\n-\n-\n\n\n\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_i = 0\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean differs.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio for treatments exceeds the critical value from the F-distribution with degrees of freedom \\((v - 1, bc - b - c - v + 2)\\) at the chosen significance level (\\(\\alpha\\)).\n\n\nExample Interpretation\n\nRows and Columns: Assess the significance of row and column effects. Significant row or column effects indicate that these blocking factors explain a substantial portion of the variability in the response.\nTreatments: Determine if treatments have a significant impact on the response variable after accounting for row and column effects.\nError: Represents unexplained variability, serving as the denominator for F-ratios.\n\nReference: Scheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#factorial-experiments-in-rowcolumn-designs",
    "href": "lectures/week-11_row-column.html#factorial-experiments-in-rowcolumn-designs",
    "title": "Row and Column Designs",
    "section": "Factorial Experiments in Row–Column Designs",
    "text": "Factorial Experiments in Row–Column Designs\nWhen treatments involve multiple factors (e.g., factorial combinations), Row–Column Designs can be extended to accommodate these complexities. This allows for the examination of interactions between factors within the blocking structure.\n\nModel with Interaction Terms\n\\[\nY_{hqijk} = \\mu + \\theta_h + \\phi_q + \\alpha_i + \\beta_j + (\\alpha\\beta)^*_{ij} + \\epsilon^*_{hqijk},\n\\]\nwhere:\n\n\\(\\alpha_i\\), \\(\\beta_j\\): Main effects of factors A and B.\n\\((\\alpha\\beta)_{ij}\\): Interaction effect between factors A and B.\n\n\n\nR Example: Two-Way Factorial in Row–Column Design\n\n# Two-Way Factorial in Row–Column Design\nset.seed(456)\n\n# Define Factors and Blocks\ndata &lt;- data.frame(\n  row = factor(rep(1:3, each = 6)),\n  column = factor(rep(rep(1:2, each = 3), 3)),\n  factorA = factor(rep(c(\"Low\", \"High\"), each = 9)),\n  factorB = factor(rep(c(\"Control\", \"Treatment\"), times = 9)),\n  response = c(\n    rnorm(3, mean = 5, sd = 2),\n    rnorm(3, mean = 10, sd = 2),\n    rnorm(3, mean = 15, sd = 2),\n    rnorm(3, mean = 5, sd = 2),\n    rnorm(3, mean = 10, sd = 2),\n    rnorm(3, mean = 15, sd = 2)\n  )\n)\n\n# View the Data\nprint(data)\n\n   row column factorA   factorB  response\n1    1      1     Low   Control  2.312957\n2    1      1     Low Treatment  6.243551\n3    1      1     Low   Control  6.601749\n4    1      2     Low Treatment  7.222215\n5    1      2     Low   Control  8.571286\n6    1      2     Low Treatment  9.351878\n7    2      1     Low   Control 16.381286\n8    2      1     Low Treatment 15.501096\n9    2      1     Low   Control 17.014705\n10   2      2    High Treatment  6.146469\n11   2      2    High   Control  3.168379\n12   2      2    High Treatment  7.622195\n13   3      1    High   Control 11.977453\n14   3      1    High Treatment 13.307857\n15   3      1    High   Control  7.118390\n16   3      2    High Treatment 18.894713\n17   3      2    High   Control 18.473872\n18   3      2    High Treatment 15.774967\n\n\n\n# Fit ANOVA Model with Interaction\nfit &lt;- lm(response ~ row + column + factorA * factorB, data = data)\nanova(fit)\n\n ANOVA for Two-Way Factorial in Row–Column Design Example 1\n  \n\n\n\nInterpretation:\n\nMain Effects: Assess the individual impact of factors A and B.\nInteraction Effect: Determine if the effect of one factor depends on the level of the other factor.\nBlocking Factors: Control for row and column variability, ensuring that treatment comparisons are not confounded by these dimensions.\n\nReference: Dean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#example-resting-metabolic-rate",
    "href": "lectures/week-11_row-column.html#example-resting-metabolic-rate",
    "title": "Row and Column Designs",
    "section": "Example: Resting Metabolic Rate",
    "text": "Example: Resting Metabolic Rate\n\nScenario\nA study aims to compare three different exercise protocols (Treatments A, B, C) on subjects’ resting metabolic rates. To control for age-related variability, subjects are grouped into four blocks based on age groups.\n\n\nStep-by-Step R Code\n\n1. Load and Prepare Data\n\n# Set Seed for Reproducibility\nset.seed(789)\n\n# Define Blocks and Treatments\nblocks &lt;- factor(rep(1:4, each = 3))\ntreatments &lt;- factor(rep(c(\"A\", \"B\", \"C\"), times = 4))\n\n# Simulate Response Variable (resting Metabolic Rate in kcal/day)\nresponse &lt;- c(\n  rnorm(3, mean = 1500, sd = 50), # Block 1\n  rnorm(3, mean = 1550, sd = 50), # Block 2\n  rnorm(3, mean = 1480, sd = 50), # Block 3\n  rnorm(3, mean = 1520, sd = 50) # Block 4\n)\n\n# Create Data Frame\ndata &lt;- data.frame(blocks, treatments, response)\n\n# View the Data\nprint(data)\n\n   blocks treatments response\n1       1          A 1526.205\n2       1          B 1386.962\n3       1          C 1499.016\n4       2          A 1559.157\n5       2          B 1531.932\n6       2          C 1525.776\n7       3          A 1446.684\n8       3          B 1471.277\n9       3          C 1429.452\n10      4          A 1556.985\n11      4          B 1499.885\n12      4          C 1469.861\n\n\nOutput Interpretation: The dataset comprises four blocks, each containing three treatments. The response variable represents the resting metabolic rate, simulated with slight block-specific means to reflect age-related variability.\n\n\n2. Fit the RCBD Model\n\n# Fit the RCBD Model Using ANOVA\nmodel &lt;- aov(response ~ blocks + treatments, data = data)\n\n# Summary of the Model\nsummary(model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nblocks       3  14341    4780   2.690  0.140\ntreatments   2   5663    2831   1.593  0.279\nResiduals    6  10664    1777               \n\n\nInterpretation: - Blocks: Capture the variability due to different age groups. - Treatments: Assess the effect of exercise protocols after accounting for age-related variability. - Error: Represents unexplained variability in resting metabolic rates.\n\n\n3. Diagnostic Plots\n\n# Diagnostic Plots for the ANOVA Model\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\nInterpretation: Examine residual plots to ensure that assumptions of normality, homoscedasticity, and independence are met. Look for random scatter in Residuals vs Fitted and Q-Q plots to assess normality.\n\n\n4. Post-Hoc Analysis (Multiple Comparisons)\n\n# Load Emmeans Package for Estimated Marginal means\nlibrary(emmeans)\n\n# Estimated Marginal means for Treatments\nemm &lt;- emmeans(model, ~treatments)\n\n# Pairwise Comparisons\npairwise_comparisons &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise_comparisons)\n\n Pairwise Comparisons for Resting Metabolic Rate Example 2\n  \n\n\n\nInterpretation: Identify which exercise protocols significantly differ in their effect on resting metabolic rates after adjusting for age-related variability."
  },
  {
    "objectID": "lectures/week-11_row-column.html#variance-calculations-for-latin-square-designs",
    "href": "lectures/week-11_row-column.html#variance-calculations-for-latin-square-designs",
    "title": "Row and Column Designs",
    "section": "Variance Calculations for Latin Square Designs",
    "text": "Variance Calculations for Latin Square Designs\nIn a Latin Square Design with \\(v\\) treatments, the variance of the estimated treatment effects is influenced by the design’s structure.\n\nFormula\nFor a Latin Square with \\(v\\) treatments:\n\\[\nVar(\\tau_i) = \\frac{\\sigma^2}{v}\n\\]\n\n\nDerivation\n\nModel Setup:\nThe ANOVA model for a Latin Square is:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\tau_k + \\epsilon_{ijk},\n\\]\nwhere \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\).\n\nEstimator for Treatment Effect (\\(\\tau_k\\)):\nThe treatment effects are estimated as:\n\n\\[\n\\hat{\\tau}^*k = \\bar{Y}^*{\\cdot \\cdot k} - \\bar{Y}_{\\cdot \\cdot \\cdot},\n\\]\nwhere \\(\\bar{Y}^*{\\cdot \\cdot k}\\) is the mean response for treatment \\(k\\) and \\(\\bar{Y}^*{\\cdot \\cdot \\cdot}\\) is the grand mean.\n\nVariance of \\(\\hat{\\tau}_k\\):\nSince each treatment is replicated \\(v\\) times (once per row and column):\n\n\\[\nVar(\\hat{\\tau}_k) = \\frac{\\sigma^2}{v}\n\\]\nConclusion: The variance of the treatment effect estimates decreases with increasing number of treatments in the Latin Square, enhancing the precision of the estimates.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#degrees-of-freedom-in-youden-designs",
    "href": "lectures/week-11_row-column.html#degrees-of-freedom-in-youden-designs",
    "title": "Row and Column Designs",
    "section": "Degrees of Freedom in Youden Designs",
    "text": "Degrees of Freedom in Youden Designs\nFor a Youden Design with parameters \\(v = 4\\) treatments, \\(b = 4\\) blocks (rows), and \\(c = 3\\) columns, the degrees of freedom for the error term are calculated as follows:\n\nFormula\n\\[\ndf_{\\text{Error}} = bc - b - c - v + 2\n\\]\n\n\nCalculation\nGiven:\n\n\\(v = 4\\)\n\\(b = 4\\)\n\\(c = 3\\)\n\nPlugging into the formula:\n\\[\ndf_{\\text{Error}} = (4 \\times 3) - 4 - 3 - 4 + 2 = 12 - 4 - 3 - 4 + 2 = 3\n\\]\nInterpretation: The error degrees of freedom indicate the number of independent pieces of information available to estimate the error variance. In this case, with 3 error degrees of freedom, the design is sufficiently constrained to estimate treatment effects after accounting for row and column effects.\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#proof-of-interaction-contrasts",
    "href": "lectures/week-11_row-column.html#proof-of-interaction-contrasts",
    "title": "Row and Column Designs",
    "section": "Proof of Interaction Contrasts",
    "text": "Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions.\n\n\nProof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij} + (\\alpha\\gamma)^*_{ik} + (\\beta\\gamma)^*_{jk} + (\\alpha\\beta\\gamma)^*_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)^*_{ik}\\) and \\((\\beta\\gamma)^*_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)^*_{ij} = (\\alpha\\gamma)^*_{ik} = (\\beta\\gamma)^*_{jk} = (\\alpha\\beta\\gamma)^*_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)^*_{ij}\\), \\((\\alpha\\gamma)^*_{ik}\\), \\((\\beta\\gamma)^*_{jk}\\), and \\((\\alpha\\beta\\gamma)^*_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#medium-difficulty",
    "href": "lectures/week-11_row-column.html#medium-difficulty",
    "title": "Row and Column Designs",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n\n# Define Factors and Covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate Responses with Treatment Effects and Covariate Effect\nY &lt;- 5 + \n     ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n     0.5 * (X - mean(X)) + \n     rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA Model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n\nCall:\nlm(formula = Y ~ Treatment + X, data = data_sim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6486 -1.8937 -0.3341  1.7617  9.2902 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -20.73160    1.87693 -11.045  &lt; 2e-16 ***\nTreatmentB    1.72225    0.78691   2.189   0.0313 *  \nTreatmentC    4.74007    0.78309   6.053 3.61e-08 ***\nX             0.50385    0.03621  13.914  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.031 on 86 degrees of freedom\nMultiple R-squared:  0.7332,    Adjusted R-squared:  0.7238 \nF-statistic: 78.76 on 3 and 86 DF,  p-value: &lt; 2.2e-16\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n\n\n\n\n\n\n# Estimated Marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\n\n  \n\n\n\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect.\n\n\n\n\n2. Balloon Data Analysis\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\nR Code Example:\n\nlibrary(car)\nlibrary(emmeans)\nlibrary(here)\n# Load and Prepare Data\n# Assuming 'balloon.txt' Has Columns: Color, RunOrder, Time\nballoon.data &lt;- read.table(here(\"data\", \"dean2017\", \"balloon.txt\"), header = TRUE)\n\nballoon.data |&gt; head()\n\n\n  \n\n\nballoon.data &lt;- balloon.data |&gt; transform(\n  x = Order - mean(Order),\n  fColor = factor(Color)\n)\n\n# Fit ANCOVA Model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n\nCall:\nlm(formula = Time ~ fColor + x, data = balloon.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.985 -1.736 -0.211  1.493  5.249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.28474    0.91736  19.932  &lt; 2e-16 ***\nfColor2      4.10560    1.29760   3.164 0.003829 ** \nfColor3      3.80129    1.29872   2.927 0.006867 ** \nfColor4     -0.07086    1.29736  -0.055 0.956843    \nx           -0.21103    0.04981  -4.237 0.000236 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.594 on 27 degrees of freedom\nMultiple R-squared:  0.5776,    Adjusted R-squared:  0.515 \nF-statistic: 9.229 on 4 and 27 DF,  p-value: 7.797e-05\n\n# Test Homogeneity of Slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n\n  \n\n\n# Estimated Marginal means\nemm_balloon &lt;- emmeans(model, ~fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\n\n  \n\n\n\nQuestions:\n\nInteraction Term:\n\nWhat does the interaction term between fColor and x indicate?\nIs the interaction term statistically significant?\n\nModel Interpretation:\n\nIf the interaction is not significant, what does this imply about the slopes?\nHow do the adjusted treatment means differ from the raw means?\n\nPairwise Comparisons:\n\nWhich balloon colors have significantly different inflation times after adjusting for run order?\nHow can these differences inform practical decisions in balloon manufacturing?\n\nAssumption Testing:\n\nDo the diagnostic plots suggest any violations of ANCOVA assumptions?\nShould any transformations or alternative models be considered based on these diagnostics?\n\n\nExpected Answers:\n\nInteraction Term:\n\nThe interaction term assesses whether the relationship between x (RunOrder) and Time differs across fColor (Color).\nA non-significant interaction suggests that slopes are homogeneous across treatments.\n\nModel Interpretation:\n\nHomogeneous slopes imply that the covariate affects all treatments equally.\nAdjusted treatment means account for differences in x, providing a fair comparison of treatment effects.\n\nPairwise Comparisons:\n\nSignificant differences indicate specific color treatments that lead to different inflation times.\nManufacturers can focus on optimizing colors that perform better in terms of inflation speed.\n\nAssumption Testing:\n\nRandom scatter in Residuals vs Fitted and Q-Q plots close to the diagonal indicate that assumptions are met.\nIf violations are present, consider data transformations or robust statistical methods."
  },
  {
    "objectID": "lectures/week-11_row-column.html#general-complete-block-designs",
    "href": "lectures/week-11_row-column.html#general-complete-block-designs",
    "title": "Row and Column Designs",
    "section": "General Complete Block Designs",
    "text": "General Complete Block Designs\n\nModel\nIn General Complete Block Designs, treatments may be replicated within blocks, allowing for the detection of interactions between treatments and blocking factors.\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit},\n\\]\nwhere:\n\n\\(Y_{hit}\\): Response for treatment \\(i\\) in block \\(h\\), replicate \\(t\\).\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\((\\theta \\tau)_{hi}\\): Interaction effect between block \\(h\\) and treatment \\(i\\).\n\\(\\epsilon_{hit}\\): Random error term.\n\n\n\nSample Size Calculations\nDetermining the appropriate sample size ensures adequate power to detect treatment effects.\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(b\\): Number of blocks.\n\\(\\sigma^2\\): Estimated variance.\n\\(\\phi\\): Desired power (e.g., 0.80).\n\\(\\Delta\\): Minimum detectable effect size.\n\nExample Calculation in R:\n# Parameters\nv &lt;- 4          # Number of treatments\nb &lt;- 5          # Number of blocks\nsigma2 &lt;- 20    # Variance estimate\nphi &lt;- 0.84     # Desired power (approx 0.80)\nDelta &lt;- 5      # Minimum detectable effect size\n\n# Calculate Sample Size\nn &lt;- ceiling((2 * v * sigma2 * phi^2) / (b * Delta^2))\nprint(n)\nInterpretation: The ceiling function ensures that the sample size is rounded up to the next integer, guaranteeing that the desired power is achieved.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#handling-block-treatment-interactions",
    "href": "lectures/week-11_row-column.html#handling-block-treatment-interactions",
    "title": "Row and Column Designs",
    "section": "Handling Block-Treatment Interactions",
    "text": "Handling Block-Treatment Interactions\n\nImportance of Interaction Terms\nIncluding interaction terms in the ANOVA model allows for the detection of how treatment effects may vary across different blocks.\n\n\nModel with Interaction\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit},\n\\]\nImplications:\n\nSignificant Interaction: Indicates that the effect of treatments differs across blocks. In such cases, it may be necessary to analyze treatment effects within each block separately or use more complex modeling techniques.\nNon-Significant Interaction: Supports the additive model, validating the use of standard Row–Column ANOVA without interaction terms.\n\n\n\nR Example: Testing for Block-Treatment Interaction\n\n# Simulated Data with Interaction\nset.seed(789)\n\n# Define Factors and Blocks\ndata &lt;- data.frame(\n  row = factor(rep(1:3, each = 6)),\n  column = factor(rep(rep(1:2, each = 3), 3)),\n  factorA = factor(rep(c(\"Low\", \"High\"), each = 9)),\n  factorB = factor(rep(c(\"Control\", \"Treatment\"), times = 9)),\n  response = c(\n    rnorm(6, mean = 5, sd = 2),\n    rnorm(6, mean = 10, sd = 2),\n    rnorm(6, mean = 15, sd = 2)\n  )\n)\n\n# Introduce Interaction Effect\ndata$response &lt;- data$response + ifelse(data$factorA == \"High\" & data$factorB == \"Treatment\", 2, 0)\n\n# Fit ANOVA Model with Interaction\nfit_interaction &lt;- lm(response ~ row + column + factorA * factorB, data = data)\nanova(fit_interaction)\n\n\n  \n\n\n\nInterpretation: - Interaction Term: A significant interaction term suggests that the effect of factorA on the response depends on the level of factorB.\n\nMain Effects: Assess the individual impact of each factor, controlling for others.\n\nReference: Kempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company."
  },
  {
    "objectID": "lectures/week-11_row-column.html#medium-difficulty-1",
    "href": "lectures/week-11_row-column.html#medium-difficulty-1",
    "title": "Row and Column Designs",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n\n# Define Factors and Covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate Responses with Treatment Effects and Covariate Effect\nY &lt;- 5 + \n     ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n     0.5 * (X - mean(X)) + \n     rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA Model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n\nCall:\nlm(formula = Y ~ Treatment + X, data = data_sim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6486 -1.8937 -0.3341  1.7617  9.2902 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -20.73160    1.87693 -11.045  &lt; 2e-16 ***\nTreatmentB    1.72225    0.78691   2.189   0.0313 *  \nTreatmentC    4.74007    0.78309   6.053 3.61e-08 ***\nX             0.50385    0.03621  13.914  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.031 on 86 degrees of freedom\nMultiple R-squared:  0.7332,    Adjusted R-squared:  0.7238 \nF-statistic: 78.76 on 3 and 86 DF,  p-value: &lt; 2.2e-16\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n\n\n\n\n\n\n# Estimated Marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\n\n  \n\n\n\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect.\n\n\n\n\n2. Balloon Data Analysis\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\nR Code Example:\n\n# Fit ANCOVA Model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n\nCall:\nlm(formula = Time ~ fColor + x, data = balloon.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.985 -1.736 -0.211  1.493  5.249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.28474    0.91736  19.932  &lt; 2e-16 ***\nfColor2      4.10560    1.29760   3.164 0.003829 ** \nfColor3      3.80129    1.29872   2.927 0.006867 ** \nfColor4     -0.07086    1.29736  -0.055 0.956843    \nx           -0.21103    0.04981  -4.237 0.000236 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.594 on 27 degrees of freedom\nMultiple R-squared:  0.5776,    Adjusted R-squared:  0.515 \nF-statistic: 9.229 on 4 and 27 DF,  p-value: 7.797e-05\n\n# Test Homogeneity of Slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n\n  \n\n\n# Estimated Marginal means\nemm_balloon &lt;- emmeans(model, ~ fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\n\n  \n\n\n\nQuestions:\n\nInteraction Term:\n\nWhat does the interaction term between fColor and x indicate?\nIs the interaction term statistically significant?\n\nModel Interpretation:\n\nIf the interaction is not significant, what does this imply about the slopes?\nHow do the adjusted treatment means differ from the raw means?\n\nPairwise Comparisons:\n\nWhich balloon colors have significantly different inflation times after adjusting for run order?\nHow can these differences inform practical decisions in balloon manufacturing?\n\nAssumption Testing:\n\nDo the diagnostic plots suggest any violations of ANCOVA assumptions?\nShould any transformations or alternative models be considered based on these diagnostics?\n\n\nExpected Answers:\n\nInteraction Term:\n\nThe interaction term assesses whether the relationship between x (RunOrder) and Time differs across fColor (Color).\nA non-significant interaction suggests that slopes are homogeneous across treatments.\n\nModel Interpretation:\n\nHomogeneous slopes imply that the covariate affects all treatments equally.\nAdjusted treatment means account for differences in x, providing a fair comparison of treatment effects.\n\nPairwise Comparisons:\n\nSignificant differences indicate specific color treatments that lead to different inflation times.\nManufacturers can focus on optimizing colors that perform better in terms of inflation speed.\n\nAssumption Testing:\n\nRandom scatter in Residuals vs Fitted and Q-Q plots close to the diagonal indicate that assumptions are met.\nIf violations are present, consider data transformations or robust statistical methods."
  },
  {
    "objectID": "lectures/week-11_row-column.html#challenging-problems",
    "href": "lectures/week-11_row-column.html#challenging-problems",
    "title": "Row and Column Designs",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\n1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\nR Code Example:\nset.seed(456)\n# Simulate Unbalanced Data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n# Simulate Responses with Treatment and Covariate Effects\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA Model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points:\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\nExpected Insights:\n\nType II SS: Often preferred when interactions are not of primary interest. It provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced. It accounts for all factors simultaneously, providing conditional main effects.\nImbalance Effects: Can distort the Type I SS results, leading to misleading conclusions. Type II and III SS offer more robust alternatives in unbalanced settings.\n\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\n2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\nR Code Example:\n\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate Data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate Responses with Three-way Interaction\nY &lt;- 5 + \nifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \nifelse(FactorB == \"B1\", 0, 3) + \nifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n0.5 * (X - mean(X)) + \nifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n      ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \nrnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit Three-way ANCOVA Model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n\nCall:\nlm(formula = Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1220 -1.5619  0.1516  1.6510  6.8368 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   -14.8115     2.7834  -5.321 2.12e-06 ***\nFactorAA2                       4.4442     2.1281   2.088   0.0416 *  \nFactorAA3                       4.8113     2.1420   2.246   0.0289 *  \nFactorBB2                       0.9491     2.1154   0.449   0.6555    \nFactorCC2                      -1.5970     2.1606  -0.739   0.4631    \nFactorCC3                       2.4882     2.1459   1.160   0.2514    \nX                               0.4291     0.0452   9.493 5.02e-13 ***\nFactorAA2:FactorBB2            -2.3872     3.0185  -0.791   0.4326    \nFactorAA3:FactorBB2            -1.9274     3.0425  -0.633   0.5292    \nFactorAA2:FactorCC2             0.7170     3.0209   0.237   0.8133    \nFactorAA3:FactorCC2             2.6386     3.1886   0.828   0.4117    \nFactorAA2:FactorCC3            -2.1941     3.0202  -0.726   0.4707    \nFactorAA3:FactorCC3             1.6601     3.1140   0.533   0.5962    \nFactorBB2:FactorCC2             3.2077     3.0003   1.069   0.2899    \nFactorBB2:FactorCC3             4.4736     3.0125   1.485   0.1435    \nFactorAA2:FactorBB2:FactorCC2  -1.2045     4.2875  -0.281   0.7799    \nFactorAA3:FactorBB2:FactorCC2   3.0157     4.3435   0.694   0.4905    \nFactorAA2:FactorBB2:FactorCC3   2.3360     4.2358   0.551   0.5836    \nFactorAA3:FactorBB2:FactorCC3  -1.9520     4.3817  -0.445   0.6578    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.989 on 53 degrees of freedom\nMultiple R-squared:  0.8424,    Adjusted R-squared:  0.7889 \nF-statistic: 15.74 on 18 and 53 DF,  p-value: 3.021e-15\n\n# Plot Three-way Interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ FactorC) +\n  labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Estimated Marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\n\n\n  \n\n\n\nInterpretation:\n\nThree-Way Interaction:\n\nThe interaction plot demonstrates how the interaction between Factor A and Factor B varies across different levels of Factor C.\nNon-parallel lines across facets indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation.\nThe effect of one factor depends on the combination of other factors, necessitating a comprehensive analysis.\n\nPractical Implications:\n\nUnderstanding three-way interactions provides nuanced insights into how multiple factors jointly influence the response variable.\nThis can inform more sophisticated decision-making and strategy development in experimental settings.\n\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer.\n\n\n3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nProof Outline:\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij}+ (\\alpha\\gamma)^*_{ik}+ (\\beta\\gamma)^*_{jk}+ (\\alpha\\beta\\gamma)^*_{ijk}+ \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)^*{ik}\\) and \\((\\beta\\gamma)^*{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProof of Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)^*_{ij}= (\\alpha\\gamma)^*_{ik}= (\\beta\\gamma)^*_{jk}= (\\alpha\\beta\\gamma)^*_{ijk}= 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This indicates that the response is purely additive with respect to the main effects.\n\nZero Interaction Contrasts: If all interaction contrasts are zero, the model exhibits additivity. Thus, interaction contrasts quantify how much the actual model deviates from an additive one.\n\n\nConclusion:\nInteraction contrasts provide a mathematical measure of the presence and magnitude of interactions. Non-zero contrasts indicate that the combined effect of factors is not merely the sum of their individual effects, highlighting the importance of considering interactions in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#variance-calculations-for-latin-square-designs-1",
    "href": "lectures/week-11_row-column.html#variance-calculations-for-latin-square-designs-1",
    "title": "Row and Column Designs",
    "section": "Variance Calculations for Latin Square Designs",
    "text": "Variance Calculations for Latin Square Designs\nIn a Latin Square Design with \\(v\\) treatments, the variance of the estimated treatment effects is influenced by the design’s structure.\n\nFormula\nFor a Latin Square with \\(v\\) treatments:\n\\[\nVar(\\tau_i) = \\frac{\\sigma^2}{v}\n\\]\n\n\nDerivation\n\nModel Setup:\nThe ANOVA model for a Latin Square is:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\tau_k + \\epsilon_{ijk},\n\\]\nwhere \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\).\n\nEstimator for Treatment Effect (\\(\\tau_k\\)):\nThe treatment effects are estimated as:\n\n\\[\n\\hat{\\tau}*k = \\bar{Y}*{\\cdot \\cdot k} - \\bar{Y}_{\\cdot \\cdot \\cdot},\n\\]\nwhere \\(\\bar{Y}*{\\cdot \\cdot k}\\) is the mean response for treatment \\(k\\) and \\(\\bar{Y}^*{\\cdot \\cdot \\cdot}\\) is the grand mean.\n\nVariance of \\(\\hat{\\tau}_k\\):\nSince each treatment is replicated \\(v\\) times (once per row and column):\n\n\\[\nVar(\\hat{\\tau}_k) = \\frac{\\sigma^2}{v}\n\\]\nConclusion: The variance of the treatment effect estimates decreases with increasing number of treatments in the Latin Square, enhancing the precision of the estimates.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#degrees-of-freedom-in-youden-designs-1",
    "href": "lectures/week-11_row-column.html#degrees-of-freedom-in-youden-designs-1",
    "title": "Row and Column Designs",
    "section": "Degrees of Freedom in Youden Designs",
    "text": "Degrees of Freedom in Youden Designs\nFor a Youden Design with parameters \\(v = 4\\) treatments, \\(b = 4\\) blocks (rows), and \\(c = 3\\) columns, the degrees of freedom for the error term are calculated as follows:\n\nFormula\n\\[\ndf_{\\text{Error}} = bc - b - c - v + 2\n\\]\n\n\nCalculation\nGiven:\n\n\\(v = 4\\)\n\\(b = 4\\)\n\\(c = 3\\)\n\nPlugging into the formula:\n\\[\ndf_{\\text{Error}} = (4 \\times 3) - 4 - 3 - 4 + 2 = 12 - 4 - 3 - 4 + 2 = 3\n\\]\nInterpretation: The error degrees of freedom indicate the number of independent pieces of information available to estimate the error variance. In this case, with 3 error degrees of freedom, the design is sufficiently constrained to estimate treatment effects after accounting for row and column effects.\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#proof-of-interaction-contrasts-1",
    "href": "lectures/week-11_row-column.html#proof-of-interaction-contrasts-1",
    "title": "Row and Column Designs",
    "section": "Proof of Interaction Contrasts",
    "text": "Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions.\n\n\nProof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij}+ (\\alpha\\gamma)^*_{ik}+ (\\beta\\gamma)^*_{jk}+ (\\alpha\\beta\\gamma)^*_{ijk}+ \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)*{ik}\\) and \\((\\beta\\gamma)*{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)^*_{ij}= (\\alpha\\gamma)^*_{ik}= (\\beta\\gamma)^*_{jk}= (\\alpha\\beta\\gamma)^*_{ijk}= 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)^*{ij}\\), \\((\\alpha\\gamma)^*{ik}\\), \\((\\beta\\gamma)^*{jk}\\), and \\((\\alpha\\beta\\gamma)^*{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#variance-calculations-for-latin-square-designs-2",
    "href": "lectures/week-11_row-column.html#variance-calculations-for-latin-square-designs-2",
    "title": "Row and Column Designs",
    "section": "Variance Calculations for Latin Square Designs",
    "text": "Variance Calculations for Latin Square Designs\nIn a Latin Square Design with \\(v\\) treatments, the variance of the estimated treatment effects is influenced by the design’s structure.\n\nFormula\nFor a Latin Square with \\(v\\) treatments:\n\\[\nVar(\\tau_i) = \\frac{\\sigma^2}{v}\n\\]\n\n\nDerivation\n\nModel Setup:\nThe ANOVA model for a Latin Square is:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\tau_k + \\epsilon_{ijk},\n\\]\nwhere \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\).\n\nEstimator for Treatment Effect (\\(\\tau_k\\)):\nThe treatment effects are estimated as:\n\n\\[\n\\hat{\\tau}*k = \\bar{Y}*{\\cdot \\cdot k} - \\bar{Y}_{\\cdot \\cdot \\cdot},\n\\]\nwhere \\(\\bar{Y}*{\\cdot \\cdot k}\\) is the mean response for treatment \\(k\\) and \\(\\bar{Y}*{\\cdot \\cdot \\cdot}\\) is the grand mean.\n\nVariance of \\(\\hat{\\tau}_k\\):\n\nSince each treatment is replicated \\(v\\) times (once per row and column):\n\\[\nVar(\\hat{\\tau}_k) = \\frac{\\sigma^2}{v}\n\\]\nConclusion: The variance of the treatment effect estimates decreases with increasing number of treatments in the Latin Square, enhancing the precision of the estimates.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#degrees-of-freedom-in-youden-designs-2",
    "href": "lectures/week-11_row-column.html#degrees-of-freedom-in-youden-designs-2",
    "title": "Row and Column Designs",
    "section": "Degrees of Freedom in Youden Designs",
    "text": "Degrees of Freedom in Youden Designs\nFor a Youden Design with parameters \\(v = 4\\) treatments, \\(b = 4\\) blocks (rows), and \\(c = 3\\) columns, the degrees of freedom for the error term are calculated as follows:\n\nFormula\n\\[\ndf_{\\text{Error}} = bc - b - c - v + 2\n\\]\n\n\nCalculation\nGiven:\n\n\\(v = 4\\)\n\\(b = 4\\)\n\\(c = 3\\)\n\nPlugging into the formula:\n\\[\ndf_{\\text{Error}} = (4 \\times 3) - 4 - 3 - 4 + 2 = 12 - 4 - 3 - 4 + 2 = 3\n\\]\nInterpretation: The error degrees of freedom indicate the number of independent pieces of information available to estimate the error variance. In this case, with 3 error degrees of freedom, the design is sufficiently constrained to estimate treatment effects after accounting for row and column effects.\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#proof-of-interaction-contrasts-2",
    "href": "lectures/week-11_row-column.html#proof-of-interaction-contrasts-2",
    "title": "Row and Column Designs",
    "section": "Proof of Interaction Contrasts",
    "text": "Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions.\n\n\nProof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij}+ (\\alpha\\gamma)^*_{ik}+ (\\beta\\gamma)^*_{jk}+ (\\alpha\\beta\\gamma)^*_{ijk}+ \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)*{ik}\\) and \\((\\beta\\gamma)*{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)*_{ij}= (\\alpha\\gamma)^*_{ik}= (\\beta\\gamma)^*_{jk}= (\\alpha\\beta\\gamma)^*_{ijk}= 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)^*{ij}\\), \\((\\alpha\\gamma)^*{ik}\\), \\((\\beta\\gamma)^*{jk}\\), and \\((\\alpha\\beta\\gamma)^*{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-11_row-column.html#adjusted-treatment-means",
    "href": "lectures/week-11_row-column.html#adjusted-treatment-means",
    "title": "Row and Column Designs",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe adjusted mean for treatment \\(i\\) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate.\n\n\nDerivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-11_row-column.html#f-test-for-homogeneity-of-slopes",
    "title": "Row and Column Designs",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Procedure\n\nFit Both Models:\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\nCompare Models Using ANOVA:\n\n# Fit Reduced Model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n# Fit Full Model with Interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n# Compare Models\nanova(model_reduced, model_full)\n\nDecision Rule:\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject \\(H_0\\) and conclude that slopes are not homogeneous.\nIf not significant, accept \\(H_0\\) and proceed with the reduced model.\n\n\nConclusion:\nA significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference:\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-11_row-column.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-11_row-column.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Row and Column Designs",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\n\nlibrary(emmeans)\n\n# Fit the ANCOVA Model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define Estimated Marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a Contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\n\n\n  \n\n\n\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-11_row-column.html#medium-difficulty-2",
    "href": "lectures/week-11_row-column.html#medium-difficulty-2",
    "title": "Row and Column Designs",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n\n# Define Factors and Covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate Responses with Treatment Effects and Covariate Effect\nY &lt;- 5 + ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) +  0.5 * (X - mean(X)) + rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA Model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n\nCall:\nlm(formula = Y ~ Treatment + X, data = data_sim)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6486 -1.8937 -0.3341  1.7617  9.2902 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -20.73160    1.87693 -11.045  &lt; 2e-16 ***\nTreatmentB    1.72225    0.78691   2.189   0.0313 *  \nTreatmentC    4.74007    0.78309   6.053 3.61e-08 ***\nX             0.50385    0.03621  13.914  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.031 on 86 degrees of freedom\nMultiple R-squared:  0.7332,    Adjusted R-squared:  0.7238 \nF-statistic: 78.76 on 3 and 86 DF,  p-value: &lt; 2.2e-16\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n\n\n\n\n\n\n# Estimated Marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\n\n  \n\n\n\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect.\n\n\n\n\n2. Balloon Data Analysis\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\nR Code Example:\n\nlibrary(car)\nlibrary(emmeans)\n\n# Fit ANCOVA Model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n\nCall:\nlm(formula = Time ~ fColor + x, data = balloon.data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.985 -1.736 -0.211  1.493  5.249 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.28474    0.91736  19.932  &lt; 2e-16 ***\nfColor2      4.10560    1.29760   3.164 0.003829 ** \nfColor3      3.80129    1.29872   2.927 0.006867 ** \nfColor4     -0.07086    1.29736  -0.055 0.956843    \nx           -0.21103    0.04981  -4.237 0.000236 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.594 on 27 degrees of freedom\nMultiple R-squared:  0.5776,    Adjusted R-squared:  0.515 \nF-statistic: 9.229 on 4 and 27 DF,  p-value: 7.797e-05\n\n# Test Homogeneity of Slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n\n  \n\n\n# Estimated Marginal means\nemm_balloon &lt;- emmeans(model, ~ fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\n\n  \n\n\n\nQuestions:\n\nInteraction Term:\n\nWhat does the interaction term between fColor and x indicate?\nIs the interaction term statistically significant?\n\nModel Interpretation:\n\nIf the interaction is not significant, what does this imply about the slopes?\nHow do the adjusted treatment means differ from the raw means?\n\nPairwise Comparisons:\n\nWhich balloon colors have significantly different inflation times after adjusting for run order?\nHow can these differences inform practical decisions in balloon manufacturing?\n\nAssumption Testing:\n\nDo the diagnostic plots suggest any violations of ANCOVA assumptions?\nShould any transformations or alternative models be considered based on these diagnostics?\n\n\nExpected Answers:\n\nInteraction Term:\n\nThe interaction term assesses whether the relationship between x (RunOrder) and Time differs across fColor (Color).\nA non-significant interaction suggests that slopes are homogeneous across treatments.\n\nModel Interpretation:\n\nHomogeneous slopes imply that the covariate affects all treatments equally.\nAdjusted treatment means account for differences in x, providing a fair comparison of treatment effects.\n\nPairwise Comparisons:\n\nSignificant differences indicate specific color treatments that lead to different inflation times.\nManufacturers can focus on optimizing colors that perform better in terms of inflation speed.\n\nAssumption Testing:\n\nRandom scatter in Residuals vs Fitted and Q-Q plots close to the diagonal indicate that assumptions are met.\nIf violations are present, consider data transformations or robust statistical methods."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "This example highlights the design of an experiment to compare the effects of inpatient and outpatient protocols on the in-laboratory measurement of resting metabolic rate (RMR) in humans. The study was conducted to address discrepancies between different experimental protocols and their implications for RMR measurements.\n\n\n\nA prior study observed that RMR measurements on elderly individuals were, on average, 8% higher using an outpatient protocol compared to an inpatient protocol.\nSuch discrepancies could undermine the comparability of results across laboratories using different protocols.\nThe goal of the study was to assess whether the protocol type has a negligible effect on RMR measurements.\n\n\n\n\n\n\n\nTo determine whether different protocols for measuring RMR significantly affect the results.\n\n\n\nThree experimental treatments were defined based on the protocol type:\n\nInpatient Protocol with Controlled Meals:\n\nPatients were fed a standardized evening meal and spent the night in the laboratory.\nRMR was measured in the morning.\n\nOutpatient Protocol with Controlled Meals:\n\nPatients were fed the same standardized evening meal at the laboratory but spent the night at home.\nRMR was measured in the morning.\n\nOutpatient Protocol with Uncontrolled Meals:\n\nPatients were instructed to eat their regular evening meal at home and spent the night at home.\nRMR was measured in the morning.\n\n\n\n\n\n\nNull Hypothesis (\\(H_0\\)): The protocol type has no effect on the measured RMR.\nAlternative Hypothesis (\\(H_a\\)): The protocol type has a significant effect on the measured RMR.\n\n\n\n\n\n\n\nTo reduce variability unrelated to the protocols, blocking factors such as age, gender, or baseline metabolic rate could be included in the design.\n\n\n\n\nRandom assignment of participants to the three protocol groups ensures that systematic biases are minimized.\n\n\n\n\n\nThe primary response variable is the measured RMR (in units such as \\(\\text{kcal/day}\\)).\n\n\n\n\n\nControlling meal content and timing for the controlled protocols.\nEnsuring adherence to the protocols for the outpatient treatments.\n\n\n\n\n\nThe data collected from the experiment can be analyzed using Analysis of Variance (ANOVA), as it is well-suited for comparing means across multiple groups.\n\n\n\nANOVA Model:\n\nResponse variable: RMR.\nFactor: Protocol type (3 levels: inpatient controlled, outpatient controlled, outpatient uncontrolled).\n\nThe ANOVA model can be expressed as:\n\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij},\n\\]\nwhere:\n\n\\(y_{ij}\\): Observed RMR for the \\(j\\)-th participant in the \\(i\\)-th protocol.\n\\(\\mu\\): Overall mean RMR.\n\\(\\tau_i\\): Effect of the \\(i\\)-th protocol.\n\\(\\epsilon_{ij}\\): Random error term.\n\n\nNull and Alternative Hypotheses:\n\n\\(H_0: \\tau_1 = \\tau_2 = \\tau_3 = 0\\) (no effect of protocol type).\n\\(H_a: \\text{At least one } \\tau_i \\neq 0\\) (protocol type has an effect).\n\nF-statistic:\n\nCompute the ratio of between-group variability to within-group variability.\n\nPost-hoc Tests:\n\nIf \\(H_0\\) is rejected, conduct pairwise comparisons (e.g., Tukey’s test) to identify significant differences between protocol types."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#introduction",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#introduction",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "This example highlights the design of an experiment to compare the effects of inpatient and outpatient protocols on the in-laboratory measurement of resting metabolic rate (RMR) in humans. The study was conducted to address discrepancies between different experimental protocols and their implications for RMR measurements.\n\n\n\nA prior study observed that RMR measurements on elderly individuals were, on average, 8% higher using an outpatient protocol compared to an inpatient protocol.\nSuch discrepancies could undermine the comparability of results across laboratories using different protocols.\nThe goal of the study was to assess whether the protocol type has a negligible effect on RMR measurements."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#experimental-design",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#experimental-design",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "To determine whether different protocols for measuring RMR significantly affect the results.\n\n\n\nThree experimental treatments were defined based on the protocol type:\n\nInpatient Protocol with Controlled Meals:\n\nPatients were fed a standardized evening meal and spent the night in the laboratory.\nRMR was measured in the morning.\n\nOutpatient Protocol with Controlled Meals:\n\nPatients were fed the same standardized evening meal at the laboratory but spent the night at home.\nRMR was measured in the morning.\n\nOutpatient Protocol with Uncontrolled Meals:\n\nPatients were instructed to eat their regular evening meal at home and spent the night at home.\nRMR was measured in the morning.\n\n\n\n\n\n\nNull Hypothesis (\\(H_0\\)): The protocol type has no effect on the measured RMR.\nAlternative Hypothesis (\\(H_a\\)): The protocol type has a significant effect on the measured RMR."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#considerations-for-experimental-design",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#considerations-for-experimental-design",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "To reduce variability unrelated to the protocols, blocking factors such as age, gender, or baseline metabolic rate could be included in the design.\n\n\n\n\nRandom assignment of participants to the three protocol groups ensures that systematic biases are minimized.\n\n\n\n\n\nThe primary response variable is the measured RMR (in units such as \\(\\text{kcal/day}\\)).\n\n\n\n\n\nControlling meal content and timing for the controlled protocols.\nEnsuring adherence to the protocols for the outpatient treatments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking-metabolic-example.html#statistical-analysis",
    "href": "lectures/week-08_rcbd-blocking-metabolic-example.html#statistical-analysis",
    "title": "Example: Resting Metabolic Rate Experiment",
    "section": "",
    "text": "The data collected from the experiment can be analyzed using Analysis of Variance (ANOVA), as it is well-suited for comparing means across multiple groups.\n\n\n\nANOVA Model:\n\nResponse variable: RMR.\nFactor: Protocol type (3 levels: inpatient controlled, outpatient controlled, outpatient uncontrolled).\n\nThe ANOVA model can be expressed as:\n\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij},\n\\]\nwhere:\n\n\\(y_{ij}\\): Observed RMR for the \\(j\\)-th participant in the \\(i\\)-th protocol.\n\\(\\mu\\): Overall mean RMR.\n\\(\\tau_i\\): Effect of the \\(i\\)-th protocol.\n\\(\\epsilon_{ij}\\): Random error term.\n\n\nNull and Alternative Hypotheses:\n\n\\(H_0: \\tau_1 = \\tau_2 = \\tau_3 = 0\\) (no effect of protocol type).\n\\(H_a: \\text{At least one } \\tau_i \\neq 0\\) (protocol type has an effect).\n\nF-statistic:\n\nCompute the ratio of between-group variability to within-group variability.\n\nPost-hoc Tests:\n\nIf \\(H_0\\) is rejected, conduct pairwise comparisons (e.g., Tukey’s test) to identify significant differences between protocol types."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html",
    "href": "lectures/week-08_rcbd-blocking.html",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "",
    "text": "In the realm of experimental design, Complete Block Designs (CBDs) play a pivotal role in controlling the effects of nuisance factors, thereby enhancing the precision of treatment comparisons. By organizing experimental units into blocks that share similar characteristics, CBDs effectively reduce variability stemming from these extraneous factors. This structured approach not only refines the efficiency of experiments but also ensures more accurate and reliable analyses.\n\n\nImagine you are testing different fertilizers on crops spread across a large farm. The farm has sections with varying soil types, sunlight exposure, and moisture levels. Without controlling for these variations, it would be challenging to discern the true effect of each fertilizer. By grouping plots with similar soil types into blocks, you can more precisely compare the performance of fertilizers within each homogeneous group, minimizing the confounding influence of soil variability.\nThis lecture delves into the structure, applications, and analysis of Complete Block Designs, with a focus on Randomized Complete Block Designs (RCBDs) and their generalizations. We will explore theoretical underpinnings, practical implementations using R, and advanced topics to provide a comprehensive understanding tailored for graduate statistics students."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#introduction",
    "href": "lectures/week-08_rcbd-blocking.html#introduction",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "",
    "text": "In the realm of experimental design, Complete Block Designs (CBDs) play a pivotal role in controlling the effects of nuisance factors, thereby enhancing the precision of treatment comparisons. By organizing experimental units into blocks that share similar characteristics, CBDs effectively reduce variability stemming from these extraneous factors. This structured approach not only refines the efficiency of experiments but also ensures more accurate and reliable analyses.\n\n\nImagine you are testing different fertilizers on crops spread across a large farm. The farm has sections with varying soil types, sunlight exposure, and moisture levels. Without controlling for these variations, it would be challenging to discern the true effect of each fertilizer. By grouping plots with similar soil types into blocks, you can more precisely compare the performance of fertilizers within each homogeneous group, minimizing the confounding influence of soil variability.\nThis lecture delves into the structure, applications, and analysis of Complete Block Designs, with a focus on Randomized Complete Block Designs (RCBDs) and their generalizations. We will explore theoretical underpinnings, practical implementations using R, and advanced topics to provide a comprehensive understanding tailored for graduate statistics students."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#objectives",
    "href": "lectures/week-08_rcbd-blocking.html#objectives",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Objectives",
    "text": "Objectives\n\nDefine and distinguish between complete and incomplete block designs.\nExplore the structure and application of Randomized Complete Block Designs (RCBDs).\nPerform Analysis of Variance (ANOVA) for RCBDs and assess the efficiency of blocking.\nUnderstand general complete block designs and their applications.\nImplement complete block designs in R and interpret the results.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#r-packages",
    "href": "lectures/week-08_rcbd-blocking.html#r-packages",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "R Packages",
    "text": "R Packages\nTo facilitate the analysis of Randomized Complete Block Designs (RCBDs) and contrasts, we will utilize the following R packages:\n\n# Required R Packages\npacman::p_load(dplyr, ggplot2, emmeans)\n\n\nemmeans: For estimating marginal means and conducting pairwise comparisons.\nggplot2: For creating visualizations and diagnostic plots.\ndplyr: For data manipulation and summarization.\n\nEnsure that these packages are installed in your R environment to follow along with the examples and exercises."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#what-are-complete-block-designs",
    "href": "lectures/week-08_rcbd-blocking.html#what-are-complete-block-designs",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "What Are Complete Block Designs?",
    "text": "What Are Complete Block Designs?\nComplete Block Designs (CBDs) are experimental designs where each block contains all the treatments being tested. This ensures that every treatment is applied in each block, allowing for direct comparisons under similar conditions.\n\nBlock: A group of experimental units that are similar in some aspect and expected to yield comparable responses.\nComplete Block: Each block includes every treatment exactly once.\n\n\nExample\nIn a clinical trial comparing drug treatments across different hospitals, each hospital represents a block. By administering all treatments in each hospital, you account for hospital-specific factors, such as patient demographics and care protocols, ensuring that treatment effects are evaluated consistently."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#randomized-complete-block-designs-rcbds",
    "href": "lectures/week-08_rcbd-blocking.html#randomized-complete-block-designs-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Randomized Complete Block Designs (RCBDs)",
    "text": "Randomized Complete Block Designs (RCBDs)\nRCBDs are a specific type of CBD where treatments are randomly assigned within each block. This randomness ensures that the assignment of treatments is free from bias and that any differences observed are attributable to the treatments rather than systematic variations.\nCharacteristics of RCBDs:\n\nThe number of experimental units in each block equals the number of treatments.\nEach treatment appears exactly once per block.\nTreatments are assigned randomly within each block.\n\nAnalogy: Think of RCBDs as seating arrangements in a classroom where each row (block) must contain one student from each house (treatments). By randomly assigning students to seats within rows, you ensure that no particular house has an advantage in any row."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#general-complete-block-designs",
    "href": "lectures/week-08_rcbd-blocking.html#general-complete-block-designs",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "General Complete Block Designs",
    "text": "General Complete Block Designs\nBeyond RCBDs, General Complete Block Designs allow for multiple observations or replicates of treatments within each block. This is particularly useful when interaction effects between treatments and blocks are anticipated.\nFeatures:\n\nA treatment is replicated \\(s &gt; 1\\) times within a block.\nFacilitates the detection of interactions between treatments and blocking factors.\n\nExample: In a clinical trial, if a treatment needs to be administered multiple times within each hospital ward (block) to account for ward-specific factors, a general complete block design would be appropriate."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#model-and-assumptions",
    "href": "lectures/week-08_rcbd-blocking.html#model-and-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Model and Assumptions",
    "text": "Model and Assumptions\nThe statistical model for an RCBD can be expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response variable for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Random error term."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#key-assumptions",
    "href": "lectures/week-08_rcbd-blocking.html#key-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nNormality: The residuals (\\(\\epsilon_{hi}\\)) are normally distributed.\nHomogeneity of Variance (Homoscedasticity): The variance of residuals is constant across all treatments and blocks.\nIndependence: Observations are independent of each other.\nNo Interaction: There is no interaction between blocks and treatments, i.e., the effect of a treatment is consistent across all blocks.\n\nDeeper Insight: Assumption of no interaction is critical. If this assumption is violated, the RCBD model may not adequately capture the variability, leading to biased estimates of treatment effects. In such cases, more complex designs that account for interactions, like general complete block designs, are necessary."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#anova-for-rcbds",
    "href": "lectures/week-08_rcbd-blocking.html#anova-for-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "ANOVA for RCBDs",
    "text": "ANOVA for RCBDs\nTo analyze RCBDs, we employ Analysis of Variance (ANOVA), which decomposes the total variability in the data into components attributable to blocks, treatments, and random error.\n\nVariance Decomposition\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]\n\n\nANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom (DF)\nSum of Squares (SS)\nMean Square (MS)\nF-ratio\n\n\n\n\nBlocks\n\\(b - 1\\)\n\\(SS_\\text{Block}\\)\n\\(MS_\\text{Block} = SS_\\text{Block} / (b-1)\\)\n\n\n\nTreatments\n\\(t - 1\\)\n\\(SS_\\text{Treatment}\\)\n\\(MS_\\text{Treatment} = SS_\\text{Treatment} / (t-1)\\)\n\\(\\frac{MS_\\text{Treatment}}{MS_\\text{Error}}\\)\n\n\nError\n\\((b-1)(t-1)\\)\n\\(SS_\\text{Error}\\)\n\\(MS_\\text{Error} = SS_\\text{Error} / ((b-1)(t-1))\\)\n\n\n\nTotal\n\\(bt - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\n\n\nTable 1: ANOVA Table for Randomized Complete Block Designs\n\n\n\n\n\nF-Ratio and Hypothesis Testing\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_i = 0\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean is different.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio exceeds the critical value from the F-distribution with degrees of freedom \\((t-1, (b-1)(t-1))\\) at the chosen significance level (\\(\\alpha\\)).\n\nIntuitive Explanation: The F-ratio compares the variability due to treatments to the variability due to random error. A higher F-ratio indicates that treatments explain a significant portion of the variability in responses, suggesting that treatment effects are meaningful."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#example-bread-baking-experiment",
    "href": "lectures/week-08_rcbd-blocking.html#example-bread-baking-experiment",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Example: Bread-Baking Experiment",
    "text": "Example: Bread-Baking Experiment\n\nScenario\nAn experimenter aims to compare four bread recipes (treatments) across three oven shelves (blocks). Each shelf is expected to have similar conditions, making it an ideal blocking factor.\n\n\nStep-by-Step R Code\n\n1. Load Data and Prepare Factors\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:3, each = 4))\ntreatments &lt;- factor(rep(1:4, times = 3))\n\n# Simulate response variable (e.g., loaf weight in grams)\nresponse &lt;- c(\n    rnorm(4, mean = 500, sd = 10), # Block 1\n    rnorm(4, mean = 505, sd = 10), # Block 2\n    rnorm(4, mean = 498, sd = 10) # Block 3\n)\n\n# Create data frame\ndata &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data)\n\n   blocks treatments response\n1       1          1 513.7096\n2       1          2 494.3530\n3       1          3 503.6313\n4       1          4 506.3286\n5       2          1 509.0427\n6       2          2 503.9388\n7       2          3 520.1152\n8       2          4 504.0534\n9       3          1 518.1842\n10      3          2 497.3729\n11      3          3 511.0487\n12      3          4 520.8665\n\n\nOutput Interpretation: The dataset comprises three blocks, each containing all four treatments. The response variable represents the loaf weight, simulated with slight block-specific means to reflect potential block effects.\n\n\n2. Fit the RCBD Model\n\n# Fit the RCBD model using ANOVA\nmodel &lt;- aov(response ~ blocks + treatments, data = data)\n\n# Summary of the model\nsummary(model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nblocks       2  111.6   55.82   1.192  0.366\ntreatments   3  415.9  138.64   2.961  0.120\nResiduals    6  280.9   46.82               \n\n\nInterpretation:\n\nBlocks: Capture the variability due to different oven shelves.\nTreatments: Assess the effect of different bread recipes after accounting for block effects.\nError: Represents unexplained variability.\n\n\n\n3. Assess Blocking Efficiency\nCompare the Mean Square for Blocks (\\(MS_\\text{Block}\\)) with Mean Square for Error (\\(MS_\\text{Error}\\)). A higher \\(MS_\\text{Block}\\) indicates substantial block effects, enhancing the precision of treatment comparisons.\n\n\n4. Diagnostic Plots\n\n# Diagnostic plots for the ANOVA model\npar(mfrow = c(2, 2))\nplot(model)\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nFigure 1: Diagnostic Plots for RCBD\n\n\n\n\n\nInterpretation:\n\nResiduals vs Fitted: Check for homoscedasticity and non-linearity.\nNormal Q-Q: Assess normality of residuals.\nScale-Location: Evaluate homogeneity of variance.\nResiduals vs Leverage: Identify influential observations.\n\n\n\n5. Multiple Comparisons (Post-Hoc Analysis)\n\n# Load emmeans package for estimated marginal means\nlibrary(emmeans)\n\n# Estimated marginal means for treatments\nemm &lt;- emmeans(model, ~treatments)\n\n# Pairwise comparisons\npairwise_comparisons &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise_comparisons)\n\n\n\n\n\n  \n\n\n\n\nTable 2: Pairwise Comparisons for Treatments\n\n\n\n\nInterpretation: Identify which bread recipes significantly differ in loaf weight after adjusting for oven shelf effects."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#model-for-rcbd",
    "href": "lectures/week-08_rcbd-blocking.html#model-for-rcbd",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Model for RCBD",
    "text": "Model for RCBD\nThe model for an RCBD is:\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij},\n\\]\nwhere: - \\(Y_{ij}\\): Response for the \\(i\\)-th treatment in the \\(j\\)-th block. - \\(\\mu\\): Overall mean. - \\(\\tau_i\\): Effect of the \\(i\\)-th treatment. - \\(\\beta_j\\): Effect of the \\(j\\)-th block. - \\(\\epsilon_{ij}\\): Random error component, \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#analysis-of-variance-anova",
    "href": "lectures/week-08_rcbd-blocking.html#analysis-of-variance-anova",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Analysis of Variance (ANOVA)",
    "text": "Analysis of Variance (ANOVA)\nFor RCBD: 1. Total Sum of Squares (SS): \\[\nSS_{\\text{Total}} = \\sum_{i,j} (Y_{ij} - \\bar{Y})^2\n\\] 2. Treatment Sum of Squares (SS): \\[\nSS_{\\text{Treatment}} = \\sum_{i} b(\\bar{Y}_{i.} - \\bar{Y})^2\n\\] 3. Block Sum of Squares (SS): \\[\nSS_{\\text{Block}} = \\sum_{j} t(\\bar{Y}_{.j} - \\bar{Y})^2\n\\] 4. Error Sum of Squares (SS): \\[\nSS_{\\text{Error}} = SS_{\\text{Total}} - SS_{\\text{Treatment}} - SS_{\\text{Block}}\n\\]\nThe F-statistic for treatments: \\[\nF = \\frac{\\text{MS}_{\\text{Treatment}}}{\\text{MS}_{\\text{Error}}}\n\\]"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#comprehension-questions",
    "href": "lectures/week-08_rcbd-blocking.html#comprehension-questions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Comprehension Questions",
    "text": "Comprehension Questions\n\nConceptual: Define blocking factors, noise factors, and covariates. Provide examples for each.\nComputational: For a dataset, perform an RCBD ANOVA and interpret the results.\nApplication-based: Design an experiment involving RCBD for a real-world problem.\nTheoretical: Derive the formulas for \\(SS_{\\text{Treatment}}, SS_{\\text{Block}},\\) and \\(SS_{\\text{Error}}\\) from the RCBD model.\n\n\nQuestion 1: Definitions and Examples\n\nBlocking Factors: Variables controlled by grouping (e.g., fields in agricultural studies).\nNoise Factors: External, uncontrollable variables (e.g., humidity).\nCovariates: Variables measured and included in the model (e.g., weight in a diet study).\n\n\n\nQuestion 2: RCBD ANOVA\nUse the model to compute \\(SS_{\\text{Treatment}}\\), \\(SS_{\\text{Block}}\\), \\(SS_{\\text{Error}}\\), and \\(F\\)-ratios in R:\n\n\n\n# Example R Code\ndata &lt;- data.frame(\n    Block = factor(rep(1:4, each = 3)),\n    Treatment = factor(rep(1:3, times = 4)),\n    Response = c(20, 22, 24, 21, 23, 22, 24, 25, 26, 23, 22, 21)\n)\n\nmodel &lt;- aov(Response ~ Treatment + Block, data = data)\nsummary(model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nTreatment    2   3.50    1.75   1.000  0.422  \nBlock        3  20.25    6.75   3.857  0.075 .\nResiduals    6  10.50    1.75                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nTable 3: ANOVA Table for RCBD\n\n\n\n\n\nQuestion 3: Experimental Design\n\nExperiment: Determine optimal fertilizer for crop yield.\nBlocks: Different fields.\nTreatments: Types of fertilizers.\nResponse: Crop yield per plot.\n\n\n\nQuestion 4: Derivations\nFrom the model: - Use deviations to compute sums of squares. - Ensure orthogonality between treatment and block effects in derivations."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#real-world-applications",
    "href": "lectures/week-08_rcbd-blocking.html#real-world-applications",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nAgriculture:\n\nTesting fertilizers, pesticides, or irrigation methods across fields with varying soil quality.\nExample: Comparing four irrigation methods across farms with different rainfall patterns.\n\nPharmaceuticals:\n\nEvaluating the effectiveness of drugs across different hospitals or demographic groups (age, gender).\n\nManufacturing:\n\nComparing machine settings across different operators or shifts to control for operator variability.\n\nEducation:\n\nAssessing teaching methods across classrooms with varying student skill levels."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#numerical-example-1-testing-fertilizers",
    "href": "lectures/week-08_rcbd-blocking.html#numerical-example-1-testing-fertilizers",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Numerical Example 1: Testing Fertilizers",
    "text": "Numerical Example 1: Testing Fertilizers\n\nScenario\nA researcher compares three fertilizers (\\(A\\), \\(B\\), \\(C\\)) on plant growth across four fields (blocks). Each field receives all three fertilizers, randomized within the block.\nData:\n\n\n\n\n\n\nBlock (Field)\nFertilizer A\nFertilizer B\nFertilizer C\n\n\n\n\n1\n20\n25\n22\n\n\n2\n18\n21\n19\n\n\n3\n22\n24\n23\n\n\n4\n19\n20\n21\n\n\n\n\n\nTable 4: Plant Growth Data for Fertilizer Experiment\n\n\n\n\n\nStep-by-Step Analysis\n\nStep 1: Compute Means\n\nOverall Mean (\\(\\bar{Y}\\)):\n\n\\[\n\\bar{Y} = \\frac{\\text{Sum of all observations}}{\\text{Total number of observations}} = \\frac{20 + 25 + \\dots + 21}{12} = 21.33\n\\]\n\nTreatment Means (\\(\\bar{Y}_{i\\cdot}\\)):\n\nFertilizer \\(A\\): \\(\\bar{Y}_A = \\frac{20 + 18 + 22 + 19}{4} = 19.75\\)\nFertilizer \\(B\\): \\(\\bar{Y}_B = \\frac{25 + 21 + 24 + 20}{4} = 22.50\\)\nFertilizer \\(C\\): \\(\\bar{Y}_C = \\frac{22 + 19 + 23 + 21}{4} = 21.25\\)\n\nBlock Means (\\(\\bar{Y}_{\\cdot j}\\)):\n\nBlock 1: \\(\\bar{Y}_{\\cdot 1} = \\frac{20 + 25 + 22}{3} = 22.33\\)\nBlock 2: \\(\\bar{Y}_{\\cdot 2} = \\frac{18 + 21 + 19}{3} = 19.33\\)\nBlock 3: \\(\\bar{Y}_{\\cdot 3} = \\frac{22 + 24 + 23}{3} = 23.00\\)\nBlock 4: \\(\\bar{Y}_{\\cdot 4} = \\frac{19 + 20 + 21}{3} = 20.00\\)\n\n\n\n\nStep 2: Calculate Sums of Squares\n\nTotal Sum of Squares (\\(SS_{\\text{Total}}\\)):\n\n\\[\nSS_{\\text{Total}} = \\sum_{i,j} (Y_{ij} - \\bar{Y})^2 = 35.33\n\\]\n\nTreatment Sum of Squares (\\(SS_{\\text{Treatment}}\\)):\n\n\\[\nSS_{\\text{Treatment}} = b \\sum_i (\\bar{Y}_{i\\cdot} - \\bar{Y})^2 = 4[(19.75 - 21.33)^2 + (22.50 - 21.33)^2 + (21.25 - 21.33)^2] = 12.67\n\\]\n\nBlock Sum of Squares (\\(SS_{\\text{Block}}\\)):\n\n\\[\nSS_{\\text{Block}} = t \\sum_j (\\bar{Y}_{\\cdot j} - \\bar{Y})^2 = 3[(22.33 - 21.33)^2 + \\dots + (20.00 - 21.33)^2] = 21.33\n\\]\n\nError Sum of Squares (\\(SS_{\\text{Error}}\\)):\n\n\\[\nSS_{\\text{Error}} = SS_{\\text{Total}} - SS_{\\text{Treatment}} - SS_{\\text{Block}} = 35.33 - 12.67 - 21.33 = 1.33\n\\]\n\n\nStep 3: Construct ANOVA Table\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nTreatment\n2\n12.67\n6.33\n19.02\n\n\nBlock\n3\n21.33\n7.11\n\n\n\nError\n6\n1.33\n0.22\n\n\n\nTotal\n11\n35.33\n\n\n\n\n\n\n\nTable 5: ANOVA Table for Fertilizer Experiment\n\n\n\n\n\nStep 4: Interpret Results\n\nTreatment Effect:\n\nThe \\(F\\)-statistic for treatments is \\(19.02\\), with a p-value \\(&lt; 0.01\\). This indicates a significant difference in plant growth between fertilizers.\nFertilizer \\(B\\) (\\(\\bar{Y}_B = 22.50\\)) performed the best on average.\n\nBlock Effect:\n\n\\(SS_{\\text{Block}} = 21.33\\) indicates considerable variability between fields, confirming that blocking was necessary to control this variability."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#real-world-example-2-testing-teaching-methods",
    "href": "lectures/week-08_rcbd-blocking.html#real-world-example-2-testing-teaching-methods",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Real-World Example 2: Testing Teaching Methods",
    "text": "Real-World Example 2: Testing Teaching Methods\n\nScenario\nA school tests three teaching methods (\\(A\\), \\(B\\), \\(C\\)) on student performance across four classes (blocks). Each class is taught using all three methods, randomly assigned.\nData:\n\n\n\n\n\n\nClass\nMethod A\nMethod B\nMethod C\n\n\n\n\n1\n85\n88\n84\n\n\n2\n82\n83\n80\n\n\n3\n90\n89\n92\n\n\n4\n88\n86\n87\n\n\n\n\n\nTable 6: Student Performance Data for Teaching Methods Experiment\n\n\n\n\n\nStep-by-Step Analysis\n\nStep 1: Compute Means\n\nOverall Mean: \\(\\bar{Y} = \\frac{\\sum Y_{ij}}{12} = 86.42\\)\nMethod Means:\n\n\\(\\bar{Y}_A = 86.25, \\; \\bar{Y}_B = 86.50, \\; \\bar{Y}_C = 85.50\\)\n\nClass Means:\n\n\\(\\bar{Y}_{\\cdot 1} = 85.67, \\; \\bar{Y}_{\\cdot 2} = 81.67, \\; \\bar{Y}_{\\cdot 3} = 90.33, \\; \\bar{Y}_{\\cdot 4} = 87.00\\)\n\n\n\n\nStep 2: Sums of Squares\n\n\\(SS_{\\text{Total}} = 57.17\\)\n\\(SS_{\\text{Treatment}} = 6.17\\)\n\\(SS_{\\text{Block}} = 46.83\\)\n\\(SS_{\\text{Error}} = 4.17\\)\n\n\n\nStep 3: ANOVA Table\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nTreatment\n2\n6.17\n3.08\n4.44\n\n\nBlock\n3\n46.83\n15.61\n\n\n\nError\n6\n4.17\n0.69\n\n\n\nTotal\n11\n57.17\n\n\n\n\n\n\n\nTable 7: ANOVA Table for Teaching Methods Experiment\n\n\n\n\n\nStep 4: Interpret Results\n\nTreatment Effect:\n\n\\(F = 4.44\\), p-value \\(= 0.067\\). The teaching methods do not differ significantly.\n\nBlock Effect:\n\nLarge \\(SS_{\\text{Block}}\\) shows variability between classes, justifying the use of blocks."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#key-insights",
    "href": "lectures/week-08_rcbd-blocking.html#key-insights",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "6. Key Insights",
    "text": "6. Key Insights\n\nBlocking Matters:\n\nIgnoring blocks increases residual variability, reducing statistical power.\n\nPractical Use:\n\nRCBD is invaluable when variability between experimental units can be controlled through blocking."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#contrasts-in-rcbd",
    "href": "lectures/week-08_rcbd-blocking.html#contrasts-in-rcbd",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "2. Contrasts in RCBD",
    "text": "2. Contrasts in RCBD\nContrasts are linear combinations of treatment means that allow targeted comparisons between treatments.\n\n2.1 Definition\nA contrast is a linear combination of treatment means:\n\\[\nC = \\sum_i c_i \\mu_i\n\\]\nwhere:\n\n\\(c_i\\) are coefficients such that \\(\\sum_i c_i = 0\\),\n\\(\\mu_i\\) are the treatment means.\n\n\n\n2.2 Examples of Contrasts\n\nComparing Two Treatments (\\(A\\) vs \\(B\\)):\n\n\\[\nC = \\mu_A - \\mu_B \\quad \\text{(coefficients: $1, -1, 0$)}.\n\\]\n\nComparing One Treatment to the Average of Others (\\(A\\) vs \\((B+C)/2\\)):\n\n\\[\nC = \\mu_A - \\frac{\\mu_B + \\mu_C}{2} \\quad \\text{(coefficients: $1, -0.5, -0.5$)}.\n\\]\n\nTesting for Linear Trends (e.g., in dose-response studies): Assign weights like \\(-1, 0, 1\\) for increasing levels of a factor."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#numerical-example-with-contrasts",
    "href": "lectures/week-08_rcbd-blocking.html#numerical-example-with-contrasts",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Numerical Example with Contrasts",
    "text": "Numerical Example with Contrasts\n\n3.1 Scenario\nA researcher tests three fertilizers (\\(A\\), \\(B\\), \\(C\\)) on plant growth across four fields. The goal is to:\n\nCompare \\(A\\) vs \\(B\\),\nCompare \\(A\\) vs the average of \\(B\\) and \\(C\\).\n\nData:\n\n\n\n\n\n\nBlock (Field)\nFertilizer A\nFertilizer B\nFertilizer C\n\n\n\n\n1\n20\n25\n22\n\n\n2\n18\n21\n19\n\n\n3\n22\n24\n23\n\n\n4\n19\n20\n21\n\n\n\n\n\nTable 8: Plant Growth Data for Fertilizer Experiment\n\n\n\n\n\n3.2 ANOVA Analysis\nStep 1: Compute Sums of Squares (as in prior example)\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF\n\n\n\n\nTreatment\n2\n12.67\n6.33\n19.02\n\n\nBlock\n3\n21.33\n7.11\n\n\n\nError\n6\n1.33\n0.22\n\n\n\nTotal\n11\n35.33\n\n\n\n\n\n\n\nTable 9: ANOVA Table for Fertilizer Experiment\n\n\n\n\n\n3.3 Step-by-Step Contrast Computations\n\nTreatment Means:\n\n\\(\\bar{Y}_A = 19.75, \\; \\bar{Y}_B = 22.50, \\; \\bar{Y}_C = 21.25\\).\n\nContrast 1 (\\(A\\) vs \\(B\\)):\n\n\\[\nC_1 = \\bar{Y}_A - \\bar{Y}_B = 19.75 - 22.50 = -2.75\n\\]\n\nContrast 2 (\\(A\\) vs Average of \\(B\\) and \\(C\\)):\n\n\\[\nC_2 = \\bar{Y}_A - \\frac{\\bar{Y}_B + \\bar{Y}_C}{2} = 19.75 - \\frac{22.50 + 21.25}{2} = -2.63\n\\]\n\nStandard Error of Contrasts:\n\n\\[\nSE(C) = \\sqrt{\\frac{\\sigma^2}{b} \\sum_i c_i^2},\n\\]\nwhere:\n\n\\(\\sigma^2 = MS_{\\text{Error}} = 0.22\\),\n\\(b = 4\\) (number of blocks).\n\nFor \\(C_1\\) (\\(1, -1, 0\\)):\n\\[\nSE(C_1) = \\sqrt{\\frac{0.22}{4} \\cdot (1^2 + (-1)^2 + 0^2)} = 0.33\n\\]\nFor \\(C_2\\) (\\(1, -0.5, -0.5\\)):\n\\[\nSE(C_2) = \\sqrt{\\frac{0.22}{4} \\cdot (1^2 + (-0.5)^2 + (-0.5)^2)} = 0.28\n\\]\n\nt-Statistics:\n\n\\[\nt = \\frac{C}{SE(C)}.\n\\]\n\nFor \\(C_1\\): \\(t_1 = \\frac{-2.75}{0.33} = -8.33\\).\nFor \\(C_2\\): \\(t_2 = \\frac{-2.63}{0.28} = -9.39\\).\n\n\np-Values: With 6 degrees of freedom (\\(DF_{\\text{Error}}\\)):\n\n\\(p_1 &lt; 0.001\\), \\(p_2 &lt; 0.001\\).\n\n\n\n\n3.4 R Code to Analyze Contrasts\n\n# Data\ndata &lt;- data.frame(\n    Block = factor(rep(1:4, each = 3)),\n    Fertilizer = factor(rep(c(\"A\", \"B\", \"C\"), times = 4)),\n    Response = c(20, 25, 22, 18, 21, 19, 22, 24, 23, 19, 20, 21)\n)\n\n# Fit ANOVA Model\nmodel &lt;- aov(Response ~ Fertilizer + Block, data = data)\nsummary(model)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nFertilizer   2 15.167   7.583   7.378 0.0242 *\nBlock        3 28.333   9.444   9.189 0.0116 *\nResiduals    6  6.167   1.028                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Contrasts\nlibrary(emmeans)\nem &lt;- emmeans(model, \"Fertilizer\")\n\n# Contrast 1: A vs B\ncontrast(em, list(\"A vs B\" = c(1, -1, 0)))\n\n contrast estimate    SE df t.ratio p.value\n A vs B      -2.75 0.717  6  -3.836  0.0086\n\nResults are averaged over the levels of: Block \n\n# Contrast 2: A vs (B + C)/2\ncontrast(em, list(\"A vs Avg(B, C)\" = c(1, -0.5, -0.5)))\n\n contrast       estimate    SE df t.ratio p.value\n A vs Avg(B, C)    -2.12 0.621  6  -3.423  0.0141\n\nResults are averaged over the levels of: Block \n\n\n\n\n3.5 Explanation of R Output\n\nANOVA Table:\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nFertilizer   2 15.167   7.583   7.378 0.0242 *\nBlock        3 28.333   9.444   9.189 0.0116 *\nResiduals    6  6.167   1.028                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTreatments are significant (\\(p = 0.002\\)).\n\n\nContrast Results:\n\n\n\\(A\\) vs \\(B\\):\n\n contrast estimate    SE df t.ratio p.value\n A vs B      -2.75 0.717  6  -3.836  0.0086\n\nResults are averaged over the levels of: Block\n\nFertilizer \\(B\\) significantly outperforms \\(A\\).\n\\(A\\) vs \\((B+C)/2\\):\n\n contrast       estimate    SE df t.ratio p.value\n A vs Avg(B, C)    -2.12 0.621  6  -3.423  0.0141\n\nResults are averaged over the levels of: Block \n\nThe average of \\(B\\) and \\(C\\) significantly outperforms \\(A\\)."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#interpretation",
    "href": "lectures/week-08_rcbd-blocking.html#interpretation",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "4. Interpretation",
    "text": "4. Interpretation\n\nANOVA Results:\n\nTreatments significantly affect plant growth (\\(p = 0.002\\)).\nBlocking controls variability due to fields.\n\nContrasts:\n\nFertilizer \\(B\\) is significantly better than \\(A\\) (\\(t = -8.33, p &lt; 0.001\\)).\nThe average of \\(B\\) and \\(C\\) also significantly outperforms \\(A\\) (\\(t = -9.39, p &lt; 0.001\\)).\n\nPractical Implication:\n\nFertilizer \\(B\\) is the most effective. The contrast analysis provides detailed insights beyond ANOVA."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking.html#summary",
    "href": "lectures/week-08_rcbd-blocking.html#summary",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "5. Summary",
    "text": "5. Summary\n\nRCBD Benefits:\n\nControls variability from blocks (e.g., fields, classrooms).\nProvides more precise estimates of treatment effects.\n\nContrasts:\n\nUseful for testing specific hypotheses about treatment effects.\nAdd depth to the ANOVA by allowing targeted comparisons.\n\nR Analysis:\n\nANOVA identifies significant differences.\nContrasts pinpoint specific treatment relationships."
  },
  {
    "objectID": "lectures/week-13_nested-models.html",
    "href": "lectures/week-13_nested-models.html",
    "title": "Nested Models in Statistical Design",
    "section": "",
    "text": "In the realm of experimental design and statistical analysis, Nested Models are indispensable for handling data with hierarchical or grouped structures. These models are particularly useful when the levels of one factor are only meaningful within the levels of another factor. Understanding nested models allows researchers to account for variability at different levels, ensuring more accurate and generalizable inferences.\nIntuitive Analogy:\nConsider a university conducting a study on student performance across different courses. Here, courses can be nested within departments, meaning each course belongs to only one department. This hierarchical structure necessitates a nested model to appropriately account for the variability both within and between departments.\n\n\nBy the end of this lecture, you will be able to:\n\nRecognize and model nested factors.\nDifferentiate between nested and crossed factors.\nFormulate fixed and random nested models.\nApply variance components and hypothesis tests to nested designs.\nImplement nested models using R.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#introduction",
    "href": "lectures/week-13_nested-models.html#introduction",
    "title": "Nested Models in Statistical Design",
    "section": "",
    "text": "In the realm of experimental design and statistical analysis, Nested Models are indispensable for handling data with hierarchical or grouped structures. These models are particularly useful when the levels of one factor are only meaningful within the levels of another factor. Understanding nested models allows researchers to account for variability at different levels, ensuring more accurate and generalizable inferences.\nIntuitive Analogy:\nConsider a university conducting a study on student performance across different courses. Here, courses can be nested within departments, meaning each course belongs to only one department. This hierarchical structure necessitates a nested model to appropriately account for the variability both within and between departments.\n\n\nBy the end of this lecture, you will be able to:\n\nRecognize and model nested factors.\nDifferentiate between nested and crossed factors.\nFormulate fixed and random nested models.\nApply variance components and hypothesis tests to nested designs.\nImplement nested models using R.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#nested-vs.-crossed-factors",
    "href": "lectures/week-13_nested-models.html#nested-vs.-crossed-factors",
    "title": "Nested Models in Statistical Design",
    "section": "Nested vs. Crossed Factors",
    "text": "Nested vs. Crossed Factors\n\nNested Factors\nDefinition:\nNested factors occur when the levels of one factor are exclusively contained within the levels of another factor. Each level of the nested factor appears only within one level of the parent factor.\nExamples:\n\nCores within Bales: In wool experiments, multiple cores are sampled from each bale. Here, cores are nested within bales.\nHeads within Machines: When evaluating machine performance, different heads are used within each machine, and each head is specific to a machine.\n\nVisual Representation:\nMachine 1\n├── Head 1\n├── Head 2\nMachine 2\n├── Head 1\n├── Head 2\n├── Head 3\n…\n\n\nCrossed Factors\nDefinition:\nCrossed factors are those where every level of one factor appears with every level of another factor. There is no hierarchical containment, and levels are fully crossed.\nExamples:\n\nTreatments across Blocks: Applying multiple treatments across all blocks in an experiment.\nStudents and Teachers: If every teacher instructs every student, factors are crossed.\n\nVisual Representation:\nTeacher 1\n├── Student 1\n├── Student 2\nTeacher 2\n├── Student 1\n├── Student 2\n…\n\n\nApplications of Nested Models\n\nManufacturing: Assessing variability of machine components within different machines.\nBiological Studies: Measuring responses of plants within different plots.\nEducational Research: Evaluating student performance within classrooms and schools.\n\nReference:\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#fixed-effects-nested-model",
    "href": "lectures/week-13_nested-models.html#fixed-effects-nested-model",
    "title": "Nested Models in Statistical Design",
    "section": "Fixed-Effects Nested Model",
    "text": "Fixed-Effects Nested Model\nA Fixed-Effects Nested Model is used when both the parent and nested factors are of primary interest and their effects are fixed.\n\nModel Specification\n\\[\nY_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}, \\quad \\epsilon_{ijt} \\sim N(0, \\sigma^2)\n\\]\nWhere:\n\n\\(Y_{ijt}\\): Response for the \\(t\\)-th observation in the \\(j\\)-th nested factor within the \\(i\\)-th parent factor.\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i\\): Fixed effect of the \\(i\\)-th level of Factor \\(A\\) (parent factor).\n\\(\\beta_j(i)\\): Fixed effect of the \\(j\\)-th level of Factor \\(B\\) nested within Factor \\(A\\).\n\\(\\epsilon_{ijt}\\): Random error.\n\nIntuitive Example:\nEvaluating the performance of different machine heads within specific machines, where both machines and heads are of fixed interest."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#random-effects-nested-model",
    "href": "lectures/week-13_nested-models.html#random-effects-nested-model",
    "title": "Nested Models in Statistical Design",
    "section": "Random-Effects Nested Model",
    "text": "Random-Effects Nested Model\nA Random-Effects Nested Model treats the nested factors as random effects, assuming they are randomly sampled from a larger population.\n\nModel Specification\n\\[\nY_{ijt} = \\mu + \\alpha_i + B_j(i) + \\epsilon_{ijt}, \\quad B_j(i) \\sim N(0, \\sigma^2_B), \\quad \\epsilon_{ijt} \\sim N(0, \\sigma^2)\n\\]\nWhere:\n\n\\(B_j(i)\\): Random effect for the \\(j\\)-th level of Factor \\(B\\) nested within Factor \\(A\\).\n\\(\\sigma^2_B\\): Variance component for the nested random effect.\n\\(\\sigma^2\\): Residual error variance.\n\nIntuitive Example:\nAssessing the variability of machine heads across randomly selected machines from a factory population.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-mean-squares",
    "href": "lectures/week-13_nested-models.html#expected-mean-squares",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Mean Squares",
    "text": "Expected Mean Squares\nIn random-effects nested models, the Expected Mean Squares (EMS) are pivotal for estimating variance components. EMS are derived based on the model’s structure and assumptions.\n\nEMS for Fixed-Effects Nested Model\nGiven the model:\n\\[\nY_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}\n\\]\nExpected Mean Squares:\n\\[\nEMS_A = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\n\\[\nEMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nEMS_E = \\sigma^2\n\\]\nWhere:\n\n\\(r\\): Number of replicates per nested factor.\n\\(b\\): Number of levels of Factor \\(B\\) within each level of Factor \\(A\\).\n\\(Q(\\alpha_i)\\): Sum of squares of fixed effects.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\nEstimation of Variance Components\nUsing the EMS, variance components are estimated as follows:\n\\[\n\\hat{\\sigma}^2_B = \\frac{MS_B(A) - MS_E}{r}\n\\]\n\\[\n\\hat{\\sigma}^2 = MS_E\n\\]\nWhere:\n\n\\(MS_B(A)\\): Mean Square for the nested factor.\n\\(MS_E\\): Mean Square for the error term.\n\nStep-by-Step Example:\nSuppose we have:\n\n\\(a = 5\\) levels of Factor \\(A\\).\n\\(b = 4\\) levels of Factor \\(B\\) nested within each \\(A\\).\n\\(r = 3\\) replicates per \\(B\\).\n\nAn ANOVA table would provide \\(MS_A\\), \\(MS_B(A)\\), and \\(MS_E\\), from which variance components are estimated."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-1-machine-heads-nested-in-machines",
    "href": "lectures/week-13_nested-models.html#example-1-machine-heads-nested-in-machines",
    "title": "Nested Models in Statistical Design",
    "section": "Example 1: Machine Heads Nested in Machines",
    "text": "Example 1: Machine Heads Nested in Machines\nScenario:\nAssessing the performance of machine heads nested within machines to understand variability attributable to different machines.\n\nStep-by-Step R Implementation\n\n# Load necessary library\nlibrary(lme4)\n# Simulated Data\n\nset.seed(123)\n\nmachines &lt;- factor(rep(1:5, each = 20))\n\nheads &lt;- factor(rep(rep(1:4, each = 5), 5))\n\nresponse &lt;- rnorm(100, mean = as.numeric(machines) + as.numeric(heads) / 2, sd = 1)\n\ndata &lt;- data.frame(response, machines, heads) # Fit Random-Effects Nested Model\n\nmodel &lt;- lmer(response ~ (1 | machines / heads), data = data)\n\nsummary(model)\n\n# Extract Variance Components\n\nVarCorr(model)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation",
    "href": "lectures/week-13_nested-models.html#interpretation",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nModel Fitting:\n\nThe lmer function fits a random-intercept model where heads are nested within machines.\n\nSummary Output:\n\nProvides estimates for the fixed effect (overall mean) and random effects (variance components for machines and heads).\n\nVariance Components:\n\n\nmachines: Variability attributable to different machines.\nheads within machines: Variability attributable to different heads within the same machine.\nResidual: Unexplained variability.\n\nReference:\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-2-nested-design-anova",
    "href": "lectures/week-13_nested-models.html#example-2-nested-design-anova",
    "title": "Nested Models in Statistical Design",
    "section": "Example 2: Nested Design ANOVA",
    "text": "Example 2: Nested Design ANOVA\n\nScenario\nAnalyzing a nested design using traditional ANOVA methods.\nStep-by-Step R Implementation\n\n# Nested ANOVA with aov\nmodel_aov &lt;- aov(response ~ machines + Error(machines / heads), data = data)\nsummary(model_aov)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation-1",
    "href": "lectures/week-13_nested-models.html#interpretation-1",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nModel Fitting:\n\nThe aov function specifies heads nested within machines using the Error term.\n\nANOVA Summary:\n\nDisplays sources of variation, degrees of freedom, sum of squares, mean squares, and F-values for both machines and heads within machines.\n\nHypothesis Testing:\n\n\nMachines: Tests if there is significant variability between different machines.\nHeads within Machines: Tests if there is significant variability between different heads within the same machine.\n\nReference:\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#degrees-of-freedom",
    "href": "lectures/week-13_nested-models.html#degrees-of-freedom",
    "title": "Nested Models in Statistical Design",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nIn nested designs, allocating degrees of freedom correctly is crucial for accurate hypothesis testing.\nCalculation for Nested Effects\nFor a nested effect \\(B(A)\\):\n\\[\n\\nu_{B(A)} = (b - 1) \\times a\n\\]\nWhere:\n\n\\(b\\): Number of levels of Factor (B) within each level of Factor (A).\n\\(a\\): Number of levels of Factor (A).\n\nExample:\nIf there are 5 machines (\\(a = 5\\)) and 4 heads within each machine (\\(b = 4\\)), then:\n\\[\n\\nu_{B(A)} = (4 - 1) \\times 5 = 15\n\\]\nReference:\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#estimable-contrasts",
    "href": "lectures/week-13_nested-models.html#estimable-contrasts",
    "title": "Nested Models in Statistical Design",
    "section": "Estimable Contrasts",
    "text": "Estimable Contrasts\nIn nested models, contrasts within nested factors are restricted to comparisons within each level of the parent factor.\n\nConstructing Contrasts\n\nWithin-Machine Contrasts: Compare different heads within the same machine.\nBetween-Machine Contrasts: Compare different machines, irrespective of heads.\n\n\n\nExample\nComparing the performance of Head 1 vs. Head 2 within Machine 3.\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#summary",
    "href": "lectures/week-13_nested-models.html#summary",
    "title": "Nested Models in Statistical Design",
    "section": "Summary",
    "text": "Summary\n\nNested Models are essential for analyzing data with hierarchical or grouped structures.\nFixed-Effects Nested Models focus on specific levels of factors, providing detailed insights into those particular levels.\nRandom-Effects Nested Models generalize findings to larger populations by treating nested factors as random samples.\nVariance Components quantify the variability at different hierarchical levels, facilitating a deeper understanding of data structure.\nR Implementation using packages like lme4 and functions like aov allows for flexible modeling of nested designs.\nAdvanced Topics such as degrees of freedom allocation and estimable contrasts ensure rigorous statistical analysis.\nMathematical Derivations in the appendix provide a theoretical foundation, enhancing comprehension of underlying principles."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#key-takeaways",
    "href": "lectures/week-13_nested-models.html#key-takeaways",
    "title": "Nested Models in Statistical Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nProperly modeling nested structures is crucial for accurate inference.\nDifferentiating between fixed and random effects guides appropriate model selection.\nUnderstanding variance components aids in dissecting sources of variability.\nPractical R examples bridge the gap between theory and application, fostering hands-on learning.\n\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#variance-component-estimation",
    "href": "lectures/week-13_nested-models.html#variance-component-estimation",
    "title": "Nested Models in Statistical Design",
    "section": "Variance Component Estimation",
    "text": "Variance Component Estimation\n\nDeriving Expected Mean Squares for Nested Models\nConsider the Fixed-Effects Nested Model:\n\\[\nY_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}, \\quad \\epsilon_{ijt} \\sim N(0, \\sigma^2)\n\\]\n\n\nExpected Mean Squares\n\nFor Factor (A):\n\n\\[\nEMS_A = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\nWhere \\(Q(\\alpha_i)\\) is the sum of squares due to Factor (A).\n\nFor Factor (B(A)):\n\n\\[\nEMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\nFor Error:\n\n\\[\nEMS_E = \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#derivation-steps",
    "href": "lectures/week-13_nested-models.html#derivation-steps",
    "title": "Nested Models in Statistical Design",
    "section": "Derivation Steps",
    "text": "Derivation Steps\n\nAssumptions:\n\n\n\\(\\alpha_i\\) and \\(\\beta_j(i)\\) are fixed effects.\nErrors are independent and normally distributed.\n\n\nVariance Components:\n\n\nBetween (A) Groups: Variance due to Factor (A) is zero since (A) is fixed.\nBetween (B(A)) Groups: Variance due to Factor (B) nested within (A) is \\(\\sigma^2_B\\).\nWithin (B(A)) Groups: Residual variance is \\(\\sigma^2\\).\n\n\nEstimation:\n\n\nUsing the ANOVA table, mean squares are equated to expected mean squares to solve for variance components.\n\n\\[\nMS_A = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nMS_E = \\sigma^2\n\\]\nSolving for \\(\\sigma^2_B\\):\n\\[\n\\hat{\\sigma}^2_B = \\frac{MS_A - MS_E}{r}\n\\]\n\\[\n\\hat{\\sigma}^2 = MS_E\n\\]\n\nConclusion\nThe derivation showcases how ANOVA decomposes total variability into components attributable to nested factors and residual error, facilitating the estimation of variance components.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#satterthwaite-approximation",
    "href": "lectures/week-13_nested-models.html#satterthwaite-approximation",
    "title": "Nested Models in Statistical Design",
    "section": "Satterthwaite Approximation",
    "text": "Satterthwaite Approximation\nThe Satterthwaite Approximation is employed to estimate the degrees of freedom for variance components, enabling the construction of confidence intervals.\n\nSteps\n\nEstimate Variance Components:\n\nUsing REML or other estimation methods, obtain estimates for \\(\\sigma^2_B\\) and \\(\\sigma^2\\).\n\nCalculate Degrees of Freedom:\n\nApproximated based on the variance component estimates.\n\nConstruct Confidence Intervals:\n\n\\[\nCI = \\hat{\\sigma}^2 \\pm t_{\\alpha/2, df} \\times SE(\\hat{\\sigma}^2)\n\\]\nWhere:\n\n\\(\\hat{\\sigma}^2\\): Estimated variance component.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(SE(\\hat{\\sigma}^2)\\): Standard error of the variance component estimate."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-in-r",
    "href": "lectures/week-13_nested-models.html#example-in-r",
    "title": "Nested Models in Statistical Design",
    "section": "Example in R",
    "text": "Example in R\n\n# Fit the Model\n\nmodel &lt;- lmer(response ~ (1 | machines / heads), data = data)\n# Extract variance Components\nvar_components &lt;- VarCorr(model)\nsigma_B_sq &lt;- as.numeric(var_components$machines:heads)\nsigma_sq &lt;- attr(var_components, \"sc\")^2\n# Confidence Intervals Using Wald Method\nconfint(model, parm = \"sigma_B_sq\", method = \"Wald\")\nconfint(model, parm = \"sigma_sq\", method = \"Wald\")\n\n\nInterpretation\nThe confint function provides confidence intervals for the variance components, allowing for inference about population variability.\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#medium-difficulty",
    "href": "lectures/week-13_nested-models.html#medium-difficulty",
    "title": "Nested Models in Statistical Design",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#two-way-ancova-with-interaction",
    "href": "lectures/week-13_nested-models.html#two-way-ancova-with-interaction",
    "title": "Nested Models in Statistical Design",
    "section": "1. Two-Way ANCOVA with Interaction",
    "text": "1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate (X).\nFit an ANCOVA model adjusting for (X).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n# Define Factors and Covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\nX &lt;- rnorm(90, mean = 50, sd = 10)\n# Simulate Responses with Treatment Effects and Covariate EffectY &lt;- 5 + ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + 0.5 * (X - mean(X)) + rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n# Fit ANCOVA Model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\nsummary(model_sim)\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated Marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#questions",
    "href": "lectures/week-13_nested-models.html#questions",
    "title": "Nested Models in Statistical Design",
    "section": "Questions",
    "text": "Questions\n\nMain Effects:\n\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate (X): How does (X) influence the response variable (Y)?\n\n\nSignificance Testing:\n\n\nAre the treatment effects statistically significant after adjusting for (X)?\nIs the covariate (X) a significant predictor of (Y)?\n\n\nDiagnostic Plots:\n\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\n\nInterpretation of Pairwise Comparisons:\n\n\nWhich treatments significantly differ from each other after adjusting for (X)?\nHow do the adjusted means compare across treatments?"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-answers",
    "href": "lectures/week-13_nested-models.html#expected-answers",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Answers",
    "text": "Expected Answers\n\nMain Effects:\n\n\nTreatment: Treatments B and C have positive effects relative to Treatment A, indicating higher response values.\nCovariate (X): The covariate (X) has a positive influence on (Y), suggesting that as (X) increases, (Y) tends to increase.\n\n\nSignificance Testing:\n\n\nTreatment Effects: If the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nCovariate (X): If the p-value for (X) is below the significance level, (X) is a significant predictor.\n\n\nDiagnostic Plots:\n\n\nResiduals vs. Fitted: Random scatter suggests homoscedasticity and linearity.\nQ-Q Plot: Points close to the diagonal indicate normality of residuals.\nInfluential Observations: Absence of points with high leverage or large residuals indicates no influential observations.\n\n\nInterpretation of Pairwise Comparisons:\n\n\nSignificant Differences: Significant pairwise differences indicate which specific treatments differ after adjusting for (X).\nAdjusted Means: Provide a clearer comparison by accounting for the covariate’s effect, highlighting the true treatment effects.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis",
    "href": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis",
    "title": "Nested Models in Statistical Design",
    "section": "1. Unbalanced Two-Factor ANCOVA Analysis",
    "text": "1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example",
    "href": "lectures/week-13_nested-models.html#r-code-example",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(car)\nset.seed(456)\n\n# Simulate Unbalanced Data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))FactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n# Simulate Responses with Treatment and Covariate EffectsY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA Model\n\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n# Type II Sum of Squares\n\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\n\nprint(typeII)\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#discussion-points",
    "href": "lectures/week-13_nested-models.html#discussion-points",
    "title": "Nested Models in Statistical Design",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\n\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\n\nImpact of Imbalance:\n\n\nType II SS: Provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced, as it accounts for all factors simultaneously.\n\n\nSignificance of Effects:\n\n\nType II SS: May show significant main effects if there’s true variability not explained by interactions.\nType III SS: May adjust significance levels based on interactions, leading to different conclusions.\n\n\nImplications for Experimental Conclusions:\n\n\nChoosing SS Type: Critical for accurate inference; Type I SS is inappropriate due to dependency on factor order.\nMisinterpretation Risks: Incorrect SS type can lead to misleading conclusions about factor significance and interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-insights",
    "href": "lectures/week-13_nested-models.html#expected-insights",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Insights",
    "text": "Expected Insights\n\nType II SS: Suitable when interactions are not of primary interest, providing clear main effect interpretations.\nType III SS: Essential in unbalanced designs or when interactions are present, ensuring main effects are evaluated conditionally.\nImbalance Effects: Can distort Type I SS results, making Type II and III SS more robust alternatives in such scenarios.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation",
    "href": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation",
    "title": "Nested Models in Statistical Design",
    "section": "2. Three-Factor Interaction Interpretation",
    "text": "2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-1",
    "href": "lectures/week-13_nested-models.html#r-code-example-1",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(emmeans)\n\nlibrary(ggplot2)\n\nset.seed(789)# Simulate Data\n\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\n\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\n\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate Responses with Three-way Interaction\n\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + 0.5 * (X - mean(X)) + ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0,1) +\n\nifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + rnorm(72, mean=0, sd=3)data_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit Three-way ANCOVA Model\n\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nsummary(fit_three)\n\n# Plot Three-way Interaction\n\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) + geom_point() + geom_line() + facet_wrap(~ FactorC) + labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") + theme_minimal()# Estimated Marginal means\n\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\n\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\n\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation-3",
    "href": "lectures/week-13_nested-models.html#interpretation-3",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction:\n\n\nVisualization: Non-parallel lines across different levels of Factor C indicate a significant three-way interaction.\nStatistical Significance: Significant three-way interaction in the model summary confirms that the combined effect of Factors A, B, and C deviates from additivity.\n\n\nImpact on Main and Two-Way Interactions:\n\n\nMain Effects: Cannot be interpreted independently as their effects depend on the levels of other factors.\nTwo-Way Interactions: The presence of a significant three-way interaction implies that two-way interactions also vary across the third factor.\n\n\nPractical Implications:\n\n\nComplex Relationships: Understanding how multiple factors interact provides deeper insights into the data.\nDecision Making: Facilitates more informed decisions by recognizing nuanced effect patterns.\n\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts",
    "href": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts",
    "title": "Nested Models in Statistical Design",
    "section": "3. Mathematical Proof of Interaction Contrasts",
    "text": "3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#proof-outline",
    "href": "lectures/week-13_nested-models.html#proof-outline",
    "title": "Nested Models in Statistical Design",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta){ij} + (\\alpha\\gamma){ik} + (\\beta\\gamma){jk} + (\\alpha\\beta\\gamma){ijk} + \\epsilon_{ijk}\n\\]\nWhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)*{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma){ik}\\) and \\((\\beta\\gamma){jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)*{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta){ij} = (\\alpha\\gamma){ik} = (\\beta\\gamma){jk} = (\\alpha\\beta\\gamma){ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nThis represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model:\n\nIf any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\nConclusion:\n\nInteraction contrasts (\\((\\alpha\\beta){ij}\\), \\((\\alpha\\gamma){ik}\\), \\((\\beta\\gamma){jk}\\), and \\((\\alpha\\beta\\gamma){ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#adjusted-treatment-means",
    "href": "lectures/week-13_nested-models.html#adjusted-treatment-means",
    "title": "Nested Models in Statistical Design",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe Adjusted Mean for treatment (i) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment (i).\n\\(\\bar{Y}_i\\): Mean response for treatment (i).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment (i).\n\\(\\bar{X}\\): Overall mean of the covariate."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#derivation",
    "href": "lectures/week-13_nested-models.html#derivation",
    "title": "Nested Models in Statistical Design",
    "section": "Derivation",
    "text": "Derivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment (i):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-13_nested-models.html#f-test-for-homogeneity-of-slopes",
    "title": "Nested Models in Statistical Design",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau){hi} + \\epsilon{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#hypothesis-testing",
    "href": "lectures/week-13_nested-models.html#hypothesis-testing",
    "title": "Nested Models in Statistical Design",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis ((H_0)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis ((H_A)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some (i, j)).\n\nTest Procedure\n\nFit Both Models:\n\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\n\nCompare Models Using ANOVA:\n\n\n# Fit Reduced Model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Fit Full Model with Interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare Models\n\nanova(model_reduced, model_full)\n\n\nDecision Rule:\n\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject (H_0) and conclude that slopes are not homogeneous.\nIf not significant, accept (H_0) and proceed with the reduced model.\n\nConclusion:\nA significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference:\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-13_nested-models.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Nested Models in Statistical Design",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\nFormula:\n\\[\nCI = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#steps-in-r",
    "href": "lectures/week-13_nested-models.html#steps-in-r",
    "title": "Nested Models in Statistical Design",
    "section": "Steps in R",
    "text": "Steps in R\n\nEstimate the Contrast:\n\nUsing the emmeans package to define and estimate contrasts.\n\nCalculate the Confidence Interval:\n\nThe emmeans package automatically provides confidence intervals when performing contrasts."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#example-1",
    "href": "lectures/week-13_nested-models.html#example-1",
    "title": "Nested Models in Statistical Design",
    "section": "Example",
    "text": "Example\n\nlibrary(emmeans)\n\n# Fit the ANCOVA Model\n\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define Estimated Marginal means\n\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a Contrast (e.g., Treatment B vs. Treatment A)\n\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\n\nsummary(contrast_AB, infer = TRUE)\n\nInterpretation:\nThe output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#medium-difficulty-1",
    "href": "lectures/week-13_nested-models.html#medium-difficulty-1",
    "title": "Nested Models in Statistical Design",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate (X).\nFit an ANCOVA model adjusting for (X).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-2",
    "href": "lectures/week-13_nested-models.html#r-code-example-2",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nset.seed(123)\n\n# Define Factors and Covariate\n\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\n\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate Responses with Treatment Effects and Covariate Effect\n\nY &lt;- 5 +\n\nifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) +\n\n0.5 * (X - mean(X)) +\n\nrnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA Model\n\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\nsummary(model_sim)\n\n# Diagnostic Plots\n\npar(mfrow = c(2, 2))\n\nplot(model_sim)\n\n# Estimated Marginal means\n\nlibrary(emmeans)\n\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\n\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\n\nsummary(pairwise_sim)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#questions-1",
    "href": "lectures/week-13_nested-models.html#questions-1",
    "title": "Nested Models in Statistical Design",
    "section": "Questions",
    "text": "Questions\n\nMain Effects:\n\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate (X): How does (X) influence the response variable (Y)?\n\n\nSignificance Testing:\n\n\nAre the treatment effects statistically significant after adjusting for (X)?\nIs the covariate (X) a significant predictor of (Y)?\n\n\nDiagnostic Plots:\n\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\n\nInterpretation of Pairwise Comparisons:\n\n\nWhich treatments significantly differ from each other after adjusting for (X)?\nHow do the adjusted means compare across treatments?"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-answers-1",
    "href": "lectures/week-13_nested-models.html#expected-answers-1",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Answers",
    "text": "Expected Answers\n\nMain Effects:\n\n\nTreatment: Treatments B and C have positive effects relative to Treatment A, indicating higher response values.\nCovariate (X): The covariate (X) has a positive influence on (Y), suggesting that as (X) increases, (Y) tends to increase.\n\n\nSignificance Testing:\n\n\nTreatment Effects: If the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nCovariate (X): If the p-value for (X) is below the significance level, (X) is a significant predictor.\n\n\nDiagnostic Plots:\n\n\nResiduals vs. Fitted: Random scatter suggests homoscedasticity and linearity.\nQ-Q Plot: Points close to the diagonal indicate normality of residuals.\nInfluential Observations: Absence of points with high leverage or large residuals indicates no influential observations.\n\n\nInterpretation of Pairwise Comparisons:\n\n\nSignificant Differences: Significant pairwise differences indicate which specific treatments differ after adjusting for (X).\nAdjusted Means: Provide a clearer comparison by accounting for the covariate’s effect, highlighting the true treatment effects.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis-1",
    "href": "lectures/week-13_nested-models.html#unbalanced-two-factor-ancova-analysis-1",
    "title": "Nested Models in Statistical Design",
    "section": "1. Unbalanced Two-Factor ANCOVA Analysis",
    "text": "1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-3",
    "href": "lectures/week-13_nested-models.html#r-code-example-3",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(car)\n\nset.seed(456)\n\n# Simulate Unbalanced Data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\n\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate Responses with Treatment and Covariate Effects\n\nY &lt;- 5 +\n\nifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) +\n\nifelse(FactorB == \"B1\", 0, 3) +\n\n0.5 * (X - mean(X)) +\n\nrnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA Model\n\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\n\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\n\nprint(typeII)\n\n# Type III Sum of Squares\n\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\n\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#discussion-points-1",
    "href": "lectures/week-13_nested-models.html#discussion-points-1",
    "title": "Nested Models in Statistical Design",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\n\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\n\nImpact of Imbalance:\n\n\nType II SS: Provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced, as it accounts for all factors simultaneously.\n\n\nSignificance of Effects:\n\n\nType II SS: May show significant main effects if there’s true variability not explained by interactions.\nType III SS: May adjust significance levels based on interactions, leading to different conclusions.\n\n\nImplications for Experimental Conclusions:\n\n\nChoosing SS Type: Critical for accurate inference; Type I SS is inappropriate due to dependency on factor order.\nMisinterpretation Risks: Incorrect SS type can lead to misleading conclusions about factor significance and interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-insights-1",
    "href": "lectures/week-13_nested-models.html#expected-insights-1",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Insights",
    "text": "Expected Insights\n\nType II SS: Suitable when interactions are not of primary interest, providing clear main effect interpretations.\nType III SS: Essential in unbalanced designs or when interactions are present, ensuring main effects are evaluated conditionally.\nImbalance Effects: Can distort Type I SS results, making Type II and III SS more robust alternatives in such scenarios.\n\nReference:\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation-1",
    "href": "lectures/week-13_nested-models.html#three-factor-interaction-interpretation-1",
    "title": "Nested Models in Statistical Design",
    "section": "2. Three-Factor Interaction Interpretation",
    "text": "2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#r-code-example-4",
    "href": "lectures/week-13_nested-models.html#r-code-example-4",
    "title": "Nested Models in Statistical Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(emmeans)\n\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate Data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\n\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\n\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate Responses with Three-way Interaction\n\nY &lt;- 5 +\n\nifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) +\n\nifelse(FactorB == \"B1\", 0, 3) +\n\nifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) +\n\n0.5 * (X - mean(X)) +\n\nifelse(FactorA == \"A1\" & FactorB == \"B1\", 0,\n\nifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) +\n\nrnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit Three-way ANCOVA Model\n\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nsummary(fit_three)\n\n# Plot Three-way Interaction\n\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n\ngeom_point() +\n\ngeom_line() +\n\nfacet_wrap(~ FactorC) +\n\nlabs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n\ntheme_minimal()\n\n# Estimated Marginal means\n\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\n\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\n\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interpretation-4",
    "href": "lectures/week-13_nested-models.html#interpretation-4",
    "title": "Nested Models in Statistical Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction:\n\n\nVisualization: The interaction plot shows non-parallel lines across different levels of Factor C, indicating a significant three-way interaction.\nStatistical Significance: The model summary reveals a significant three-way interaction, confirming that the combined effect of Factors A, B, and C is not purely additive.\n\n\nImpact on Main and Two-Way Interactions:\n\n\nMain Effects: Cannot be interpreted independently as their effects depend on the levels of other factors.\nTwo-Way Interactions: The presence of a significant three-way interaction implies that two-way interactions also vary across the third factor.\n\n\nPractical Implications:\n\n\nComplex Relationships: Understanding how multiple factors interact provides deeper insights into the data.\nDecision Making: Facilitates more informed decisions by recognizing nuanced effect patterns.\n\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts-1",
    "href": "lectures/week-13_nested-models.html#mathematical-proof-of-interaction-contrasts-1",
    "title": "Nested Models in Statistical Design",
    "section": "3. Mathematical Proof of Interaction Contrasts",
    "text": "3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#proof-outline-1",
    "href": "lectures/week-13_nested-models.html#proof-outline-1",
    "title": "Nested Models in Statistical Design",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta){ij} + (\\alpha\\gamma){ik} + (\\beta\\gamma){jk} + (\\alpha\\beta\\gamma){ijk} + \\epsilon_{ijk}\n\\]\nWhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)*{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma){ik}\\) and \\((\\beta\\gamma){jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)*{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta){ij} = (\\alpha\\gamma){ik} = (\\beta\\gamma){jk} = (\\alpha\\beta\\gamma){ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nThis represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model:\n\nIf any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\nConclusion:\n\nInteraction contrasts (\\((\\alpha\\beta){ij}\\), \\((\\alpha\\gamma){ik}\\), \\((\\beta\\gamma){jk}\\), and \\((\\alpha\\beta\\gamma){ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#expected-mean-squares-for-nested-models",
    "href": "lectures/week-13_nested-models.html#expected-mean-squares-for-nested-models",
    "title": "Nested Models in Statistical Design",
    "section": "Expected Mean Squares for Nested Models",
    "text": "Expected Mean Squares for Nested Models\nFor the model \\(Y_{ijt} = \\mu + \\alpha_i + \\beta_j(i) + \\epsilon_{ijt}\\), the Expected Mean Squares (EMS) are:\n\\[\nEMS_A = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\n\\[\nEMS_B(A) = \\sigma^2 + r \\sigma^2_B\n\\]\n\\[\nEMS_E = \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/week-13_nested-models.html#derivation-steps-1",
    "href": "lectures/week-13_nested-models.html#derivation-steps-1",
    "title": "Nested Models in Statistical Design",
    "section": "Derivation Steps",
    "text": "Derivation Steps\n\nAssumptions:\n\n\nFixed Effects: \\(\\alpha_i\\) and \\(\\beta_j(i)\\) are fixed.\nRandom Error: \\(\\epsilon_{ijt} \\sim N(0, \\sigma^2)\\).\nReplicates: \\(r\\) replicates per nested factor.\n\n\nVariance Components:\n\n\nBetween (A) Groups: Variance due to Factor (A) is captured by \\(\\alpha_i\\).\nBetween (B(A)) Groups: Variance due to Factor (B) nested within (A) is \\(\\sigma^2_B\\).\nWithin (B(A)) Groups: Residual variance is \\(\\sigma^2\\).\n\n\nCalculation of EMS:\n\n\nEMS for Factor (A):\n\n\\[\nEMS_A = Var(\\alpha_i) + Var(\\beta_j(i)) + Var(\\epsilon_{ijt}) = \\sigma^2 + r \\sigma^2_B + r b Q(\\alpha_i)\n\\]\n\nEMS for Factor (B(A)):\n\n\\[\nEMS_B(A) = Var(\\beta_j(i)) + Var(\\epsilon_{ijt}) = \\sigma^2 + r \\sigma^2_B\n\\]\n\nEMS for Error:\n\n\\[\nEMS_E = Var(\\epsilon_{ijt}) = \\sigma^2\n\\]\n\nEstimation of Variance Components:\n\nUsing the ANOVA table, mean squares are equated to EMS to solve for \\(\\sigma^2_B\\) and \\(\\sigma^2\\):\n\\[\n\\hat{\\sigma}^2_B = \\frac{MS_A - MS_E}{r}\n\\]\n\\[\n\\hat{\\sigma}^2 = MS_E\n\\]\nWhere:\n\n\\(MS_A\\): Mean Square for Factor (A).\n\\(MS_E\\): Mean Square for Error.\n\nConclusion:\nThe derivation demonstrates how ANOVA decomposes total variability into components attributable to nested factors and residual error, enabling the estimation of variance components.\nReference:\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#interaction-contrasts-measure-deviations-from-additivity",
    "href": "lectures/week-13_nested-models.html#interaction-contrasts-measure-deviations-from-additivity",
    "title": "Nested Models in Statistical Design",
    "section": "Interaction Contrasts Measure Deviations from Additivity",
    "text": "Interaction Contrasts Measure Deviations from Additivity\nDefinition:\nIn a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#proof-steps",
    "href": "lectures/week-13_nested-models.html#proof-steps",
    "title": "Nested Models in Statistical Design",
    "section": "Proof Steps",
    "text": "Proof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta){ij} + (\\alpha\\gamma){ik} + (\\beta\\gamma){jk} + (\\alpha\\beta\\gamma){ijk} + \\epsilon_{ijk}\n\\]\nWhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)*{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma){ik}\\) and \\((\\beta\\gamma){jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)*{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta){ij} = (\\alpha\\gamma){ik} = (\\beta\\gamma){jk} = (\\alpha\\beta\\gamma){ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nThis represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model:\n\nIf any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\nConclusion:\n\nInteraction contrasts (\\((\\alpha\\beta){ij}\\), \\((\\alpha\\gamma){ik}\\), \\((\\beta\\gamma){jk}\\), and \\((\\alpha\\beta\\gamma){ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-13_nested-models.html#references",
    "href": "lectures/week-13_nested-models.html#references",
    "title": "Nested Models in Statistical Design",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company.\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-06_higher-order.html",
    "href": "lectures/week-06_higher-order.html",
    "title": "Higher-Order Designs",
    "section": "",
    "text": "In experimental research, understanding the effects of multiple factors and their interactions is crucial for optimizing outcomes and making informed decisions. Factorial designs allow researchers to evaluate not only the individual (main) effects of factors but also how factors interact with each other to influence a response variable. Higher-order factorial designs, involving three or more factors, extend these principles, enabling the exploration of complex relationships within data.\nExample: Imagine you’re a chef experimenting with different ingredients to perfect a recipe. Factor A could be the type of flour (whole wheat, all-purpose), Factor B the type of sugar (granulated, brown), and Factor C the baking temperature (350°F, 375°F, 400°F). Each factor influences the final product’s taste and texture. However, the optimal combination might not be obvious because the effect of sugar type could depend on the type of flour and the baking temperature. Higher-order factorial designs help uncover these intricate interactions systematically.\nThis lecture delves into the structure, analysis, and interpretation of higher-order factorial designs, with a focus on two-way ANOVA models. We’ll explore main effects, interaction effects, handling unequal sample sizes, and practical implementation using R.\n\n\n\nUnderstand the structure and applications of higher-order factorial designs.\nDifferentiate between main effects and interaction effects.\nPerform two-way ANOVA and interpret its results.\nAddress challenges posed by unequal replication in factorial experiments.\nVisualize interactions and understand their implications.\nApply advanced statistical techniques using R for complex datasets.\nEngage with exercises to reinforce understanding and application of concepts."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#objectives",
    "href": "lectures/week-06_higher-order.html#objectives",
    "title": "Higher-Order Designs",
    "section": "",
    "text": "Understand the structure and applications of higher-order factorial designs.\nDifferentiate between main effects and interaction effects.\nPerform two-way ANOVA and interpret its results.\nAddress challenges posed by unequal replication in factorial experiments.\nVisualize interactions and understand their implications.\nApply advanced statistical techniques using R for complex datasets.\nEngage with exercises to reinforce understanding and application of concepts."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#factorial-design-notation",
    "href": "lectures/week-06_higher-order.html#factorial-design-notation",
    "title": "Higher-Order Designs",
    "section": "Factorial Design Notation",
    "text": "Factorial Design Notation\nA factorial experiment involves multiple factors, each with different levels. The notation \\(a \\times b \\times c \\times \\dots\\) denotes the number of levels for each factor. For example:\n\n2 × 3 × 4 Design:\n\nFactor A: 2 levels\nFactor B: 3 levels\nFactor C: 4 levels\nTotal Treatment Combinations: \\(2 \\times 3 \\times 4 = 24\\)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#experimental-objectives",
    "href": "lectures/week-06_higher-order.html#experimental-objectives",
    "title": "Higher-Order Designs",
    "section": "Experimental Objectives",
    "text": "Experimental Objectives\n\nEstimate Main Effects: Determine the independent influence of each factor on the response.\nQuantify Two-Factor Interactions: Assess how pairs of factors jointly affect the response.\nInvestigate Higher-Order Interactions: Explore interactions involving three or more factors.\n\nExample: In a manufacturing process, factors could include machine speed (fast, medium, slow), material type (A, B), and temperature (high, low). Understanding not only how each factor affects product quality but also how combinations (e.g., fast speed with high temperature) influence outcomes is essential for process optimization."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#general-model-for-higher-order-factorial-designs",
    "href": "lectures/week-06_higher-order.html#general-model-for-higher-order-factorial-designs",
    "title": "Higher-Order Designs",
    "section": "General Model for Higher-Order Factorial Designs",
    "text": "General Model for Higher-Order Factorial Designs\nFor \\(p\\) factors, the general linear model is:\n\\[\nY_{ijk\\ldots} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\ldots + \\epsilon_{ijk\\ldots}\n\\]\nWhere:\n\n\\(Y_{ijk\\ldots}\\): Observed response.\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i, \\beta_j, \\gamma_k\\): Main effects of factors \\(A, B, C\\).\n\\((\\alpha\\beta)_{ij}\\): Two-factor interaction between \\(A\\) and \\(B\\).\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Three-factor interaction.\n\\(\\epsilon_{ijk\\ldots} \\sim N(0, \\sigma^2)\\): Random error term.\n\n\nAssumptions\n\nIndependence: Observations are independent.\nHomoscedasticity: Constant variance across all treatment combinations.\nNormality: Errors are normally distributed.\n\nImportance: These assumptions underpin the validity of ANOVA results. Violations can lead to incorrect inferences, necessitating diagnostic checks and potential corrective measures."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#analysis-with-equal-sample-sizes",
    "href": "lectures/week-06_higher-order.html#analysis-with-equal-sample-sizes",
    "title": "Higher-Order Designs",
    "section": "Analysis with Equal Sample Sizes",
    "text": "Analysis with Equal Sample Sizes\nWhen each treatment combination has the same number of replicates (\\(n\\)), the analysis simplifies due to balanced design properties.\n\nDegrees of Freedom (DF)\n\nFactor A: \\(a - 1\\)\nFactor B: \\(b - 1\\)\nInteraction AB: \\((a - 1)(b - 1)\\)\nError: \\(N - ab\\)\nTotal: \\(N - 1\\)\n\nWhere \\(N = abn\\) is the total number of observations.\n\n\nSum of Squares (SS)\n\nTotal SS (\\(SS_T\\)): Total variability in responses.\nFactor SS (\\(SS_A, SS_B\\)): Variability due to main effects.\nInteraction SS (\\(SS_{AB}\\)): Variability due to interactions.\nError SS (\\(SS_E\\)): Unexplained variability.\n\n\n\nMean Squares (MS)\n\\[\nMS = \\frac{SS}{DF}\n\\]\n\n\nANOVA Table Example\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF-statistic\n\n\n\n\nFactor A\n\\(a-1\\)\n\\(SS_A\\)\n\\(MS_A\\)\n\\(MS_A / MS_E\\)\n\n\nFactor B\n\\(b-1\\)\n\\(SS_B\\)\n\\(MS_B\\)\n\\(MS_B / MS_E\\)\n\n\nInteraction AB\n\\((a-1)(b-1)\\)\n\\(SS_{AB}\\)\n\\(MS_{AB}\\)\n\\(MS_{AB} / MS_E\\)\n\n\nError\n\\(N - ab\\)\n\\(SS_E\\)\n\\(MS_E\\)\n-\n\n\nTotal\n\\(N - 1\\)\n\\(SS_T\\)\n-\n-\n\n\n\nNote: In unbalanced designs, Type I, II, and III sums of squares may be used to handle unequal sample sizes."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#main-effects-vs.-interaction-effects",
    "href": "lectures/week-06_higher-order.html#main-effects-vs.-interaction-effects",
    "title": "Higher-Order Designs",
    "section": "Main Effects vs. Interaction Effects",
    "text": "Main Effects vs. Interaction Effects\n\nMain Effect: The independent effect of a single factor on the response.\nInteraction Effect: Occurs when the effect of one factor depends on the level of another factor.\n\n\nVisualizing Interactions\nInteraction Plots: Graphical representations where lines represent one factor’s levels across another factor’s levels.\n\nParallel Lines: Indicate no interaction; main effects are additive.\nNon-Parallel Lines: Indicate interaction; effects are dependent.\n\nExample in R:\ninteraction.plot(data$FactorA, data$FactorB, data$Response, col=c(\"blue\", \"red\"))\nInterpretation: If lines cross or are not parallel, there is an interaction effect, meaning the impact of one factor varies with the level of the other factor."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#example-popcorn-microwave-experiment",
    "href": "lectures/week-06_higher-order.html#example-popcorn-microwave-experiment",
    "title": "Higher-Order Designs",
    "section": "Example: Popcorn-Microwave Experiment",
    "text": "Example: Popcorn-Microwave Experiment\nScenario: Three factors—Brand (3 levels), Power (2 levels), and Time (3 levels)—are tested to maximize popcorn popping percentage.\nObjective: Identify the optimal combination of brand, power, and time for best popping performance.\nR Code Example:\n# Load data\ndata &lt;- read.csv(\"popcorn.csv\")\n\n# Fit three-way ANOVA model\nfit &lt;- aov(PoppingRate ~ Brand * Power * Time, data = data)\n\n# Summary of ANOVA\nsummary(fit)\n\n# Interaction Plot\ninteraction.plot(data$Time, data$Brand, data$PoppingRate, col=c(\"green\", \"orange\", \"purple\"))\nInterpretation: Analyzing the ANOVA table and interaction plots reveals how each factor and their interactions influence the popping rate."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#types-of-sums-of-squares",
    "href": "lectures/week-06_higher-order.html#types-of-sums-of-squares",
    "title": "Higher-Order Designs",
    "section": "Types of Sums of Squares",
    "text": "Types of Sums of Squares\n\nType I (Sequential): Tests factors sequentially, dependent on the order of entry.\nType II (Marginal): Tests main effects after accounting for other main effects but ignoring interactions.\nType III (Conditional): Tests main effects after accounting for all other factors, including interactions.\n\nR Implementation with car Package:\nlibrary(car)\n\n# Fit ANOVA model\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Type II Sums of Squares\nAnova(fit, type=\"II\")\n\n# Type III Sums of Squares\nAnova(fit, type=\"III\")\nConsiderations: - Type III is commonly used in software like SAS and SPSS, especially when interactions are present. - Type II is suitable when interactions are not of primary interest.\nReferences: Christensen (2018) provides an in-depth discussion on handling unbalanced designs and choosing appropriate sums of squares."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#handling-three-factor-interactions",
    "href": "lectures/week-06_higher-order.html#handling-three-factor-interactions",
    "title": "Higher-Order Designs",
    "section": "Handling Three-Factor Interactions",
    "text": "Handling Three-Factor Interactions\nIn higher-order designs, interactions can involve three or more factors, complicating interpretation.\n\nThree-Factor Interaction\nOccurs when the two-factor interaction between two factors depends on the level of a third factor.\nInterpretation: If the two-factor interaction between Factors A and B changes across levels of Factor C, a three-factor interaction is present.\n\n\nExample in R\n# Fit three-way ANOVA model\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Summary of ANOVA\nsummary(fit)\n\n# Three-factor interaction plot\nlibrary(ggplot2)\nggplot(data, aes(x=FactorA, y=Response, color=FactorB, linetype=FactorC)) +\n  geom_point() +\n  geom_line() +\n  labs(title=\"Three-Factor Interaction Plot\")\nInterpretation: Assess if the lines representing different levels of Factor B change across Factor C levels, indicating an interaction."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#addressing-complex-interactions",
    "href": "lectures/week-06_higher-order.html#addressing-complex-interactions",
    "title": "Higher-Order Designs",
    "section": "Addressing Complex Interactions",
    "text": "Addressing Complex Interactions\n\nHigher-Order Interactions\nBeyond three factors, interactions become increasingly complex and harder to interpret. It’s essential to focus on interactions that make theoretical sense and are supported by the data.\n\n\nSimplifying the Model\n\nModel Selection: Start with a full model including all interactions. If higher-order interactions are not significant, consider removing them for simpler interpretation.\nHierarchical Models: Retain lower-order interactions when higher-order interactions are present, but omit higher-order interactions if lower-order ones are not significant.\n\nExample in R:\n# Full model\nfull_fit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Reduced model without three-way interaction\nreduced_fit &lt;- aov(Response ~ FactorA * FactorB + FactorA * FactorC + FactorB * FactorC, data = data)\n\n# Compare models\nanova(reduced_fit, full_fit)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#residual-analysis",
    "href": "lectures/week-06_higher-order.html#residual-analysis",
    "title": "Higher-Order Designs",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nResiduals (\\(e_{ijk\\ldots}\\)) are the differences between observed and predicted values. Analyzing residuals helps detect violations of model assumptions.\n\nResidual Plots\n\nResiduals vs. Fitted Values:\n\nPurpose: Check for homoscedasticity and model fit.\nInterpretation:\n\nRandom Scatter: Assumptions hold.\nPatterns (e.g., funnel shape): Indicate heteroscedasticity.\n\n\nNormal Q-Q Plot:\n\nPurpose: Assess normality of residuals.\nInterpretation:\n\nPoints near the line: Normality holds.\nDeviations: Non-normality present.\n\n\nResiduals vs. Order of Data:\n\nPurpose: Detect non-independence or autocorrelation.\nInterpretation:\n\nRandom Scatter: Independence holds.\nPatterns or Trends: Violation of independence.\n\n\n\n\n\nR Implementation\n# Fit the model\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\n# Residuals vs Fitted\nplot(fit, which=1)\n\n# Normal Q-Q Plot\nplot(fit, which=2)\n\n# Residuals vs Order\ndata$Order &lt;- 1:nrow(data)\nplot(data$Order, resid(fit), main=\"Residuals vs. Order\", xlab=\"Order\", ylab=\"Residuals\")\nabline(h=0, col=\"red\")\nInterpretation: Examine each plot for signs of assumption violations and consider corrective actions if needed."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#remedies-for-assumption-violations",
    "href": "lectures/week-06_higher-order.html#remedies-for-assumption-violations",
    "title": "Higher-Order Designs",
    "section": "Remedies for Assumption Violations",
    "text": "Remedies for Assumption Violations\n\nTransformations:\n\nLog Transformation: Useful for stabilizing variance when it increases with the mean.\nSquare-Root Transformation: Often used for count data.\nBox-Cox Transformation: Systematic method to find the best power transformation.\n\nR Implementation:\n\n   library(MASS)\n   \n   # Box-Cox Transformation\n   boxcox_fit &lt;- boxcox(fit, lambda=seq(-2, 2, by=0.1))\n   \n   # Identify optimal lambda\n   optimal_lambda &lt;- boxcox_fit$x[which.max(boxcox_fit$y)]\n   \n   # Transform Response\n   data$Response_transformed &lt;- (data$Response^optimal_lambda - 1) / optimal_lambda\n   \n   # Re-fit the model with transformed data\n   fit_transformed &lt;- aov(Response_transformed ~ FactorA * FactorB * FactorC, data = data)\n   summary(fit_transformed)\n\nAlternative Models:\n\nMixed-Effects Models: Address non-independence by incorporating random effects.\nNonparametric Methods: Utilize methods like the Kruskal-Wallis test when assumptions are severely violated.\n\nR Implementation:\n\n   library(lme4)\n   \n   # Mixed-Effects Model Example\n   mixed_fit &lt;- lmer(Response ~ FactorA * FactorB * FactorC + (1|RandomFactor), data = data)\n   summary(mixed_fit)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#interaction-plots",
    "href": "lectures/week-06_higher-order.html#interaction-plots",
    "title": "Higher-Order Designs",
    "section": "Interaction Plots",
    "text": "Interaction Plots\nTwo-Way Interaction Plot:\ninteraction.plot(data$FactorA, data$FactorB, data$Response, col=c(\"blue\", \"red\", \"green\"))\nThree-Way Interaction Plot:\nlibrary(ggplot2)\n\nggplot(data, aes(x=FactorA, y=Response, color=FactorB)) +\n  geom_point() +\n  geom_line(aes(group=FactorB)) +\n  facet_wrap(~ FactorC) +\n  labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\")\nInterpretation: Non-parallel lines in two-way plots indicate interaction effects. In three-way plots, varying interaction patterns across the third factor’s levels suggest higher-order interactions.\nExample Interpretation: In the popcorn experiment, if the optimal baking time varies significantly across different brands and power settings, interaction plots will reveal these dependencies, guiding recommendations for specific conditions."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#medium-difficulty",
    "href": "lectures/week-06_higher-order.html#medium-difficulty",
    "title": "Higher-Order Designs",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANOVA with Interaction:\n\nDataset: Simulate data for a \\(3 \\times 2\\) factorial design with factors A (3 levels) and B (2 levels), each with 5 replicates.\nTasks:\n\nFit a two-way ANOVA model with interaction.\nGenerate interaction plots.\nInterpret the ANOVA table and interaction plots.\n\n\nHints:\n\n   set.seed(123)\n   FactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each=10))\n   FactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each=5, times=3))\n   Response &lt;- c(rnorm(5, 10, 2), rnorm(5, 12, 2),\n                rnorm(5, 14, 2), rnorm(5, 16, 2),\n                rnorm(5, 18, 2), rnorm(5, 20, 2))\n   data_sim &lt;- data.frame(FactorA, FactorB, Response)\n   \n   fit_sim &lt;- aov(Response ~ FactorA * FactorB, data = data_sim)\n   summary(fit_sim)\n   \n   interaction.plot(data_sim$FactorA, data_sim$FactorB, data_sim$Response, col=c(\"blue\", \"red\"))\n\nContrast Construction:\n\nScenario: In a three-factor experiment (A, B, C), construct a contrast to compare the average effect of Factor A at high levels of Factors B and C versus low levels.\nTasks:\n\nDefine the contrast coefficients.\nCalculate the contrast estimate and its variance.\nTest the significance of the contrast.\n\n\nHints:\n\nUse emmeans or multcomp packages for contrast testing."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#challenging-problems",
    "href": "lectures/week-06_higher-order.html#challenging-problems",
    "title": "Higher-Order Designs",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor Design Analysis:\n\nDataset: Use an unbalanced dataset with unequal replicates across treatment combinations.\nTasks:\n\nFit a two-way ANOVA model.\nPerform Type II and Type III sum of squares analysis.\nCompare results and discuss how imbalance affects the interpretation of main and interaction effects.\n\n\nHints:\n\n   library(car)\n   # Assume 'data_unbalanced' is loaded with unequal replicates\n   fit_unbalanced &lt;- aov(Response ~ FactorA * FactorB, data = data_unbalanced)\n   \n   # Type II SS\n   Anova(fit_unbalanced, type=\"II\")\n   \n   # Type III SS\n   Anova(fit_unbalanced, type=\"III\")\n\nThree-Factor Interaction Interpretation:\n\nDataset: Simulate or use a real dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nFit a three-way ANOVA model.\nVisualize the three-way interaction.\nUse emmeans to explore and interpret the interaction.\nDiscuss how the three-way interaction affects the interpretation of main and two-way interactions.\n\n\nHints:\n\n   library(emmeans)\n   \n   # Simulate data with three-way interaction\n   set.seed(456)\n   FactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each=12))\n   FactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each=6, times=3))\n   FactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times=6))\n   Response &lt;- rnorm(36, mean=10 + as.numeric(FactorA)*2 + as.numeric(FactorB)*3 +\n                     as.numeric(FactorC)*1.5 + \n                     as.numeric(FactorA)*as.numeric(FactorB) +\n                     as.numeric(FactorA)*as.numeric(FactorC) +\n                     as.numeric(FactorB)*as.numeric(FactorC) +\n                     as.numeric(FactorA)*as.numeric(FactorB)*as.numeric(FactorC), 2)\n   data_three &lt;- data.frame(FactorA, FactorB, FactorC, Response)\n   \n   fit_three &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data_three)\n   summary(fit_three)\n   \n   # Interaction plot\n   ggplot(data_three, aes(x=FactorA, y=Response, color=FactorB)) +\n     geom_point() +\n     geom_line(aes(group=FactorB)) +\n     facet_wrap(~ FactorC) +\n     labs(title=\"Three-Way Interaction Plot\")\n   \n   # Using emmeans\n   emm &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\n   contrast(emm, interaction = \"trt.vs.ctrl\", adjust = \"none\")\n\nMathematical Proof of Interaction Contrasts:\n\nTask: Derive the formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\n\nHints:\n\nUse the definition of main effects and interaction terms.\nUtilize properties of linear algebra to demonstrate additivity."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#medium-difficulty-1",
    "href": "lectures/week-06_higher-order.html#medium-difficulty-1",
    "title": "Higher-Order Designs",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANOVA with Interaction:\n\nDataset: Simulate a balanced \\(3 \\times 2\\) factorial design with factors A (3 levels) and B (2 levels), each with 5 replicates.\nTasks:\n\nFit a two-way ANOVA model with interaction.\nGenerate interaction plots.\nInterpret the ANOVA table and interaction plots.\n\n\nR Code Example:\n\n   set.seed(123)\n   FactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each=10))\n   FactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each=5, times=3))\n   Response &lt;- c(rnorm(5, 10, 2), rnorm(5, 12, 2),\n                rnorm(5, 14, 2), rnorm(5, 16, 2),\n                rnorm(5, 18, 2), rnorm(5, 20, 2))\n   data_sim &lt;- data.frame(FactorA, FactorB, Response)\n   \n   # Fit the model\n   fit_sim &lt;- aov(Response ~ FactorA * FactorB, data = data_sim)\n   summary(fit_sim)\n   \n   # Interaction plot\n   interaction.plot(data_sim$FactorA, data_sim$FactorB, data_sim$Response, col=c(\"blue\", \"red\"))\n\nConstructing and Testing Contrasts:\n\nScenario: In a three-factor experiment (A, B, C), construct a contrast comparing the average effect of Factor A at high levels of Factors B and C versus low levels.\nTasks:\n\nDefine the contrast coefficients.\nCalculate the contrast estimate and its variance.\nTest the significance of the contrast using ANOVA.\n\n\nR Code Example:\n\n   library(emmeans)\n   \n   # Assume 'fit' is a fitted three-way ANOVA model\n   emm &lt;- emmeans(fit, ~ FactorA * FactorB * FactorC)\n   \n   # Define contrast coefficients for comparing high vs. low levels\n   # Example: Contrast comparing (A1 + A2 + A3)/3 at high levels vs. low levels\n   contrast_matrix &lt;- matrix(c(1/3, 1/3, 1/3, -1/3, -1/3, -1/3), ncol=1)\n   colnames(contrast_matrix) &lt;- \"A_high_vs_low\"\n   \n   # Perform the contrast\n   contrast_results &lt;- contrast(emm, method = contrast_matrix)\n   summary(contrast_results, infer = TRUE)"
  },
  {
    "objectID": "lectures/week-06_higher-order.html#challenging-problems-1",
    "href": "lectures/week-06_higher-order.html#challenging-problems-1",
    "title": "Higher-Order Designs",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor Design Analysis:\n\nDataset: Utilize a dataset with an unequal number of replicates across treatment combinations.\nTasks:\n\nFit a two-way ANOVA model.\nPerform Type II and Type III sum of squares analysis.\nCompare and discuss how the choice of sum of squares type affects the interpretation of main and interaction effects.\n\n\nR Code Example:\n\n   library(car)\n   \n   # Assume 'data_unbalanced' is your dataset\n   fit_unbalanced &lt;- aov(Response ~ FactorA * FactorB, data = data_unbalanced)\n   \n   # Type II SS\n   typeII &lt;- Anova(fit_unbalanced, type=\"II\")\n   print(typeII)\n   \n   # Type III SS\n   typeIII &lt;- Anova(fit_unbalanced, type=\"III\")\n   print(typeIII)\nDiscussion Points: - How do Type II and Type III SS handle imbalance? - Which interactions or main effects become significant under each type? - Implications for experimental conclusions.\n\nThree-Factor Interaction Interpretation:\n\nDataset: Use a simulated or real dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nFit a three-way ANOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\n\nR Code Example:\n\n   library(emmeans)\n   library(ggplot2)\n   \n   # Simulate data with three-way interaction\n   set.seed(456)\n   FactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each=12))\n   FactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each=6, times=3))\n   FactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times=6))\n   Response &lt;- rnorm(36, mean=10 + as.numeric(FactorA)*2 + as.numeric(FactorB)*3 +\n                     as.numeric(FactorC)*1.5 + \n                     as.numeric(FactorA)*as.numeric(FactorB) +\n                     as.numeric(FactorA)*as.numeric(FactorC) +\n                     as.numeric(FactorB)*as.numeric(FactorC) +\n                     as.numeric(FactorA)*as.numeric(FactorB)*as.numeric(FactorC), 2)\n   data_three &lt;- data.frame(FactorA, FactorB, FactorC, Response)\n   \n   # Fit the model\n   fit_three &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data_three)\n   summary(fit_three)\n   \n   # Interaction plot\n   ggplot(data_three, aes(x=FactorA, y=Response, color=FactorB, linetype=FactorC)) +\n     geom_point() +\n     geom_line() +\n     labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\")\n   \n   # Using emmeans for contrasts\n   emm &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\n   contrast_results &lt;- contrast(emm, method = \"pairwise\", adjust = \"none\")\n   summary(contrast_results)\nInterpretation: - How does the interaction manifest across different levels? - What do the estimated marginal means reveal about the three-way interaction? - Implications for factor effect interpretations.\n\nMathematical Proof of Interaction Contrasts:\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nSteps:\n\nDefine the contrast for three-way interaction.\nShow that if all interaction contrasts are zero, the model reduces to an additive model.\nUse properties of linear combinations and contrasts to demonstrate the relationship.\n\n\nHints:\n\nUtilize the definitions of main effects and interactions.\nEmploy algebraic manipulation to isolate interaction terms.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-06_higher-order.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Higher-Order Designs",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor a \\(k\\)-factor interaction, the confidence interval for a contrast \\(L\\) is given by:\n\\[\n\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\nDerivation:\n\nEstimator: \\(\\hat{L} = \\sum c_i \\hat{\\mu}_i\\)\nVariance: \\(\\text{Var}(\\hat{L}) = \\sigma^2 \\sum \\frac{c_i^2}{n_i}\\) (Assuming equal sample sizes and independence)\nConfidence Interval: Using the properties of the normal distribution for large sample sizes or exact t-distribution for smaller samples."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#box-cox-transformation",
    "href": "lectures/week-06_higher-order.html#box-cox-transformation",
    "title": "Higher-Order Designs",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nThe Box-Cox transformation seeks an optimal power transformation to stabilize variance and normalize residuals.\n$$ Y’() =\n\\[\\begin{cases}\n\n\\frac{Y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\\n\n\\ln(Y) & \\text{if } \\lambda = 0.\n\n\\end{cases}\\]\n$$\nSteps in R:\n\nFit the ANOVA model:\n\nfit &lt;- aov(Response ~ FactorA * FactorB * FactorC, data = data)\n\nApply Box-Cox:\n\nlibrary(MASS)\nboxcox_fit &lt;- boxcox(fit, lambda=seq(-2,2, by=0.1))\noptimal_lambda &lt;- boxcox_fit$x[which.max(boxcox_fit$y)]\n\nTransform the Response:\n\ndata$Response_transformed &lt;- (data$Response^optimal_lambda - 1) / optimal_lambda\n\nRe-fit the Model with Transformed Data:\n\nfit_transformed &lt;- aov(Response_transformed ~ FactorA * FactorB * FactorC, data = data)\nsummary(fit_transformed)\nInterpretation: The optimal \\(\\lambda\\) indicates the best power transformation to achieve homoscedasticity and normality. Re-fitting the model on transformed data should show improved diagnostic plots."
  },
  {
    "objectID": "lectures/week-06_higher-order.html#mathematical-proof-of-interaction-contrasts",
    "href": "lectures/week-06_higher-order.html#mathematical-proof-of-interaction-contrasts",
    "title": "Higher-Order Designs",
    "section": "Mathematical Proof of Interaction Contrasts",
    "text": "Mathematical Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, the interaction contrast measures how the combined effect of three factors deviates from the sum of their individual and two-factor interaction effects.\nProof Steps:\n\nModel Setup:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\n\nAdditivity Condition: If \\((\\alpha\\beta\\gamma)_{ijk} = 0\\) for all \\(i,j,k\\), then:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + \\epsilon_{ijk}\n\\]\nThis represents an additive model where the combined effects of factors are purely additive without higher-order interactions.\n\nInteraction Contrast: Define the three-way interaction contrast as:\n\n\\[\nL = (\\alpha\\beta\\gamma)_{ijk}\n\\]\n\nZero Interaction Contrasts Imply Additivity: If \\(L = 0\\) for all combinations, then:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + \\epsilon_{ijk}\n\\]\nHence, the absence of three-way interactions leads to an additive relationship among factors.\n\nConclusion: The interaction contrasts \\(L\\) quantify the extent to which the actual response deviates from what would be expected under additivity. Non-zero contrasts indicate the presence of higher-order interactions.\n\nReference: Dean, Voss & Draguljić (2017) provide a comprehensive exploration of interaction contrasts and their role in factorial designs."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html",
    "href": "lectures/week-12_mixed-models.html",
    "title": "Mixed Models",
    "section": "",
    "text": "In the landscape of statistical modeling, Random-Effects and Mixed-Effects Models play a pivotal role in analyzing data that exhibit hierarchical or grouped structures. Unlike fixed-effects models, which assume that the levels of a factor are the only ones of interest, random-effects models consider factors as random samples from a larger population. This distinction allows for the assessment of variability not just within treatments but also across different groups or clusters.\nIntuitive Analogy: Imagine conducting a study on the effectiveness of a new teaching method across various schools. If we treat schools as fixed effects, we’re only interested in those specific schools. However, if we consider schools as random effects, we acknowledge that these schools are a random sample from a broader population, enabling us to generalize our findings beyond the studied institutions.\nThis week’s focus encompasses:\n\nDifferentiating between fixed-effects and random-effects models.\nDelving into single and multiple random-effects models.\nExploring mixed-effects models that integrate both fixed and random components.\nImplementing these models in R with practical examples.\nEngaging with advanced topics through detailed proofs and mathematical derivations.\nReinforcing learning through targeted exercises.\n\n\n\nBy the end of this lecture, you will be able to:\n\nDifferentiate between fixed-effects and random-effects models.\nApply single random-effects models and interpret their results.\nAnalyze mixed-effects models, combining fixed and random components.\nEvaluate model assumptions and perform diagnostic checks.\nCompute confidence intervals and test hypotheses for random effects.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#objectives",
    "href": "lectures/week-12_mixed-models.html#objectives",
    "title": "Mixed Models",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDifferentiate between fixed-effects and random-effects models.\nApply single random-effects models and interpret their results.\nAnalyze mixed-effects models, combining fixed and random components.\nEvaluate model assumptions and perform diagnostic checks.\nCompute confidence intervals and test hypotheses for random effects.\nEngage with advanced mathematical concepts through proofs and derivations.\nEnhance your understanding through practical R examples and exercises."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#fixed-effects",
    "href": "lectures/week-12_mixed-models.html#fixed-effects",
    "title": "Mixed Models",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nDefinition\nFixed Effects are factors in a model where the levels are specifically chosen and of primary interest. The goal is to make inferences about these particular levels.\n\n\nExamples\n\nComparing the effectiveness of three specific fertilizers (A, B, C).\nEvaluating the performance of particular teaching methods.\n\n\n\nModeling Assumptions\n\nFactor levels are constants and not random.\nEffects are modeled as fixed parameters.\n\nIntuitive Example: Assessing the impact of three distinct fertilizers on plant growth where only these three fertilizers are of interest."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#random-effects",
    "href": "lectures/week-12_mixed-models.html#random-effects",
    "title": "Mixed Models",
    "section": "Random Effects",
    "text": "Random Effects\n\nDefinition\nRandom Effects are factors in a model where the levels are randomly sampled from a larger population. The focus is on understanding the variability across these random levels rather than the levels themselves.\n\n\nExamples\n\nVariability among machine operators selected randomly from a factory.\nBatch-to-batch variability in a manufacturing process.\n\n\n\nModeling Assumptions\n\nFactor levels are random variables, typically assumed to follow a normal distribution.\nEffects are modeled as random variables with a specified distribution.\n\nIntuitive Example: Studying the effect of different classrooms (randomly selected from all classrooms in a district) on student performance, aiming to generalize findings beyond the sampled classrooms."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#random-effects-one-way-model",
    "href": "lectures/week-12_mixed-models.html#random-effects-one-way-model",
    "title": "Mixed Models",
    "section": "Random-Effects One-Way Model",
    "text": "Random-Effects One-Way Model\nThe Random-Effects One-Way Model is suitable when a single random factor influences the response variable. This model accounts for variability due to the random factor and the inherent experimental error.\n\nModel Specification\n\\[\nY_{it} = \\mu + T_i + \\varepsilon_{it}, \\quad T_i \\sim N(0, \\sigma_T^2), \\quad \\varepsilon_{it} \\sim N(0, \\sigma^2)\n\\]\nWhere:\n\n\\(Y_{it}\\): Response for the \\(t\\)-th observation in the \\(i\\)-th group.\n\\(\\mu\\): Overall mean.\n\\(T_i\\): Random effect for the \\(i\\)-th group.\n\\(\\varepsilon_{it}\\): Random error term.\n\n\n\nKey Properties\n\nExpectation: \\(E[Y_{it}] = \\mu\\)\nVariance: \\(Var(Y_{it}) = \\sigma_T^2 + \\sigma^2\\)\nCovariance: \\(Cov(Y_{it}, Y_{is}) = \\sigma_T^2\\) for \\(t \\neq s\\) within the same group.\n\n\n\nANOVA for Random-Effects Models\nUnlike fixed-effects ANOVA, random-effects ANOVA decomposes the total variability into components attributable to random effects and residual error.\nANOVA Table:\n\n\n\n\n\n\n\n\n\n\nSource\nDegrees of Freedom\nSum of Squares (SS)\nMean Square (MS)\nExpected Mean Square\n\n\n\n\nTreatments\n\\(a - 1\\)\n\\(SS_T\\)\n\\(MS_T = \\frac{SS_T}{a - 1}\\)\n\\(a \\sigma_T^2 + \\sigma^2\\)\n\n\nError\n\\(a(n - 1)\\)\n\\(SS_E\\)\n\\(MS_E = \\frac{SS_E}{a(n - 1)}\\)\n\\(\\sigma^2\\)\n\n\nTotal\n\\(an - 1\\)\n\\(SS_{Total}\\)\n\n\n\n\n\nWhere:\n\n\\(a\\): Number of groups.\n\\(n\\): Number of observations per group.\n\n\n\nEstimation of Variance Components\nUsing Restricted Maximum Likelihood (REML), we estimate:\n\\[\n\\hat{\\sigma}^2_T = \\frac{MS_T - MS_E}{n}, \\quad \\hat{\\sigma}^2 = MS_E\n\\]\nExample in R:\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data\nset.seed(123)\ndata_single_random &lt;- data.frame(\n  group = factor(rep(1:5, each = 4)),\n  response = c(\n    rnorm(4, mean = 10, sd = 2),\n    rnorm(4, mean = 12, sd = 2),\n    rnorm(4, mean = 11, sd = 2),\n    rnorm(4, mean = 13, sd = 2),\n    rnorm(4, mean = 12, sd = 2)\n  )\n)\n\n# Fit Random-Effects Model\nmodel_random &lt;- lmer(response ~ 1 + (1 | group), data = data_single_random)\nsummary(model_random)\n\n# Extract Variance Components\nVarCorr(model_random)\nStep-by-Step Interpretation: 1. Model Fitting: The lmer function fits a random-intercept model where group is the random effect. 2. Summary Output: Provides estimates of fixed effects (overall mean) and random effects (variance components). 3. Variance Components: Extracted using VarCorr, showing the estimated variances for random effects and residuals."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#model-specification-1",
    "href": "lectures/week-12_mixed-models.html#model-specification-1",
    "title": "Mixed Models",
    "section": "Model Specification",
    "text": "Model Specification\n\\[\nY_{ij} = \\mu + \\alpha_i + T_j + \\varepsilon_{ij}\n\\]\nWhere:\n\n\\(Y_{ij}\\): Response for the \\(i\\)-th fixed effect level and \\(j\\)-th random effect level.\n\\(\\mu\\): Overall mean.\n\\(\\alpha_i\\): Fixed effect for the \\(i\\)-th level of a fixed factor.\n\\(T_j \\sim N(0, \\sigma_T^2)\\): Random effect for the \\(j\\)-th level of a random factor.\n\\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\): Random error term.\n\nIntuitive Example: Assessing the impact of different teaching methods (fixed effect) across various schools (random effect) on student performance.\n\nExample: Randomized Block Design with Random Blocks\n\\[\nY_{ij} = \\mu + \\alpha_i + B_j + \\varepsilon_{ij}, \\quad B_j \\sim N(0, \\sigma_B^2)\n\\]\nWhere:\n\nalpha_i: Fixed effect of treatment.\nB_j: Random effect of block (e.g., different classrooms).\n\n\n\nR Implementation\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data for Mixed-Effects Model\nset.seed(456)\ndata_mixed &lt;- data.frame(\n  treatment = factor(rep(c(\"A\", \"B\", \"C\"), times = 10)),\n  block = factor(rep(1:10, each = 3)),\n  response = c(\n    rnorm(3, mean = 5, sd = 1),\n    rnorm(3, mean = 6, sd = 1),\n    rnorm(3, mean = 7, sd = 1),\n    rnorm(3, mean = 5, sd = 1),\n    rnorm(3, mean = 6, sd = 1),\n    rnorm(3, mean = 7, sd = 1),\n    rnorm(3, mean = 5, sd = 1),\n    rnorm(3, mean = 6, sd = 1),\n    rnorm(3, mean = 7, sd = 1),\n    rnorm(3, mean = 5, sd = 1)\n  )\n)\n\n# Fit Mixed-Effects Model\nmodel_mixed &lt;- lmer(response ~ treatment + (1 | block), data = data_mixed)\nsummary(model_mixed)\n\n# Confidence Intervals for Fixed Effects\nconfint(model_mixed, parm = \"treatment\")\n\n# Diagnostics\npar(mfrow = c(2, 2))\nplot(model_mixed)\nInterpretation: - Fixed Effects (treatment): Assess the impact of different treatments on the response variable. - Random Effects (block): Account for variability across different blocks (e.g., schools, classrooms). - Confidence Intervals: Provide a range of plausible values for the fixed effects. - Diagnostics: Residual plots help verify model assumptions such as normality and homoscedasticity."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#key-assumptions",
    "href": "lectures/week-12_mixed-models.html#key-assumptions",
    "title": "Mixed Models",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nNormality: Random effects and residuals are normally distributed.\nIndependence: Observations are independent within and across groups.\nHomogeneity of Variance (Homoscedasticity): Variance is constant across all levels of fixed and random factors.\nNo Correlation: Random effects are uncorrelated with fixed effects."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#diagnostic-procedures",
    "href": "lectures/week-12_mixed-models.html#diagnostic-procedures",
    "title": "Mixed Models",
    "section": "Diagnostic Procedures",
    "text": "Diagnostic Procedures\n\nResidual Analysis\n\nResiduals vs. Fitted Values Plot: Checks for non-linearity, unequal error variances, and outliers.\nQ-Q Plot of Residuals: Assesses the normality of residuals.\nScale-Location Plot: Evaluates homoscedasticity.\nResiduals vs. Random Effects Plot: Detects any patterns that suggest violations of independence or homogeneity.\n\n\n\nVariance Components Estimation\n\nREML (Restricted Maximum Likelihood): Preferred method for estimating variance components as it accounts for the loss of degrees of freedom.\n\n\n\nR Implementation for Diagnostics\n# Fit the mixed-effects model\nmodel &lt;- lmer(response ~ treatment + (1 | block), data = data_mixed)\n\n# Diagnostic Plots\npar(mfrow = c(2, 2))\nplot(model)\n\n# Shapiro-Wilk Test for Normality of Residuals\nshapiro.test(resid(model))\n\n# Check for Homoscedasticity\nlibrary(car)\nleveneTest(response ~ treatment, data = data_mixed)\nInterpretation: - Normality Tests: Non-significant p-values indicate no evidence against normality. - Levene’s Test: Assesses equality of variances across groups; non-significant p-values suggest homoscedasticity. - Residual Plots: Should display random scatter without discernible patterns."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#fitting-random-effects-models",
    "href": "lectures/week-12_mixed-models.html#fitting-random-effects-models",
    "title": "Mixed Models",
    "section": "Fitting Random-Effects Models",
    "text": "Fitting Random-Effects Models\nRandom-effects models are essential when dealing with grouped or hierarchical data. Below is a step-by-step example of fitting a random-effects model using R.\n\nExample: Single Random Effect\nScenario: Assessing the effect of different diets on weight loss across various clinics. Clinics are randomly sampled from a larger population.\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data\nset.seed(123)\ndata_random &lt;- data.frame(\n  clinic = factor(rep(1:10, each = 5)),\n  diet = factor(rep(c(\"Diet1\", \"Diet2\", \"Diet3\", \"Diet4\", \"Diet5\"), times = 10)),\n  weight_loss = c(\n    rnorm(5, mean = 2, sd = 0.5),\n    rnorm(5, mean = 2.5, sd = 0.5),\n    rnorm(5, mean = 3, sd = 0.5),\n    rnorm(5, mean = 3.5, sd = 0.5),\n    rnorm(5, mean = 4, sd = 0.5),\n    rnorm(5, mean = 2, sd = 0.5),\n    rnorm(5, mean = 2.5, sd = 0.5),\n    rnorm(5, mean = 3, sd = 0.5),\n    rnorm(5, mean = 3.5, sd = 0.5),\n    rnorm(5, mean = 4, sd = 0.5)\n  )\n)\n\n# Fit Random-Effects Model\nmodel_random &lt;- lmer(weight_loss ~ 1 + (1 | clinic), data = data_random)\nsummary(model_random)\n\n# Extract Variance Components\nVarCorr(model_random)\nStep-by-Step Interpretation: 1. Data Preparation: Simulated data includes 10 clinics, each with 5 diet treatments. 2. Model Fitting: lmer fits a model with a random intercept for clinics. 3. Summary Output: Provides estimates for the overall mean, random effects variance, and residual variance. 4. Variance Components: VarCorr displays the estimated variances for the random effects and residuals."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#fitting-mixed-effects-models",
    "href": "lectures/week-12_mixed-models.html#fitting-mixed-effects-models",
    "title": "Mixed Models",
    "section": "Fitting Mixed-Effects Models",
    "text": "Fitting Mixed-Effects Models\nMixed-Effects Models accommodate both fixed and random factors, offering a versatile framework for complex data structures.\n\nExample: Two-Way Factorial with Random Blocks\nScenario: Investigating the effect of two factors (e.g., fertilizer type and watering frequency) on plant growth across randomly selected greenhouses.\n# Load necessary library\nlibrary(lme4)\n\n# Simulated Data\nset.seed(456)\ndata_mixed &lt;- data.frame(\n  greenhouse = factor(rep(1:15, each = 4)),\n  fertilizer = factor(rep(c(\"F1\", \"F2\"), each = 2, times = 15)),\n  watering = factor(rep(c(\"W1\", \"W2\"), times = 30)),\n  growth = c(\n    rnorm(60, mean = 10, sd = 1),\n    rnorm(60, mean = 12, sd = 1)\n  )\n)\n\n# Introduce Interaction Effect\ndata_mixed$growth &lt;- with(data_mixed, growth + ifelse(fertilizer == \"F2\" & watering == \"W2\", 2, 0))\n\n# Fit Mixed-Effects Model with Interaction\nmodel_mixed &lt;- lmer(growth ~ fertilizer * watering + (1 | greenhouse), data = data_mixed)\nsummary(model_mixed)\n\n# Estimated Marginal Means and Pairwise Comparisons\nlibrary(emmeans)\nemm &lt;- emmeans(model_mixed, ~ fertilizer * watering)\npairs(emm)\nInterpretation: - Fixed Effects (fertilizer, watering, fertilizer:watering): Assess the main effects and interaction between fertilizer type and watering frequency. - Random Effects (greenhouse): Account for variability across different greenhouses. - Interaction Effect: Significant interaction indicates that the effect of one factor depends on the level of the other factor. - Pairwise Comparisons: Identify specific group differences after accounting for other factors."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#example-resting-metabolic-rate",
    "href": "lectures/week-12_mixed-models.html#example-resting-metabolic-rate",
    "title": "Mixed Models",
    "section": "Example: Resting Metabolic Rate",
    "text": "Example: Resting Metabolic Rate\n\nScenario\nA study aims to compare three different exercise protocols (Treatments A, B, C) on subjects’ resting metabolic rates. To control for age-related variability, subjects are grouped into four blocks based on age groups.\n\n\nStep-by-Step R Code\n\n1. Load and Prepare Data\n# Set seed for reproducibility\nset.seed(789)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:4, each = 3))\ntreatments &lt;- factor(rep(c(\"A\", \"B\", \"C\"), times = 4))\n\n# Simulate response variable (resting metabolic rate in kcal/day)\nresponse &lt;- c(\n  rnorm(3, mean = 1500, sd = 50),  # Block 1\n  rnorm(3, mean = 1550, sd = 50),  # Block 2\n  rnorm(3, mean = 1480, sd = 50),  # Block 3\n  rnorm(3, mean = 1520, sd = 50)   # Block 4\n)\n\n# Create data frame\ndata_rmr &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data_rmr)\nOutput Interpretation: The dataset comprises four blocks (age groups), each containing three treatments (A, B, C). The response variable represents the resting metabolic rate, simulated with slight block-specific means to reflect age-related variability.\n\n\n2. Fit the RCBD Model\n# Fit the RCBD model using ANOVA\nmodel_rcbd &lt;- aov(response ~ blocks + treatments, data = data_rmr)\n\n# Summary of the model\nsummary(model_rcbd)\nInterpretation: - Blocks: Capture variability due to different age groups. - Treatments: Assess the effect of exercise protocols after accounting for age-related variability. - Error: Represents unexplained variability in resting metabolic rates.\n\n\n3. Diagnostic Plots\n# Diagnostic plots for the ANOVA model\npar(mfrow = c(2, 2))\nplot(model_rcbd)\nInterpretation: Examine residual plots to ensure that assumptions of normality, homoscedasticity, and independence are met. Look for random scatter in Residuals vs Fitted and Q-Q plots to assess normality.\n\n\n4. Post-Hoc Analysis (Multiple Comparisons)\n# Load emmeans package for estimated marginal means\nlibrary(emmeans)\n\n# Estimated marginal means for treatments\nemm_rmr &lt;- emmeans(model_rcbd, ~ treatments)\n\n# Pairwise comparisons\npairwise_comparisons_rmr &lt;- contrast(emm_rmr, method = \"pairwise\")\nsummary(pairwise_comparisons_rmr)\nInterpretation: Identify which exercise protocols significantly differ in their effect on resting metabolic rates after adjusting for age-related variability."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#variance-component-estimation",
    "href": "lectures/week-12_mixed-models.html#variance-component-estimation",
    "title": "Mixed Models",
    "section": "Variance Component Estimation",
    "text": "Variance Component Estimation\n\nDeriving Expected Mean Squares\nIn random-effects models, the Expected Mean Squares (EMS) are fundamental for estimating variance components.\n\n\nRandom-Effects One-Way Model\n\\[\nY_{it} = \\mu + T_i + \\varepsilon_{it}, \\quad T_i \\sim N(0, \\sigma_T^2), \\quad \\varepsilon_{it} \\sim N(0, \\sigma^2)\n\\]\nExpected Mean Squares:\n\\[\nEMS_T = \\sigma_T^2 + \\sigma^2\n\\]\n\\[\nEMS_E = \\sigma^2\n\\]\nDerivation:\n\nFor Treatments:\n\n\\[\nEMS_T = Var(T_i) + Var(\\varepsilon_{it}) = \\sigma_T^2 + \\sigma^2\n\\]\n\nFor Error:\n\n\\[\nEMS_E = Var(\\varepsilon_{it}) = \\sigma^2\n\\]\nEstimation:\nUsing ANOVA sums of squares, we set up equations based on EMS to solve for variance components.\n\\[\nMS_T = \\sigma_T^2 + \\sigma^2\n\\]\n\\[\nMS_E = \\sigma^2\n\\]\nSolving for \\(\\sigma_T^2\\):\n\\[\n\\hat{\\sigma}_T^2 = MS_T - MS_E\n\\]\nConclusion: This derivation highlights how ANOVA estimates variance components by leveraging the properties of expected mean squares.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#confidence-intervals-for-variance-components",
    "href": "lectures/week-12_mixed-models.html#confidence-intervals-for-variance-components",
    "title": "Mixed Models",
    "section": "Confidence Intervals for Variance Components",
    "text": "Confidence Intervals for Variance Components\n\nSatterthwaite Approximation\nThe Satterthwaite Approximation provides a method to estimate the degrees of freedom for variance components, facilitating the construction of confidence intervals.\n\n\nSteps\n\nEstimate Variance Components: Using REML or other estimation methods.\nCalculate Degrees of Freedom: Approximated based on the variance component estimates.\nConstruct Confidence Intervals:\n\n\\[\nCI = \\hat{\\sigma}^2 \\pm t_{\\alpha/2, df} \\times SE(\\hat{\\sigma}^2)\n\\]\nWhere:\n\n\\(\\hat{\\sigma}^2\\): Estimated variance component.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(SE(\\hat{\\sigma}^2)\\): Standard error of the variance component estimate.\n\nExample in R:\n# Fit the model\nmodel &lt;- lmer(response ~ 1 + (1 | group), data = data_random)\n\n# Extract variance components\nvar_components &lt;- VarCorr(model)\nsigma_T_sq &lt;- as.numeric(var_components$group)\nsigma_sq &lt;- attr(var_components, \"sc\")^2\n\n# Estimate standard errors using the lme4 package's built-in functions\nconfint(model, parm = \"theta_\", method = \"Wald\")\nInterpretation: The confint function provides confidence intervals for the variance components, allowing for inference about the population variability.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#medium-difficulty",
    "href": "lectures/week-12_mixed-models.html#medium-difficulty",
    "title": "Mixed Models",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + \n     ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n     0.5 * (X - mean(X)) + \n     rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#challenging-problems",
    "href": "lectures/week-12_mixed-models.html#challenging-problems",
    "title": "Mixed Models",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\n1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\nR Code Example:\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     0.5 * (X - mean(X)) + \n     rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points:\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\nExpected Insights:\n\nType II SS: Often preferred when interactions are not of primary interest. It provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced. It accounts for all factors simultaneously, providing conditional main effects.\nImbalance Effects: Can distort the Type I SS results, leading to misleading conclusions. Type II and III SS offer more robust alternatives in unbalanced settings.\n\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\n2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\nR Code Example:\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n     0.5 * (X - mean(X)) + \n     ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n            ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \n     rnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ FactorC) +\n  labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n  theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\nInterpretation:\n\nThree-Way Interaction:\n\nThe interaction plot demonstrates how the interaction between Factor A and Factor B varies across different levels of Factor C.\nNon-parallel lines across facets indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation.\nThe effect of one factor depends on the combination of other factors, necessitating a comprehensive analysis.\n\nPractical Implications:\n\nUnderstanding three-way interactions provides nuanced insights into how multiple factors jointly influence the response variable.\nThis can inform more sophisticated decision-making and strategy development in experimental settings.\n\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer.\n\n\n3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nProof Outline:\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)_{ik}\\) and \\((\\beta\\gamma)_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)_{ij} = (\\alpha\\gamma)_{ik} = (\\beta\\gamma)_{jk} = (\\alpha\\beta\\gamma)_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), \\((\\beta\\gamma)_{jk}\\), and \\((\\alpha\\beta\\gamma)_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#variance-calculations-for-latin-square-designs",
    "href": "lectures/week-12_mixed-models.html#variance-calculations-for-latin-square-designs",
    "title": "Mixed Models",
    "section": "Variance Calculations for Latin Square Designs",
    "text": "Variance Calculations for Latin Square Designs\nIn a Latin Square Design with \\(v\\) treatments, the variance of the estimated treatment effects is influenced by the design’s structure.\n\nFormula\nFor a Latin Square with \\(v\\) treatments:\n\\[\nVar(\\tau_i) = \\frac{\\sigma^2}{v}\n\\]\n\n\nDerivation\n\nModel Setup:\nThe ANOVA model for a Latin Square is:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\tau_k + \\epsilon_{ijk},\n\\]\nwhere \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\).\n\nEstimator for Treatment Effect (\\(\\tau_k\\)):\nThe treatment effects are estimated as:\n\n\\[\n\\hat{\\tau}_k = \\bar{Y}_{\\cdot \\cdot k} - \\bar{Y}_{\\cdot \\cdot \\cdot},\n\\]\nwhere \\(\\bar{Y}_{\\cdot \\cdot k}\\) is the mean response for treatment \\(k\\) and \\(\\bar{Y}_{\\cdot \\cdot \\cdot}\\) is the grand mean.\n\nVariance of \\(\\hat{\\tau}_k\\):\nSince each treatment is replicated \\(v\\) times (once per row and column):\n\n\\[\nVar(\\hat{\\tau}_k) = \\frac{\\sigma^2}{v}\n\\]\nConclusion: The variance of the treatment effect estimates decreases with increasing number of treatments in the Latin Square, enhancing the precision of the estimates.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#degrees-of-freedom-in-youden-designs",
    "href": "lectures/week-12_mixed-models.html#degrees-of-freedom-in-youden-designs",
    "title": "Mixed Models",
    "section": "Degrees of Freedom in Youden Designs",
    "text": "Degrees of Freedom in Youden Designs\nFor a Youden Design with parameters \\(v = 4\\) treatments, \\(b = 4\\) blocks (rows), and \\(c = 3\\) columns, the degrees of freedom for the error term are calculated as follows:\n\nFormula\n\\[\ndf_{\\text{Error}} = bc - b - c - v + 2\n\\]\n\n\nCalculation\nGiven:\n\n\\(v = 4\\)\n\\(b = 4\\)\n\\(c = 3\\)\n\nPlugging into the formula:\n\\[\ndf_{\\text{Error}} = (4 \\times 3) - 4 - 3 - 4 + 2 = 12 - 4 - 3 - 4 + 2 = 3\n\\]\nInterpretation: The error degrees of freedom indicate the number of independent pieces of information available to estimate the error variance. In this case, with 3 error degrees of freedom, the design is sufficiently constrained to estimate treatment effects after accounting for row and column effects.\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#proof-of-interaction-contrasts",
    "href": "lectures/week-12_mixed-models.html#proof-of-interaction-contrasts",
    "title": "Mixed Models",
    "section": "Proof of Interaction Contrasts",
    "text": "Proof of Interaction Contrasts\n\nInteraction Contrasts Measure Deviations from Additivity\nDefinition: In a three-factor design, interaction contrasts quantify how the combined effect of three factors deviates from the additive effects of each factor individually and their two-way interactions.\n\n\nProof Steps\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)_{ik}\\) and \\((\\beta\\gamma)_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)_{ij} = (\\alpha\\gamma)_{ik} = (\\beta\\gamma)_{jk} = (\\alpha\\beta\\gamma)_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), \\((\\beta\\gamma)_{jk}\\), and \\((\\alpha\\beta\\gamma)_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#adjusted-treatment-means",
    "href": "lectures/week-12_mixed-models.html#adjusted-treatment-means",
    "title": "Mixed Models",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe Adjusted Mean for treatment \\(i\\) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate.\n\n\nDerivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes",
    "href": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes",
    "title": "Mixed Models",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)_{hi} + \\epsilon_{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Procedure\n\nFit Both Models:\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\nCompare Models Using ANOVA:\n# Fit reduced model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Fit full model with interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare models\nanova(model_reduced, model_full)\nDecision Rule:\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject \\(H_0\\) and conclude that slopes are not homogeneous.\nIf not significant, accept \\(H_0\\) and proceed with the reduced model.\n\n\nConclusion: A significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference: Scheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Mixed Models",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\nCI = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#medium-difficulty-1",
    "href": "lectures/week-12_mixed-models.html#medium-difficulty-1",
    "title": "Mixed Models",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\n1. Two-Way ANCOVA with Interaction\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate \\(X\\).\nFit an ANCOVA model adjusting for \\(X\\).\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + \n     ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + \n     0.5 * (X - mean(X)) + \n     rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\nQuestions:\n\nMain Effects:\n\nTreatment: What are the estimated effects of treatments A, B, and C?\nCovariate \\(X\\): How does \\(X\\) influence the response variable \\(Y\\)?\n\nSignificance Testing:\n\nAre the treatment effects statistically significant after adjusting for \\(X\\)?\nIs the covariate \\(X\\) a significant predictor of \\(Y\\)?\n\nDiagnostic Plots:\n\nDo the residual plots indicate any violations of ANCOVA assumptions such as non-linearity, heteroscedasticity, or non-normality?\nAre there any influential observations that may affect the model’s validity?\n\nInterpretation of Pairwise Comparisons:\n\nWhich treatments significantly differ from each other after adjusting for \\(X\\)?\nHow do the adjusted means compare across treatments?\n\n\nExpected Answers:\n\nMain Effects:\n\nTreatments B and C have positive effects relative to Treatment A.\nCovariate \\(X\\) has a positive influence on \\(Y\\), indicating that as \\(X\\) increases, \\(Y\\) tends to increase.\n\nSignificance Testing:\n\nIf the p-values for Treatment are below the significance level (e.g., 0.05), treatments have significant effects.\nIf the p-value for \\(X\\) is below the significance level, \\(X\\) is a significant predictor.\n\nDiagnostic Plots:\n\nRandom scatter in Residuals vs Fitted suggests homoscedasticity and linearity.\nQ-Q plot close to the diagonal indicates normality of residuals.\nNo points with high leverage or large residuals indicate no influential observations.\n\nInterpretation of Pairwise Comparisons:\n\nSignificant pairwise differences indicate which specific treatments differ after adjusting for \\(X\\).\nAdjusted means provide a clearer comparison by accounting for the covariate’s effect."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#challenging-problems-1",
    "href": "lectures/week-12_mixed-models.html#challenging-problems-1",
    "title": "Mixed Models",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\n1. Unbalanced Two-Factor ANCOVA Analysis\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\nR Code Example:\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     0.5 * (X - mean(X)) + \n     rnorm(15, mean=0, sd=3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)\nDiscussion Points:\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\nExpected Insights:\n\nType II SS: Often preferred when interactions are not of primary interest. It provides unbiased estimates of main effects under the assumption of no interaction.\nType III SS: Necessary when interactions are present or when the design is unbalanced. It accounts for all factors simultaneously, providing conditional main effects.\nImbalance Effects: Can distort the Type I SS results, leading to misleading conclusions. Type II and III SS offer more robust alternatives in unbalanced settings.\n\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\n\n\n2. Three-Factor Interaction Interpretation\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\nR Code Example:\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + \n     ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + \n     ifelse(FactorB == \"B1\", 0, 3) + \n     ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + \n     0.5 * (X - mean(X)) + \n     ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, \n            ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + \n     rnorm(72, mean=0, sd=3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x=FactorA, y=Y, color=FactorB, group=FactorB)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ FactorC) +\n  labs(title=\"Three-Way Interaction Plot\", x=\"Factor A\", y=\"Response\") +\n  theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)\nInterpretation:\n\nThree-Way Interaction:\n\nThe interaction plot demonstrates how the interaction between Factor A and Factor B varies across different levels of Factor C.\nNon-parallel lines across facets indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation.\nThe effect of one factor depends on the combination of other factors, necessitating a comprehensive analysis.\n\nPractical Implications:\n\nUnderstanding three-way interactions provides nuanced insights into how multiple factors jointly influence the response variable.\nThis can inform more sophisticated decision-making and strategy development in experimental settings.\n\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer.\n\n\n3. Mathematical Proof of Interaction Contrasts\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations.\n\nProof Outline:\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)_{ij}\\): Measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)_{ik}\\) and \\((\\beta\\gamma)_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\): Measures the deviation from additivity among all three factors.\n\n\nProve Deviations from Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)_{ij} = (\\alpha\\gamma)_{ik} = (\\beta\\gamma)_{jk} = (\\alpha\\beta\\gamma)_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\n This represents a purely additive model where the combined effect of factors is the sum of their individual effects.\n\nNon-Additive Model: If any of the interaction terms are non-zero, the model deviates from additivity, indicating that the effect of one factor depends on the levels of other factors.\n\n\nConclusion:\nInteraction contrasts (\\((\\alpha\\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), \\((\\beta\\gamma)_{jk}\\), and \\((\\alpha\\beta\\gamma)_{ijk}\\)) measure the extent to which the observed responses deviate from an additive model. Non-zero contrasts signify the presence of interactions, emphasizing the importance of considering these higher-order effects in experimental analysis.\n\nReference: Dean, Voss & Draguljić (2017). Design and Analysis of Experiments. Springer."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#adjusted-treatment-means-1",
    "href": "lectures/week-12_mixed-models.html#adjusted-treatment-means-1",
    "title": "Mixed Models",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe Adjusted Mean for treatment \\(i\\) accounts for the influence of blocking factors, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate.\n\n\nDerivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery, D. C., & Runger, G. C. (2020). Applied Statistics and Probability for Engineers. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes-1",
    "href": "lectures/week-12_mixed-models.html#f-test-for-homogeneity-of-slopes-1",
    "title": "Mixed Models",
    "section": "F-Test for Homogeneity of Slopes",
    "text": "F-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)_{hi} + \\epsilon_{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Procedure\n\nFit Both Models:\n\nReduced Model: Without interaction.\nFull Model: With interaction.\n\nCompare Models Using ANOVA:\n# Fit reduced model\nmodel_reduced &lt;- lm(Time ~ fColor + x, data = balloon.data)\n\n# Fit full model with interaction\nmodel_full &lt;- lm(Time ~ fColor * x, data = balloon.data)\n\n# Compare models\nanova(model_reduced, model_full)\nDecision Rule:\n\nIf the interaction term is significant (\\(p &lt; \\alpha\\)), reject \\(H_0\\) and conclude that slopes are not homogeneous.\nIf not significant, accept \\(H_0\\) and proceed with the reduced model.\n\n\nConclusion: A significant interaction term indicates that the effect of the covariate varies across treatments, violating the homogeneity of slopes assumption and necessitating a more complex model.\nReference: Scheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts-1",
    "href": "lectures/week-12_mixed-models.html#confidence-intervals-for-higher-order-contrasts-1",
    "title": "Mixed Models",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\nCI = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\nUsing the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval:\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\nExample:\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~ Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\nInterpretation: The output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "syllabus/schedule.html",
    "href": "syllabus/schedule.html",
    "title": "STAT 454/545 - Course Schedule",
    "section": "",
    "text": "The following is a tentative schedule for the course, outlining the topics covered each week, the corresponding reading assignments, and the due dates for assignments and assessments. This schedule is subject to change, and students should refer to the Canvas Assignments and announcements for the most up-to-date information.\nHowever, the final exam date is not subject to change, as it is determined and scheduled by the university. Please plan accordingly based on the final exam schedule provided by UNM:\n\nFinal Exam Date: Tuesday, May 13, 2025, 10:00 a.m.–12:00 p.m.\nLocation: Classroom (as specified in the syllabus)\n\nStudents are strongly encouraged to stay proactive, monitor changes, and ensure they meet all revised deadlines and expectations as announced.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReadings & Homework Assignments\n\n\n\n\n1\nJan. 21 & 23, 2025\nIntroduction to Experimental Design\nCh. 1 (Sections 1.1–1.1.1), Ch. 2 (Sections 2.1–2.2).\n\n\n2\nJan. 28 & 30, 2025\nCompletely Randomized Designs (CRD)\nCh. 3 (Sections 3.1–3.5); Homework 1 (due Feb. 6).\n\n\n3\nFeb. 4 & 6, 2025\nMultiple Comparisons\nCh. 4 (Sections 4.1–4.4); Homework 2 (due Feb. 13).\n\n\n4\nFeb. 11 & 13, 2025\nChecking Model Assumptions\nCh. 5 (Sections 5.1–5.3); Homework 3 (due Feb. 20).\n\n\n5\nFeb. 18 & 20, 2025\nTwo-Way Factorial Designs\nCh. 6 (Sections 6.1–6.5); Homework 4 (due Feb. 27).\n\n\n6\nFeb. 25 & 27, 2025\nHigher-Order Factorial Designs\nCh. 7 (Sections 7.1–7.6); Homework 5 (due Mar. 6).\n\n\n7\nMar. 4 & 6, 2025\nMidterm Exam\nReview all chapters and notes.\n\n\n8\nMar. 11 & 13, 2025\nBlocking & RCBD\nCh. 10 (Sections 10.1–10.4); Homework 6 (due Mar. 25).\n\n\n9\nMar. 16–20, 2025\n==Spring Break==\nNo classes.\n\n\n10\nMar. 25 & 27, 2025\nComplete Block Designs\nCh. 10 (Sections 10.6–10.8); Homework 7 (due Apr. 3).\n\n\n11\nApr. 1 & 3, 2025\nAnalysis of Covariance (ANCOVA)\nCh. 9 (Sections 9.1–9.4); Homework 8 (due Apr. 10).\n\n\n12\nApr. 8 & 10, 2025\nRow–Column (Latin square) Designs\nCh. 12 (Sections 12.1–12.4); Homework 9 (due Apr. 17).\n\n\n13\nApr. 15 & 17, 2025\nRandom & Mixed Models\nCh. 17 (Sections 17.1–17.8); Homework 10 (due Apr. 24).\n\n\n14\nApr. 22 & 24, 2025\nNested Models\nCh. 18 (Sections 18.1–18.4); Homework 11 (due May 1).\n\n\n15\nApr. 29 & May 1, 2025\nSplit-Plot Designs\nCh. 19 (Sections 19.1–19.5); Homework 12 (due May 6).\n\n\n16\nMay 6 & 8, 2025\nCatching Up and Review\nComprehensive review and Q&A.\n\n\n17\nMay 13\nFinal Exam\nComprehensive (Weeks 1–15)."
  },
  {
    "objectID": "syllabus/schedule.html#tentative-course-schedule",
    "href": "syllabus/schedule.html#tentative-course-schedule",
    "title": "STAT 454/545 - Course Schedule",
    "section": "",
    "text": "Week\nDate\nTopics\nReadings & Homework Assignments\n\n\n\n\n1\nJan. 21 & 23, 2025\nIntroduction to Experimental Design\nCh. 1 (Sections 1.1–1.1.1), Ch. 2 (Sections 2.1–2.2).\n\n\n2\nJan. 28 & 30, 2025\nCompletely Randomized Designs (CRD)\nCh. 3 (Sections 3.1–3.5); Homework 1 (due Feb. 6).\n\n\n3\nFeb. 4 & 6, 2025\nMultiple Comparisons\nCh. 4 (Sections 4.1–4.4); Homework 2 (due Feb. 13).\n\n\n4\nFeb. 11 & 13, 2025\nChecking Model Assumptions\nCh. 5 (Sections 5.1–5.3); Homework 3 (due Feb. 20).\n\n\n5\nFeb. 18 & 20, 2025\nTwo-Way Factorial Designs\nCh. 6 (Sections 6.1–6.5); Homework 4 (due Feb. 27).\n\n\n6\nFeb. 25 & 27, 2025\nHigher-Order Factorial Designs\nCh. 7 (Sections 7.1–7.6); Homework 5 (due Mar. 6).\n\n\n7\nMar. 4 & 6, 2025\nMidterm Exam\nReview all chapters and notes.\n\n\n8\nMar. 11 & 13, 2025\nBlocking & RCBD\nCh. 10 (Sections 10.1–10.4); Homework 6 (due Mar. 25).\n\n\n9\nMar. 16–20, 2025\n==Spring Break==\nNo classes.\n\n\n10\nMar. 25 & 27, 2025\nComplete Block Designs\nCh. 10 (Sections 10.6–10.8); Homework 7 (due Apr. 3).\n\n\n11\nApr. 1 & 3, 2025\nAnalysis of Covariance (ANCOVA)\nCh. 9 (Sections 9.1–9.4); Homework 8 (due Apr. 10).\n\n\n12\nApr. 8 & 10, 2025\nRow–Column (Latin square) Designs\nCh. 12 (Sections 12.1–12.4); Homework 9 (due Apr. 17).\n\n\n13\nApr. 15 & 17, 2025\nRandom & Mixed Models\nCh. 17 (Sections 17.1–17.8); Homework 10 (due Apr. 24).\n\n\n14\nApr. 22 & 24, 2025\nNested Models\nCh. 18 (Sections 18.1–18.4); Homework 11 (due May 1).\n\n\n15\nApr. 29 & May 1, 2025\nSplit-Plot Designs\nCh. 19 (Sections 19.1–19.5); Homework 12 (due May 6).\n\n\n16\nMay 6 & 8, 2025\nCatching Up and Review\nComprehensive review and Q&A.\n\n\n17\nMay 13\nFinal Exam\nComprehensive (Weeks 1–15)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Instructor: 📚 Davood Tofighi, Ph.D.\n\nLocation: 📍 Science Math Learning Center, Room 120\n\nClass Times: 🕒 Tuesdays and Thursdays, 2:00 PM - 3:15 PM\n\nOffice Hours: 📅 By appointment\n\nCourse Website: UNM Canvas\n\nCourse GitHub Page: 🌐 Course GitHub Page\n\n\n\nThis course introduces key principles of experimental design and analysis of variance (ANOVA) integrated with regression models. Key topics include:\n\nRandomization, replication, and blocking principles\n\nOne-way and factorial ANOVA models\n\nManaging unbalanced data and covariates\n\nAdvanced designs: Latin square, split-plot, nested, and mixed-effects models\n\nDiagnostics: residual analysis, transformations, and multiple comparisons\n\n\n\n\n\nPrimary Textbook:\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nAccess the eBook via Springer eBooks.\n\nSupplemental Readings:\n\nMaterials in the public domain will be shared on the Contents page of the course GitHub site.\nNote: Some readings may not be shared due to copyright restrictions.\n\n\n\n\n\n\n📄 Syllabus: Full Course Syllabus (PDF)\n\n📝 Lecture Notes: Available weekly on the Course GitHub Page\n\n\n\n\n\n\n\nPlease Read the Syllabus\n\n\n\nThis page provides an overview of the course, but not all critical information is listed here. You are required to read the full syllabus for policies, grading details, and due dates.\n\n\n\n\n\n\n\n\nGitHub Cache Issue\n\n\n\nGitHub caches pages, so refresh your browser to access the latest content:\n- Windows/Linux: Press Ctrl + Shift + R\n- Mac: Press Cmd + Shift + R\nFor persistent issues, clear your browser cache or use an incognito window.\n\n\n\nAssignments: Posted and submitted through Canvas Assignments.\n🛠️ Software Tools:\n\nR Project Download\n\nRStudio IDE\n\nVS Code Editor\n\n\n\n\n\n\n\n\nComponent\nWeight\n\n\n\n\n🏠 Homework Assignments\n50%\n\n\n📊 Midterm Exam\n20%\n\n\n🏁 Final Exam\n30%\n\n\n⭐ Extra Credit\nUp to 4%\n\n\n\n\n\n\n\nMidterm Exam:\n\n📅 Date: Thursday, March 6, 2025\n\n📚 Coverage: Topics from Weeks 1-7\n\nFinal Exam:\n\n📅 Date: Tuesday, May 13, 2025, 10:00 AM - 12:00 PM\n\n📚 Coverage: Comprehensive (Weeks 1-15)\n\n\n\n\n\n🚫 No make-up exams unless in the case of proven emergencies as outlined in the syllabus and UNM policies.\n\n📝 Notify the instructor before the exam date if a conflict arises.\n\n📄 Exams are closed book, but you are allowed:\n\nUp to three double-sided cheat sheets (US Letter size).\n\nA calculator (no cell phones or other electronics).\n\n\n\n\n\n\n\n🔔 Announcements:\nAll course updates will be posted on Canvas. You are responsible for checking regularly.\n📬 Questions:\n\nPost general questions on the Canvas Discussion Board.\n\nFor personal matters, contact the instructor via Canvas Email.\n\n\n\n\n\n\nAccessibility Services:\nContact Accessibility Resource Center at arcsrvs@unm.edu or call 277-3506.\nUNM Policies and Ethics:\n\nStudent Code of Conduct\n\nAcademic Honesty Policy\n\nTitle IX Resources:\nVisit the Office of Equal Opportunity for Title IX-related support.\n\n\n\n\nFor personal matters, email the instructor via Canvas Email."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "This course introduces key principles of experimental design and analysis of variance (ANOVA) integrated with regression models. Key topics include:\n\nRandomization, replication, and blocking principles\n\nOne-way and factorial ANOVA models\n\nManaging unbalanced data and covariates\n\nAdvanced designs: Latin square, split-plot, nested, and mixed-effects models\n\nDiagnostics: residual analysis, transformations, and multiple comparisons"
  },
  {
    "objectID": "index.html#required-textbook-and-materials",
    "href": "index.html#required-textbook-and-materials",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Primary Textbook:\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nAccess the eBook via Springer eBooks.\n\nSupplemental Readings:\n\nMaterials in the public domain will be shared on the Contents page of the course GitHub site.\nNote: Some readings may not be shared due to copyright restrictions."
  },
  {
    "objectID": "index.html#key-course-links",
    "href": "index.html#key-course-links",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "📄 Syllabus: Full Course Syllabus (PDF)\n\n📝 Lecture Notes: Available weekly on the Course GitHub Page\n\n\n\n\n\n\n\nPlease Read the Syllabus\n\n\n\nThis page provides an overview of the course, but not all critical information is listed here. You are required to read the full syllabus for policies, grading details, and due dates.\n\n\n\n\n\n\n\n\nGitHub Cache Issue\n\n\n\nGitHub caches pages, so refresh your browser to access the latest content:\n- Windows/Linux: Press Ctrl + Shift + R\n- Mac: Press Cmd + Shift + R\nFor persistent issues, clear your browser cache or use an incognito window.\n\n\n\nAssignments: Posted and submitted through Canvas Assignments.\n🛠️ Software Tools:\n\nR Project Download\n\nRStudio IDE\n\nVS Code Editor"
  },
  {
    "objectID": "index.html#grading-and-assessments",
    "href": "index.html#grading-and-assessments",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Component\nWeight\n\n\n\n\n🏠 Homework Assignments\n50%\n\n\n📊 Midterm Exam\n20%\n\n\n🏁 Final Exam\n30%\n\n\n⭐ Extra Credit\nUp to 4%"
  },
  {
    "objectID": "index.html#exams-and-policies",
    "href": "index.html#exams-and-policies",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Midterm Exam:\n\n📅 Date: Thursday, March 6, 2025\n\n📚 Coverage: Topics from Weeks 1-7\n\nFinal Exam:\n\n📅 Date: Tuesday, May 13, 2025, 10:00 AM - 12:00 PM\n\n📚 Coverage: Comprehensive (Weeks 1-15)\n\n\n\n\n\n🚫 No make-up exams unless in the case of proven emergencies as outlined in the syllabus and UNM policies.\n\n📝 Notify the instructor before the exam date if a conflict arises.\n\n📄 Exams are closed book, but you are allowed:\n\nUp to three double-sided cheat sheets (US Letter size).\n\nA calculator (no cell phones or other electronics)."
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "🔔 Announcements:\nAll course updates will be posted on Canvas. You are responsible for checking regularly.\n📬 Questions:\n\nPost general questions on the Canvas Discussion Board.\n\nFor personal matters, contact the instructor via Canvas Email."
  },
  {
    "objectID": "index.html#support-and-resources",
    "href": "index.html#support-and-resources",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Accessibility Services:\nContact Accessibility Resource Center at arcsrvs@unm.edu or call 277-3506.\nUNM Policies and Ethics:\n\nStudent Code of Conduct\n\nAcademic Honesty Policy\n\nTitle IX Resources:\nVisit the Office of Equal Opportunity for Title IX-related support."
  },
  {
    "objectID": "index.html#contact-information",
    "href": "index.html#contact-information",
    "title": "📊 STAT 454/545: Analysis of Variance and Experimental Design",
    "section": "",
    "text": "For personal matters, email the instructor via Canvas Email."
  },
  {
    "objectID": "r_help.html",
    "href": "r_help.html",
    "title": "R Help",
    "section": "",
    "text": "Welcome to the R Help Section. This resource organizes R help files into structured categories based on Subjects, Topics, and implementation through R Base and Packages.\n\n\n\nBelow are the available subjects and their corresponding topics. Select a topic to view detailed help pages.\n\n\n\nData Manipulation\n\nCreating Data Frames\nWorking with Factors\nSubsetting Data\nData Transformation and Wrangling\n\nData Visualization\n\nBase R Graphics\nggplot2 Basics\n\nStatistical Analysis\n\nHypothesis Testing\nRegression\nANOVA\n\nProgramming with R\n\nControl Structures\nWriting Functions\nDebugging\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr\ntidyr\n\n\n\n\n\nggplot2 \n\n\n\n\n\nemmeans\nmultcomp\npwr\n\n\n\n\n\n\n\n\nExplore Subjects and Topics: Use the Subjects and Topics section to browse R concepts by category.\nR Base Help: Click on the R Base links to learn about core R functions.\nPackages Help: Dive into R packages to explore advanced features and functionalities.\nAppendix Folder Structure:\n\nR Base Functions: appendix/r-base/\nR Packages: appendix/r-packages/\n\n\n\n\n\n\nHave suggestions or found an issue? Reach out to us via GitHub Discussions.\n\n\n\n\n\nHome\nAppendix"
  },
  {
    "objectID": "r_help.html#r-subjects-topics",
    "href": "r_help.html#r-subjects-topics",
    "title": "R Help",
    "section": "",
    "text": "Below are the available subjects and their corresponding topics. Select a topic to view detailed help pages.\n\n\n\nData Manipulation\n\nCreating Data Frames\nWorking with Factors\nSubsetting Data\nData Transformation and Wrangling\n\nData Visualization\n\nBase R Graphics\nggplot2 Basics\n\nStatistical Analysis\n\nHypothesis Testing\nRegression\nANOVA\n\nProgramming with R\n\nControl Structures\nWriting Functions\nDebugging"
  },
  {
    "objectID": "r_help.html#r-packages",
    "href": "r_help.html#r-packages",
    "title": "R Help",
    "section": "",
    "text": "dplyr\ntidyr\n\n\n\n\n\nggplot2 \n\n\n\n\n\nemmeans\nmultcomp\npwr"
  },
  {
    "objectID": "r_help.html#how-to-use-this-help",
    "href": "r_help.html#how-to-use-this-help",
    "title": "R Help",
    "section": "",
    "text": "Explore Subjects and Topics: Use the Subjects and Topics section to browse R concepts by category.\nR Base Help: Click on the R Base links to learn about core R functions.\nPackages Help: Dive into R packages to explore advanced features and functionalities.\nAppendix Folder Structure:\n\nR Base Functions: appendix/r-base/\nR Packages: appendix/r-packages/"
  },
  {
    "objectID": "r_help.html#feedback",
    "href": "r_help.html#feedback",
    "title": "R Help",
    "section": "",
    "text": "Have suggestions or found an issue? Reach out to us via GitHub Discussions."
  },
  {
    "objectID": "r_help.html#navigation",
    "href": "r_help.html#navigation",
    "title": "R Help",
    "section": "",
    "text": "Home\nAppendix"
  },
  {
    "objectID": "home_lectures.html",
    "href": "home_lectures.html",
    "title": "Lecture Notes - STAT 454/545",
    "section": "",
    "text": "Welcome to the Lecture Notes page for STAT 454/545: Analysis of Variance and Experimental Design. Here, you will find weekly lecture notes and additional resources in HTML format, accessible via the course GitHub page.\n\n\n\n\n\n\nWeek\nTopic\nLecture Notes Link\n\n\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n2\nCompletely Randomized Designs (CRD)\nWeek 2 Notes: Completely Randomized Designs (CRD)\n\n\n3\nMultiple Comparisons\nWeek 3 Notes: Multiple Comparisons\n\n\n4\nChecking Model Assumptions\nWeek 4 Notes: Checking Model Assumptions\n\n\n5\nTwo-Way Factorial Designs\nWeek 5 Notes: Two-Way Factorial Designs\n\n\n6\nHigher-Order Factorial Designs\nWeek 6 Notes: Higher-Order Factorial Designs\n\n\n7\nMidterm Review\nWeek 7 Notes: Midterm Review\n\n\n8\nBlocking & RCBD\nWeek 8 Notes: Blocking & RCBD\n\n\n9\nComplete Block Designs\nWeek 9 Notes: Complete Block Designs\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 101 Notes: Analysis of Covariance (ANCOVA)\n\n\n11\nRow–Column Designs (Latin Square)\nWeek 11 Notes: Row–Column Designs (Latin Square)\n\n\n12\nRandom & Mixed Models\nWeek 12 Notes: Random & Mixed Models\n\n\n13\nNested Models\nWeek 13 Notes: Nested Models\n\n\n14\nSplit-Plot Designs\nWeek 14 Notes: Split-Plot Designs\n\n\n15\nCatch-Up and Review\nWeek 15 Notes: Catch-Up and Review\n\n\n16\nFinal Exam Review Notes\nFinal Exam Review Notes\n\n\n\n\n\n\nIn addition to lecture notes, supplemental materials such as examples, data files, and problem sets will be provided as needed.\n\n📁 GitHub Repository: Course Repository\n\n🔗 Supplemental Readings: Available on Canvas or shared links.\n\n\n\n\n\nPre-Class Preparation: Read the lecture notes and supplemental readings before each class to familiarize yourself with the topics.\n\nDuring Class: Focus on key points highlighted in the notes. Participate in group activities and discussions.\n\nPost-Class Review: Revisit the lecture notes and solve related assignments. Use the notes for exam preparation.\n\n\n\n\nIf you experience issues accessing the lecture notes:\n1. Refresh your browser as noted earlier.\n2. Try accessing the page in incognito mode.\n3. Clear your browser cache using the appropriate settings (Chrome, Firefox, Edge).\nIf problems persist, contact the instructor via Canvas Email for further assistance.\n\n\n\n\nInstructor: Dr. Davood Tofighi\n\nOffice Hours: By appointment\n\nCourse GitHub Page: Access Here\n\nCanvas Course Page: UNM Canvas"
  },
  {
    "objectID": "home_lectures.html#weekly-lecture-notes",
    "href": "home_lectures.html#weekly-lecture-notes",
    "title": "Lecture Notes - STAT 454/545",
    "section": "",
    "text": "Week\nTopic\nLecture Notes Link\n\n\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n1\nIntroduction to Experimental Design\nWeek 1 Notes: Introduction to Experimental Design\n\n\n2\nCompletely Randomized Designs (CRD)\nWeek 2 Notes: Completely Randomized Designs (CRD)\n\n\n3\nMultiple Comparisons\nWeek 3 Notes: Multiple Comparisons\n\n\n4\nChecking Model Assumptions\nWeek 4 Notes: Checking Model Assumptions\n\n\n5\nTwo-Way Factorial Designs\nWeek 5 Notes: Two-Way Factorial Designs\n\n\n6\nHigher-Order Factorial Designs\nWeek 6 Notes: Higher-Order Factorial Designs\n\n\n7\nMidterm Review\nWeek 7 Notes: Midterm Review\n\n\n8\nBlocking & RCBD\nWeek 8 Notes: Blocking & RCBD\n\n\n9\nComplete Block Designs\nWeek 9 Notes: Complete Block Designs\n\n\n10\nAnalysis of Covariance (ANCOVA)\nWeek 101 Notes: Analysis of Covariance (ANCOVA)\n\n\n11\nRow–Column Designs (Latin Square)\nWeek 11 Notes: Row–Column Designs (Latin Square)\n\n\n12\nRandom & Mixed Models\nWeek 12 Notes: Random & Mixed Models\n\n\n13\nNested Models\nWeek 13 Notes: Nested Models\n\n\n14\nSplit-Plot Designs\nWeek 14 Notes: Split-Plot Designs\n\n\n15\nCatch-Up and Review\nWeek 15 Notes: Catch-Up and Review\n\n\n16\nFinal Exam Review Notes\nFinal Exam Review Notes"
  },
  {
    "objectID": "home_lectures.html#supplemental-resources",
    "href": "home_lectures.html#supplemental-resources",
    "title": "Lecture Notes - STAT 454/545",
    "section": "",
    "text": "In addition to lecture notes, supplemental materials such as examples, data files, and problem sets will be provided as needed.\n\n📁 GitHub Repository: Course Repository\n\n🔗 Supplemental Readings: Available on Canvas or shared links."
  },
  {
    "objectID": "home_lectures.html#how-to-use-lecture-notes",
    "href": "home_lectures.html#how-to-use-lecture-notes",
    "title": "Lecture Notes - STAT 454/545",
    "section": "",
    "text": "Pre-Class Preparation: Read the lecture notes and supplemental readings before each class to familiarize yourself with the topics.\n\nDuring Class: Focus on key points highlighted in the notes. Participate in group activities and discussions.\n\nPost-Class Review: Revisit the lecture notes and solve related assignments. Use the notes for exam preparation."
  },
  {
    "objectID": "home_lectures.html#troubleshooting-github-pages",
    "href": "home_lectures.html#troubleshooting-github-pages",
    "title": "Lecture Notes - STAT 454/545",
    "section": "",
    "text": "If you experience issues accessing the lecture notes:\n1. Refresh your browser as noted earlier.\n2. Try accessing the page in incognito mode.\n3. Clear your browser cache using the appropriate settings (Chrome, Firefox, Edge).\nIf problems persist, contact the instructor via Canvas Email for further assistance."
  },
  {
    "objectID": "home_lectures.html#contact-information",
    "href": "home_lectures.html#contact-information",
    "title": "Lecture Notes - STAT 454/545",
    "section": "",
    "text": "Instructor: Dr. Davood Tofighi\n\nOffice Hours: By appointment\n\nCourse GitHub Page: Access Here\n\nCanvas Course Page: UNM Canvas"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html",
    "href": "assignments/assignment12_nested_models.html",
    "title": "Assignment 12: Nested Models",
    "section": "",
    "text": "Objective: Analyze nested models, including nested random effects, interactions, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#instructions",
    "href": "assignments/assignment12_nested_models.html#instructions",
    "title": "Assignment 12: Nested Models",
    "section": "",
    "text": "Objective: Analyze nested models, including nested random effects, interactions, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#general-mixed-model",
    "href": "assignments/assignment12_nested_models.html#general-mixed-model",
    "title": "Assignment 12: Nested Models",
    "section": "12.3 General Mixed Model",
    "text": "12.3 General Mixed Model\nData Source: Dean et al. (2017), Chapter 12, Exercise 3\nConsider the model:\n\\[\nY_{ijkl} = \\mu + \\alpha_i + B_j(i) + C_k(ji) + \\delta_l + (\\alpha\\delta)_{il} + (B\\delta)_{lj}(i) + \\epsilon_{ijkl}\n\\]\n\nQuestions\n\nCalculate the expected mean squares for all effects in the model.\nWhich ratio would you use to test \\(H_0: \\delta_l + (\\alpha\\delta)_{il} \\, \\text{are all equal?}\\)\nWhich ratio would you use to test \\(H_0: \\sigma^2_\\alpha = 0\\)?"
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#operator-experiment",
    "href": "assignments/assignment12_nested_models.html#operator-experiment",
    "title": "Assignment 12: Nested Models",
    "section": "18.6 Operator Experiment",
    "text": "18.6 Operator Experiment\nData Source: Dean et al. (2017), Chapter 18, Exercise 6\nAn experiment aimed to determine how much variation in measured manganese concentration in steel is due to operator variability. Ten steel samples were sliced from a billet, and four operators each measured the manganese content twice per sample in a random order.\n\nQuestions\n\nWrite a model for this experiment. Clearly indicate which effects are fixed, random, crossed, and nested.\nWrite the degrees of freedom, sums of squares, and expected mean squares for each source of variation.\nInvestigate whether the normal distribution is a reasonable approximation for the error terms (using the data in Table 18.11). Transformation may be necessary.\nIf the normal distribution is reasonable, analyze the experiment. Obtain variance estimates for the random effects and identify the major sources of variation."
  },
  {
    "objectID": "assignments/assignment12_nested_models.html#table-18.11-manganese-concentrations-percentages-for-the-operator-experiment",
    "href": "assignments/assignment12_nested_models.html#table-18.11-manganese-concentrations-percentages-for-the-operator-experiment",
    "title": "Assignment 12: Nested Models",
    "section": "Table 18.11 Manganese Concentrations (Percentages) for the Operator Experiment",
    "text": "Table 18.11 Manganese Concentrations (Percentages) for the Operator Experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nOperator 1\n\nOperator 2\n\nOperator 3\n\nOperator 4\n\n\n\n\n\n\n1\n2\n1\n2\n1\n2\n1\n2\n\n\n1\n0.63\n0.60\n0.62\n0.62\n0.60\n0.60\n0.59\n0.61\n\n\n2\n0.64\n0.63\n0.63\n0.64\n0.67\n0.65\n0.62\n0.64\n\n\n3\n0.60\n0.58\n0.60\n0.61\n0.60\n0.60\n0.58\n0.60\n\n\n4\n0.75\n0.74\n0.74\n0.74\n0.74\n0.73\n0.73\n0.76\n\n\n5\n0.71\n0.68\n0.69\n0.70\n0.69\n0.67\n0.68\n0.71\n\n\n6\n0.65\n0.63\n0.62\n0.65\n0.63\n0.64\n0.62\n0.64\n\n\n7\n0.67\n0.64\n0.66\n0.67\n0.65\n0.65\n0.64\n0.66\n\n\n8\n0.65\n0.63\n0.65\n0.64\n0.62\n0.62\n0.60\n0.62\n\n\n9\n0.68\n0.66\n0.67\n0.68\n0.67\n0.67\n0.65\n0.68\n\n\n10\n0.67\n0.64\n0.66\n0.66\n0.65\n0.64\n0.64\n0.66\n\n\n\n\nGrading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nGeneral Mixed Model\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n10\n\n\nOperator Experiment\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n20\n\n\n\n(d)\n30\n\n\nTotal Score\n\n100%"
  },
  {
    "objectID": "assignments/assignment9_complete_block_designs.html",
    "href": "assignments/assignment9_complete_block_designs.html",
    "title": "Assignment 9: Complete Block Designs",
    "section": "",
    "text": "Objective: Analyze complete block designs, including block-treatment interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment9_complete_block_designs.html#instructions",
    "href": "assignments/assignment9_complete_block_designs.html#instructions",
    "title": "Assignment 9: Complete Block Designs",
    "section": "",
    "text": "Objective: Analyze complete block designs, including block-treatment interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment9_complete_block_designs.html#effervescent-experiment",
    "href": "assignments/assignment9_complete_block_designs.html#effervescent-experiment",
    "title": "Assignment 9: Complete Block Designs",
    "section": "Effervescent Experiment",
    "text": "Effervescent Experiment\nData Source: Dean et al. (2017), Chapter 10, Exercise 13\nThe effervescent experiment, conducted by B. Bailey, J. Lewis, J. Speiser, Z. Thomas, and S. White (2011), aimed to compare the dissolving times of two brands of cold medicine tablets in different water temperatures.\n\nFactors and Levels:\n\nBrand: 1 (Name Brand), 2 (Store Brand)\nWater Temperature: 1 (6°C), 2 (23°C), 3 (40°C)\n\nDesign: A complete block design with:\n\nBlocks:\n\nBlock I: Stirred liquid at 350 RPM using a magnetic stirring plate.\nBlock II: Unstirred liquid.\n\nObservations: Four per treatment combination, with a total of six treatment combinations.\nEach observation represents an average of times measured by four experimenters.\n\nResponse Variable: Dissolving time in seconds.\n\nThe data is provided in Table 10.26.\n\nDissolving time for the effervescent experiment (Order of observation in parentheses)\n\n\n\n\n\n\n\nTreatment Combination\nBlock I (Stirred)\nBlock II (Unstirred)\n\n\n\n\n11\n75.525 (8), 70.325 (9), 69.925 (17), 69.800 (23)\n83.475 (34), 86.800 (41), 83.750 (44), 79.575 (46)\n\n\n12\n68.125 (3), 47.525 (4), 61.475 (6), 58.625 (14)\n71.759 (36), 70.825 (37), 73.925 (42), 71.550 (48)\n\n\n13\n44.825 (7), 36.200 (10), 39.350 (11), 37.425 (13)\n51.975 (28), 50.100 (29), 51.225 (33), 53.700 (47)\n\n\n21\n78.350 (1), 76.050 (12), 78.425 (15), 71.525 (16)\n92.725 (31), 77.957 (35), 85.425 (39), 87.333 (45)\n\n\n22\n40.575 (2), 40.000 (5), 39.500 (20), 40.400 (22)\n42.275 (30), 44.425 (32), 42.475 (38), 44.300 (42)\n\n\n23\n27.450 (18), 26.600 (19), 24.950 (21), 26.325 (24)\n25.400 (25), 26.333 (26), 25.875 (27), 26.650 (40)\n\n\n\n\nQuestions\n\nPlot and Explore the Data: Create appropriate plots to visualize the dissolving time for each treatment combination, such as interaction plots, boxplots, or scatterplots. Comment on any patterns, anomalies, or interesting features observed in the data.\nFit a Block-Treatment Model:\n\n\nFit a linear model with block and treatment factors.\nExamine residuals to check assumptions, including outlier detection, variance equality, and normality of residuals.\nReport and interpret results.\n\n\nAnalyze Temperature Effects:\n\n\nPerform pairwise comparisons between the three temperature levels using appropriate post-hoc tests.\nTest for linear and quadratic trends in temperature effects on dissolving time, assuming these trends were pre-planned in the experimental design."
  },
  {
    "objectID": "assignments/assignment9_complete_block_designs.html#insole-cushion-experiment",
    "href": "assignments/assignment9_complete_block_designs.html#insole-cushion-experiment",
    "title": "Assignment 9: Complete Block Designs",
    "section": "Insole Cushion Experiment",
    "text": "Insole Cushion Experiment\nData Source: Dean et al. (2017), Chapter 10, Exercise 15\nThis experiment, conducted in 1995 by V. Agresti, S. Decker, T. Karakostas, E. Patterson, and S. Schwartz, investigated how different insole cushions and brands affect the vertical ground reaction force.\n\nFactors and Levels:\n\nCushion Type (C): 1 (Regular), 2 (Heel)\nBrand (D): 1 (Name Brand), 2 (Store Brand)\n\nDesign:\n\nSingle participant, alternating between dominant (right) and non-dominant (left) legs over two blocks (days).\nEach of the four treatment combinations measured five times in randomized order per block.\n\nResponse Variable: Maximum deceleration of the vertical ground reaction force (in Newtons).\n\nThe data is provided in Table 10.28.\n\nData for the insole cushion experiment (Order of observation in parentheses)\n\n\n\n\n\n\n\nC\nD\nResponse in Newtons (Order)\n\n\n\n\n\n\nBlock I (Right Leg)\n\n\n1\n1\n899.99 (3), 910.81 (5), 927.79 (10), 888.77 (11), 911.93 (16)\n\n\n1\n2\n924.92 (2), 900.10 (6), 923.55 (12), 891.56 (17), 885.73 (20)\n\n\n2\n1\n888.09 (4), 954.11 (7), 937.41 (9), 911.85 (14), 908.41 (18)\n\n\n2\n2\n884.01 (1), 918.36 (8), 880.23 (13), 891.16 (15), 917.16 (19)\n\n\n\n\nBlock II (Left Leg)\n\n\n1\n1\n852.94 (22), 866.28 (27), 886.65 (28), 851.14 (33), 869.80 (34)\n\n\n1\n2\n882.95 (21), 865.58 (24), 868.15 (25), 893.82 (37), 875.98 (38)\n\n\n2\n1\n920.93 (26), 880.26 (31), 897.10 (35), 893.78 (39), 885.80 (40)\n\n\n2\n2\n872.50 (23), 892.76 (29), 895.93 (30), 899.44 (32), 912.00 (36)\n\n\n\n\nQuestions\n\nFit a Model with Interaction:\n\n\nInclude block-treatment interactions in the ANOVA model.\nCreate and interpret an analysis of variance (ANOVA) table.\n\n\nGenerate Interaction Plots:\n\n\nCreate plots for:\n\nCushion (C) × Brand (D) interaction.\nCushion (C) × Block interaction.\nBrand (D) × Block interaction.\nTreatment combination × Block interaction.\n\nIdentify and describe contrasts of interest for further analysis.\n\n\nConfidence Intervals:\n\n\nCompute confidence intervals for means and contrasts identified in (b). Use these intervals to validate observed differences or interactions.\n\n\nRe-examine the Data:\n\n\nCheck assumptions, focusing on the two potential outliers.\nRe-analyze the data excluding one or both outliers.\nDiscuss whether the conclusions differ between analyses and justify which analysis should be reported."
  },
  {
    "objectID": "assignments/assignment9_complete_block_designs.html#yeast-experiment",
    "href": "assignments/assignment9_complete_block_designs.html#yeast-experiment",
    "title": "Assignment 9: Complete Block Designs",
    "section": "Yeast Experiment",
    "text": "Yeast Experiment\nData Source: Dean et al. (2017), Chapter 10, Exercise 16\nThis experiment explored the effects of yeast type, initial temperature, and water bath temperature on the rate of yeast rising.\n\nFactors and Levels:\n\nInitial Water Temperature (C): 100°F, 115°F, 130°F\nYeast Type (D): 1 (Rapid Rise), 2 (Regular)\nBath Temperature (E): 70°F, 85°F\n\nDesign:\n\nThree blocks, with each experimenter-team forming a block.\nEach treatment combination observed twice per block.\n\nResponse Variable: Percentage gain in height after 15 minutes.\n\nThe data is provided in Table 10.29.\n\nData for the yeast experiment (percentage rise). Treatment combinations are the levels of (water temperature, yeast, bath temperature)\n\n\nTreatment Combination\nBlock 1\nBlock 2\nBlock 3\n\n\n\n\n111\n8.2, 2.7\n10.9, 1.8\n11.4, 4.8\n\n\n112\n30.0, 39.8\n31.8, 36.0\n42.4, 20.0\n\n\n121\n12.6, 17.7\n3.5, 3.4\n3.4, 8.5\n\n\n122\n64.7, 73.9\n42.0, 32.6\n34.5, 30.0\n\n\n211\n18.1, 5.4\n12.3, 18.3\n8.5, 8.0\n\n\n212\n63.5, 66.3\n23.7, 57.5\n30.6, 45.3\n\n\n221\n4.2, 12.2\n7.7, 8.3\n6.0, 8.2\n\n\n222\n96.8, 71.1\n34.1, 40.9\n49.3, 46.0\n\n\n311\n44.4, 16.4\n5.0, 4.8\n8.5, 3.3\n\n\n312\n58.2, 63.3\n29.2, 27.8\n37.5, 18.2\n\n\n321\n19.8, 9.4\n4.8, 6.7\n6.4, 12.9\n\n\n322\n99.7, 92.3\n53.2, 58.9\n43.9, 73.7\n\n\n\n\nQuestions\n\nRandomization Justification: Explain why treatment combinations should be randomized within each block before measurements.\nResidual Normality:\n\n\nDiscuss the general importance of checking residual normality.\nExplain its relevance to this experiment’s context.\n\n\nSample Size for Precision:\n\n\nCalculate the minimum number of observations per treatment combination required to achieve a 99% confidence interval with a half-width of ≤5%. Use an error variance estimate of 9 percent².\n\n\nANOVA Table:\n\n\nConstruct an ANOVA table for the data.\nDiscuss the significance of main effects, interactions, and their implications.\n\n\nVisualize Treatment Effects:\n\n\nCreate two plots to illustrate the main points about treatment effects. Justify the choice of these plots.\n\n\nTrend Analysis:\n\n\nTest for a significant linear trend in percentage height gain as initial water temperature increases. Use a significance level of 0.01 and state conclusions."
  },
  {
    "objectID": "assignments/assignment9_complete_block_designs.html#grading-allocation",
    "href": "assignments/assignment9_complete_block_designs.html#grading-allocation",
    "title": "Assignment 9: Complete Block Designs",
    "section": "Grading Allocation",
    "text": "Grading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nEffervescent Experiment\n(a)\n10\n\n\n\n(b)\n15\n\n\n\n(c)\n15\n\n\nInsole Cushion Experiment\n(a)\n10\n\n\n\n(b)\n15\n\n\n\n(c)\n10\n\n\n\n(d)\n10\n\n\nYeast Experiment\n(a)\n5\n\n\n\n(b)\n5\n\n\n\n(c)\n5\n\n\n\n(d)\n5\n\n\n\n(e)\n5\n\n\n\n(f)\n5"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html",
    "href": "assignments/assignment4_checking_model_assumptions.html",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "",
    "text": "Objective: Evaluate the assumptions of the one-way ANOVA model and apply transformations to address violations.\nSubmit your completed assignment on Canvas by the due date.\nEnsure your submission includes all required components and is formatted correctly.\nPost your questions on Canvas or attend office hours for assistance.\n\nTotal points: 135"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#instructions",
    "href": "assignments/assignment4_checking_model_assumptions.html#instructions",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "",
    "text": "Objective: Evaluate the assumptions of the one-way ANOVA model and apply transformations to address violations.\nSubmit your completed assignment on Canvas by the due date.\nEnsure your submission includes all required components and is formatted correctly.\nPost your questions on Canvas or attend office hours for assistance.\n\nTotal points: 135"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#exercise-3-margarine-experiment-amy-l.-phelps-1987",
    "href": "assignments/assignment4_checking_model_assumptions.html#exercise-3-margarine-experiment-amy-l.-phelps-1987",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "Exercise 3: Margarine Experiment (Amy L. Phelps, 1987)",
    "text": "Exercise 3: Margarine Experiment (Amy L. Phelps, 1987)\nThe data in Table 5.16 show the melting times in seconds for three different brands of margarine (coded 1–3) and one brand of butter (coded 4). The butter was included for comparison purposes. The initial sizes and shapes of the margarine/butter pats were as similar as possible, and they were melted individually in a clean frying pan under constant heat.\nTasks:\n\nCheck the Equal-Variance Assumption\n\n\nEvaluate the equal-variance assumption for the model (3.3.1) using these data.\nIf the assumption is violated, choose the best transformation of the form: \\(y' = y^\\lambda\\) to stabilize the variances.\nRecheck the assumptions using the transformed data.\n\n\nCompute a Confidence Interval with Transformed Data\n\n\nUsing the transformed data from part (a), compute a 95% confidence interval comparing the average melting times for the margarines with the average melting time for the butter.\n\n\nRepeat Confidence Interval with Untransformed Data\n\n\nRepeat part (b) using the untransformed data and Satterthwaite’s approximation for unequal variances.\nCompare the results with those obtained in part (b).\n\n\nPreferred Analysis\n\n\nBased on the results from parts (b) and (c), determine which analysis you prefer and explain your reasoning."
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#exercise-6-bicycle-experiment-debra-schomer-1987",
    "href": "assignments/assignment4_checking_model_assumptions.html#exercise-6-bicycle-experiment-debra-schomer-1987",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "Exercise 6: Bicycle Experiment (Debra Schomer, 1987)",
    "text": "Exercise 6: Bicycle Experiment (Debra Schomer, 1987)\nThe bicycle experiment investigated the crank rates required to maintain specific speeds while riding a bicycle in twelfth gear on flat ground. The chosen speeds were 5, 10, 15, 20, and 25 mph (coded 1–5). The data are provided in Table 5.19. The experimenter fitted the one-way ANOVA model (3.3.1) and plotted the standardized residuals. She noted:\n“The larger spread of data at lower speeds is due to the difficulty of maintaining such low speeds consistently in such a high gear. This also causes additional strain on the bicycle.”\nAs a result, the experimenter observed differences in variances of the error variables across treatment levels.\nTasks:\n\nEvaluate Error Variances\n\n\nPlot the standardized residuals against the fitted values (\\(\\hat{y}_i\\)).\nCompare the sample variances and evaluate the equality of error variances across treatments.\n\n\nApply a Transformation\n\n\nSelect the best transformation of the data of the form: \\(y' = y^\\lambda\\)\nUsing the transformed data, test the hypotheses that the linear and quadratic trends in crank rates due to the different speeds are negligible.\nUse an overall significance level of \\(\\alpha = 0.01\\).\n\n\nTest Trends with Untransformed Data\n\n\nRepeat part (b) using the untransformed data and Satterthwaite’s approximation for unequal variances.\n\n\nDiscuss Methods\n\n\nCompare the results obtained in parts (b) and (c).\nDiscuss the relative merits of the methods applied and provide a recommendation based on your findings.\n\n\nTable 5.19 for Exercise 6: Data for the Bicycle Experiment\n\nData for the bicycle experiment investigating crank rates at different speeds.\n\n\n\n\n\n\n\n\n\nCode\nTreatment (Speed, mph)\nCrank Rates\nCrank Rates\nCrank Rates\n\n\n\n\n1\n5\n15\n19\n22\n\n\n2\n10\n32\n34\n27\n\n\n3\n15\n44\n47\n44\n\n\n4\n20\n59\n61\n61\n\n\n5\n25\n75\n73\n75"
  },
  {
    "objectID": "assignments/assignment4_checking_model_assumptions.html#grading-rubric",
    "href": "assignments/assignment4_checking_model_assumptions.html#grading-rubric",
    "title": "Assignment 4: Checking Model Assumptions",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\n\n\nExercise\nTask\nPoints Allocated\nTotal Points\nKey Learning Objective\n\n\n\n\n3\n(a) Check Equal-Variance Assumption\n15\n65\nAssess variance assumptions, identify violations, and apply suitable data transformation.\n\n\n\n(b) Confidence Interval with Transformed Data\n20\n\nCompute and interpret confidence intervals with transformed data.\n\n\n\n(c) Confidence Interval with Untransformed Data\n20\n\nUse Satterthwaite’s approximation to address unequal variances.\n\n\n\n(d) Preferred Analysis\n10\n\nEvaluate methods and justify a preferred analysis.\n\n\n6\n(a) Evaluate Error Variances\n15\n70\nAnalyze residuals and assess variance equality.\n\n\n\n(b) Transformation and Hypothesis Testing\n25\n\nApply data transformations and test for trends with appropriate significance levels.\n\n\n\n(c) Untransformed Data Hypothesis Testing\n20\n\nConduct hypothesis testing using untransformed data with unequal variance adjustments.\n\n\n\n(d) Compare Methods\n10\n\nCompare merits of different analytical approaches.\n\n\n\n\nTotal Points Available: 135"
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html",
    "href": "assignments/assignment8_randomized_complete_block_design.html",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "",
    "text": "Objective: Perform randomization for randomized block designs, evaluate assumptions, and conduct statistical analysis using ANOVA.\nUse R to conduct the randomization and statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your randomization and analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html#instructions",
    "href": "assignments/assignment8_randomized_complete_block_design.html#instructions",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "",
    "text": "Objective: Perform randomization for randomized block designs, evaluate assumptions, and conduct statistical analysis using ANOVA.\nUse R to conduct the randomization and statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your randomization and analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html#exercise-1-randomized-complete-block-design",
    "href": "assignments/assignment8_randomized_complete_block_design.html#exercise-1-randomized-complete-block-design",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "Exercise 1: Randomized Complete Block Design",
    "text": "Exercise 1: Randomized Complete Block Design\nData Source: Dean et al. (2017), Chapter 10, Exercise 1\nTask: Conduct a randomization for a randomized complete block design with:\n\n\\(v = 4\\) treatments,\n\\(b = 5\\) blocks,\nEach treatment is observed \\(s = 1\\) time in each block.\n\nDeliverables:\n\nShow the randomized treatment allocation within each block.\nProvide the final design in tabular format, with rows representing blocks and columns representing treatment allocations."
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html#exercise-2-general-complete-block-design-ex.-10.2",
    "href": "assignments/assignment8_randomized_complete_block_design.html#exercise-2-general-complete-block-design-ex.-10.2",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "Exercise 2: General Complete Block Design (Ex. 10.2)",
    "text": "Exercise 2: General Complete Block Design (Ex. 10.2)\nData Source: Dean et al. (2017), Chapter 10, Exercise 2\nTask: Conduct a randomization for a general complete block design with:\n\n\\(v = 3\\) treatments,\n\\(b = 4\\) blocks,\nEach treatment is observed \\(s = 2\\) times in each block.\n\nDeliverables:\n\nDescribe the randomization process for the treatments.\nPresent the final design in a table showing the randomized treatment allocation in each block."
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html#exercise-3-dcis-experiment-randomization-ex.-10.3",
    "href": "assignments/assignment8_randomized_complete_block_design.html#exercise-3-dcis-experiment-randomization-ex.-10.3",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "Exercise 3: DCIS Experiment Randomization (Ex. 10.3)",
    "text": "Exercise 3: DCIS Experiment Randomization (Ex. 10.3)\nData Source: Dean et al. (2017), Chapter 10, Exercise 3\nScenario: Suppose the DCIS experiment from Example 10.3.2 (p. 309) was designed as a randomized complete block design with:\n\n\\(b = 4\\) blocks,\n\\(k = v = 6\\) treatments per block,\nEach treatment is observed \\(s = 1\\) time per block.\n\nTask:\n\nPerform a randomization of the six treatments (\\(A, B, C, D, E, F\\)) within each block.\nPresent the final randomized design for all four blocks.\n\nDeliverables:\n\nDetail the randomization steps used to allocate treatments within blocks.\nProvide a table showing the randomized treatment assignments within each block."
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html#exercise-4-respiratory-exchange-ratio-experiment-ex.-10.4",
    "href": "assignments/assignment8_randomized_complete_block_design.html#exercise-4-respiratory-exchange-ratio-experiment-ex.-10.4",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "Exercise 4: Respiratory Exchange Ratio Experiment (Ex. 10.4)",
    "text": "Exercise 4: Respiratory Exchange Ratio Experiment (Ex. 10.4)\nData Source: Dean et al. (2017), Chapter 10, Exercise 4\nScenario: In the resting metabolic rate experiment introduced in Example 10.4.1 (p. 311), the experimenters also measured the respiratory exchange ratio, another measure of energy expenditure. The data for the second 30 minutes of testing are provided below.\n\nThe data are from Example 10.4.1, but the respiratory exchange ratio data are used here.\n\n\nSubject\nProtocol 1\nProtocol 2\nProtocol 3\n\n\n\n\n1\n0.79\n0.80\n0.83\n\n\n2\n0.84\n0.84\n0.81\n\n\n3\n0.84\n0.93\n0.88\n\n\n4\n0.83\n0.85\n0.79\n\n\n5\n0.84\n0.78\n0.88\n\n\n6\n0.83\n0.75\n0.86\n\n\n7\n0.77\n0.76\n0.71\n\n\n8\n0.83\n0.85\n0.78\n\n\n9\n0.81\n0.77\n0.72\n\n\n\nTasks:\n\nEvaluate Assumptions:\n\nEvaluate the assumptions of the block-treatment model (Equation 10.4.1) for these data.\n\nANOVA Analysis:\n\nConstruct an analysis of variance (ANOVA) table.\nTest for equality of the effects of the protocols on the respiratory exchange ratio. Report your conclusions.\n\nEvaluate the usefulness of blocking.\n\nDeliverables:\n\nDiscuss the assumptions of the block-treatment model for the respiratory exchange ratio data.\nPresent the ANOVA table and results for the respiratory exchange ratio data.\nEvaluate the usefulness of blocking in this experiment.\nInclude any R code used for the analysis.\nProvide a clear explanation of the process and results."
  },
  {
    "objectID": "assignments/assignment8_randomized_complete_block_design.html#grading-rubric",
    "href": "assignments/assignment8_randomized_complete_block_design.html#grading-rubric",
    "title": "Assignment 8: Randomized Complete Block Design",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\nExercise\nCriteria\nPoints\n\n\n\n\nExercise 1\nCorrect randomized design and explanation\n20\n\n\nExercise 2\nAccurate general block design and steps\n20\n\n\nExercise 3\nProper randomization and final table\n20\n\n\nExercise 4\nAssumption evaluation and ANOVA results\n30\n\n\nClarity and R Code\nClearly explained process and R code used\n10"
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html",
    "href": "assignments/assignment6_higher_order_factorial_designs.html",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "",
    "text": "Objective: Explore higher-order factorial designs and interactions in experimental data.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 140"
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#instructions",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#instructions",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "",
    "text": "Objective: Explore higher-order factorial designs and interactions in experimental data.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 140"
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.7-coating-experiment",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.7-coating-experiment",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Exercise 7.7: Coating Experiment",
    "text": "Exercise 7.7: Coating Experiment\nThis exercise examines the effects of spray parameters on the thermal spray coating properties of alumina (\\(\\text{Al}_2\\text{O}_3\\)). The experimental factors and levels are as follows:\n\nFuel ratio (A): 1:2.8 and 1:2.0\nCarrier gas flow rate (B): 1.33 and 3.21 \\(\\text{L/s}^{-1}\\)\nFrequency of detonations (C): 2 and 4 Hz\nSpray distance (D): 180 and 220 mm\n\nThe response variable is porosity (vol. %). Data are shown in Table 7.19.\nQuestions:\n\nDesign an Analysis\n\nAssuming negligible threeand four-factor interactions, outline the steps required to analyze the data (refer to Step (g) of the checklist in Chapter 2).\n\nCheck Model Assumptions\n\nEvaluate whether the assumptions underlying the model are valid.\n\nPerform the Analysis\n\nConduct the analysis you outlined in part (1), including interaction plots if appropriate. Clearly state your conclusions.\nTable 7.19: Data for the Coating Experiment\n\n\n\nA\nB\nC\nD\n\\(y_{ijkl}\\)\n\n\n\n\n2\n2\n2\n2\n5.95\n\n\n2\n2\n2\n1\n4.57\n\n\n2\n2\n1\n2\n4.03\n\n\n…\n…\n…\n…\n…\n\n\n\nSource: Adapted from Saravanan et al. (2001), Journal of Physics D: Applied Physics."
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.11-antifungal-antibiotic-experiment",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.11-antifungal-antibiotic-experiment",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Exercise 7.11: Antifungal Antibiotic Experiment",
    "text": "Exercise 7.11: Antifungal Antibiotic Experiment\nThis exercise investigates the effects of incubation conditions on the yield of an antifungal antibiotic.\n\nFactors and Levels:\nIncubation temperature (A): 25, 30, and 37 \\(^\\circ \\text{C}\\)\nCarbon concentration (B): 2%, 5%, and 7.5%\nNitrogen concentration (C): 0.5%, 1%, and 3%\n\nThe response variable is the antifungal yield (measured in activity against Candida albicans). Data are shown in Table 7.23.\nQuestions:\n\nAssess Main Effects\n\nConstruct plots to assess the significance of main effects of \\(A\\), \\(B\\), and \\(C\\) on the response. What are your conclusions?\n\nInteraction Assumptions\n\nState the assumptions you made regarding interactions while analyzing main effects in part (1).\n\nTwo-Way Interactions\n\nConstruct plots to assess the significance of two-way interactions. Do they alter your conclusions from part (1)?\n\nFit a Model\n\nAssuming no three-way interaction, fit a model with all main effects and two-way interactions. Discuss the significance of the effects and compare with your conclusions from part (3).\n\nModel Diagnostics\n\nEvaluate whether the assumptions of normality and equal error variances are satisfied. Identify potential outliers.\nTable 7.23: Data for Antifungal Antibiotic Experiment\n\n\n\nA\nB\nC\n\\(y_{ijk}\\)\n\n\n\n\n25\n2\n0.5\n25.84\n\n\n25\n2\n1\n51.86\n\n\n25\n2\n3\n32.59\n\n\n30\n5\n1\n41.11\n\n\n37\n7.5\n0.5\n51.86\n\n\n…\n…\n…\n…\n\n\n\nSource: Gupte and Kulkarni (2003), Journal of Chemical Technology and Biotechnology."
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.12-antifungal-antibiotic-experiment-extended",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#exercise-7.12-antifungal-antibiotic-experiment-extended",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Exercise 7.12: Antifungal Antibiotic Experiment (Extended)",
    "text": "Exercise 7.12: Antifungal Antibiotic Experiment (Extended)\nFor this exercise, consider the data from Table 7.23 but assume no negligible three-factor interaction. Modify levels of \\(A\\) and \\(B\\) so their third levels are 35 and 8, respectively, to ensure equal spacing.\nQuestions:\n\nTrend Contrasts\n\nCreate a table listing the 27 treatment combinations. Include contrast coefficients for:\n\nLinear and quadratic trends in \\(A\\) and \\(B\\)\nInteraction trends (\\(A \\times B\\): Linear-Linear, Linear-Quadratic, etc.)\n\n\nNormalize Contrasts\n\nIdentify divisors to normalize each contrast and manually compute least squares estimates for normalized Linear \\(A\\) and Quadratic \\(A\\) contrasts.\n\nOrthogonal Contrasts for \\(C\\)\n\nPropose two orthogonal contrasts for \\(C\\) to address its uneven spacing and add them to the table in part (1).\n\nOrthogonal Contrasts Analysis\n\nUse software to calculate least squares estimates for all 26 orthogonal contrasts. Generate a half-normal probability plot and interpret the results.\n\nAlternative Analysis\n\nApply the Voss and Wang method to analyze the orthogonal contrasts. Compare these findings to part (4)."
  },
  {
    "objectID": "assignments/assignment6_higher_order_factorial_designs.html#table-of-grades-and-rubric",
    "href": "assignments/assignment6_higher_order_factorial_designs.html#table-of-grades-and-rubric",
    "title": "Assignment 6: Higher-Order Factorial Designs",
    "section": "Table of Grades and Rubric",
    "text": "Table of Grades and Rubric\n\n\n\n\n\n\n\n\n\nExercise\nCriteria\nMax Points\nDescription\n\n\n\n\nExercise 7.7\nPart (a): Analysis Design\n10\nOutline the analysis steps, assumptions, and appropriate statistical methods (e.g., ANOVA, interaction plots).\n\n\n\nPart (b): Model Assumptions\n10\nAssess normality, homoscedasticity, and independence of residuals using statistical tests and plots.\n\n\n\nPart (c): Analysis and Plots\n20\nConduct ANOVA, interpret significant effects, create interaction plots, and draw meaningful conclusions.\n\n\nTotal for Exercise 7.7\n\n40\n\n\n\nExercise 7.11\nPart (a): Main Effects\n10\nConstruct main effects plots and evaluate their significance.\n\n\n\nPart (b): Interaction Assumptions\n5\nClearly state and justify interaction assumptions.\n\n\n\nPart (c): Two-Way Interactions\n15\nConstruct interaction plots, interpret results, and update conclusions from main effects analysis.\n\n\n\nPart (d): Model Fitting\n15\nFit a model including all main effects and two-way interactions, interpret significant effects.\n\n\n\nPart (e): Diagnostics\n10\nEvaluate residual diagnostics for normality, homoscedasticity, and outliers.\n\n\nTotal for Exercise 7.11\n\n55\n\n\n\nExercise 7.12\nPart (a): Trend Contrasts\n15\nCreate a table with contrast coefficients for linear, quadratic, and interaction terms.\n\n\n\nPart (b): Normalization\n10\nCompute normalization divisors and least squares estimates.\n\n\n\nPart (c): Orthogonal Contrasts\n10\nPropose and justify two orthogonal contrasts for CC.\n\n\n\nPart (d): Computer Analysis\n15\nCompute estimates, create a half-normal probability plot, and interpret results.\n\n\n\nPart (e): Voss and Wang Method\n10\nApply the method to examine orthogonal contrasts and compare with results from the half-normal plot.\n\n\nTotal for Exercise 7.12\n\n60"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "",
    "text": "Objective: Analyze random and mixed effects models, including random effects, interactions, and post-hoc tests.\nUse R tool to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 130"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#instructions",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#instructions",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "",
    "text": "Objective: Analyze random and mixed effects models, including random effects, interactions, and post-hoc tests.\nUse R tool to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 130"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#random-effects-model",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#random-effects-model",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "17.3 Random Effects Model",
    "text": "17.3 Random Effects Model\nData Source: Dean et al. (2017), Chapter 17, Exercise 3\nConsider the following random-effects model:\n\\[\nY_{ijkmt} = \\mu + A_i + B_j + C_k + D_m + (AB)_{ij} + (BC)_{jk} + (BD)_{jm} + \\epsilon_{ijkmt}\n\\]\nWhere:\n\n\\(i = 1, \\dots, a\\), \\(j = 1, \\dots, b\\), \\(k = 1, \\dots, c\\), \\(m = 1, \\dots, d\\), \\(t = 1, \\dots, r\\).\n\\(A_i \\sim N(0, \\sigma^2_A)\\), \\(B_j \\sim N(0, \\sigma^2_B)\\), \\(C_k \\sim N(0, \\sigma^2_C)\\), \\(D_m \\sim N(0, \\sigma^2_D)\\).\n\\((AB)_{ij} \\sim N(0, \\sigma^2_{AB})\\), \\((BC)_{jk} \\sim N(0, \\sigma^2_{BC})\\), \\((BD)_{jm} \\sim N(0, \\sigma^2_{BD})\\).\n\\(\\epsilon_{ijkmt} \\sim N(0, \\sigma^2)\\).\n\n\nQuestions\n\nWrite out the expected mean squares for all main effects and interactions in the model.\nHow would you test the null hypothesis \\(H_0^A: \\sigma^2_A = 0\\) against the alternative \\(H_A^A: \\sigma^2_A &gt; 0\\)?\nHow would you test the null hypothesis \\(H_0^B: \\sigma^2_B = 0\\) against the alternative \\(H_A^B: \\sigma^2_B &gt; 0\\)?\nGive formulae for unbiased estimates of \\(\\sigma^2_{BD}\\) and \\(\\sigma^2_B\\).\nProvide formulae for individual 95% confidence intervals for \\(\\sigma^2_{BD}\\) and \\(\\sigma^2_B\\). What is the overall confidence level?"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#buttermilk-biscuit-experiment",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#buttermilk-biscuit-experiment",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "12.4 Buttermilk Biscuit Experiment",
    "text": "12.4 Buttermilk Biscuit Experiment\nData Source: Dean et al. (2017), Chapter 12, Exercise 4\nThe buttermilk biscuit experiment was conducted by Stacie Taylor in 1995 to compare three brands of refrigerated buttermilk biscuits (Factor \\(A\\), 3 levels, fixed effect) on their fluffiness. The biscuits were baked on a tray for 7 minutes at 425°F, with 6 biscuits baked at a time. The experiment was run as a general complete block design with blocks of size \\(k = 6\\).\n\nQuestions\n\nUse a mixed model with interaction to represent the data, where the random effect represents the block (run of the oven) and the fixed effect represents the biscuit brand. Write out the model including all assumptions.\nCheck the assumptions of the model for the data in Table 17.20 as far as possible.\nWrite out the expected mean squares for all terms in the model.\nCreate a block × brand interaction plot for the blocks observed in the experiment.\nTest the hypothesis that the variance in height due to block × brand interactions is negligible. Interpret your conclusions in terms of the plot in part (d).\nCalculate a set of 95% simultaneous confidence intervals for the pairwise comparisons between the brands. State your conclusions.\n\n\nTable 17.20: Biscuit Experiment Data\n\n\n\n\n\n\n\n\n\n\n\nBlock\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2 (150.0)\n1 (188.2)\n2 (177.8)\n3 (166.7)\n3 (187.5)\n1 (182.4)\n\n\n2\n1 (183.3)\n2 (183.3)\n2 (183.3)\n3 (176.5)\n1 (160.0)\n3 (187.5)\n\n\n3\n1 (178.9)\n3 (182.4)\n2 (193.8)\n3 (176.5)\n2 (188.9)\n1 (188.9)\n\n\n4\n2 (177.8)\n1 (145.5)\n3 (155.0)\n1 (173.7)\n3 (200.0)\n2 (187.5)\n\n\n5\n1 (205.6)\n3 (188.2)\n3 (142.9)\n2 (161.9)\n2 (177.8)\n1 (159.1)"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#golf-ball-experiment",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#golf-ball-experiment",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "12.6 Golf Ball Experiment",
    "text": "12.6 Golf Ball Experiment\nData Source: Dean et al. (2017), Chapter 12, Exercise 6\nAn experiment by Tim Kelaghan (1995) examined whether different brands of golf balls travel the same distance when hit by amateur golfers. Three brands (\\(v = 3\\)) were selected, and some number of golfers (\\(b\\)) determined. A pilot study was conducted with two golfers, each hitting \\(s = 6\\) balls per brand in a random order.\n\nQuestions\n\nUsing the pilot data (Table 17.22), calculate a 95% upper bound for the error variance \\(\\sigma^2\\).\nFor the main experiment, determine the number of golfers (\\(b\\)) needed to ensure that simultaneous 95% confidence intervals for the pairwise differences between brands have a width of at most 20 yards.\n\n\nTable 17.22: Golf Ball Distances (in Yards)\n\n\nGolfer\nBrand\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n1\n209\n204\n179\n230\n233\n245\n\n\n\n2\n188\n211\n242\n222\n187\n233\n\n\n\n3\n219\n204\n247\n215\n197\n161\n\n\n2\n1\n240\n207\n192\n190\n226\n188\n\n\n\n2\n216\n195\n240\n215\n219\n238\n\n\n\n3\n195\n221\n205\n192\n183\n230"
  },
  {
    "objectID": "assignments/assignment11_random_and_mixed_effects_models.html#grading-allocation",
    "href": "assignments/assignment11_random_and_mixed_effects_models.html#grading-allocation",
    "title": "Assignment 11: Random and Mixed Effects Models",
    "section": "Grading Allocation",
    "text": "Grading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nRandom Effects Model\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n10\n\n\n\n(d)\n10\n\n\n\n(e)\n10\n\n\nBiscuit Experiment\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n10\n\n\n\n(d)\n10\n\n\n\n(e)\n10\n\n\n\n(f)\n10\n\n\nGolf Ball Experiment\n(a)\n10\n\n\n\n(b)\n10"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html",
    "href": "appendix/r-packages/tidyr.html",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "The tidyr package, part of the tidyverse, is designed for data tidying—reshaping data into a tidy format where each variable is a column, each observation is a row, and each value is a single cell.\nKey functions include: - tidyr::pivot_longer(): Reshape wide data into long format. - tidyr::pivot_wider(): Reshape long data into wide format. - tidyr::separate(): Split a column into multiple columns. - tidyr::unite(): Combine multiple columns into one. - tidyr::drop_na(): Remove rows with missing values. - tidyr::replace_na(): Replace missing values.\n\n\n\n\nReshaping data for analysis or visualization.\nCleaning and preparing raw datasets.\nCombining or splitting columns for better data structure.\n\n\n\n\n\nSimplifies handling of messy or untidy data.\nEnsures compatibility with other tidyverse tools.\nOffers a consistent and intuitive syntax.\n\n\n\n\n\n\n\nLoad tidyr using pacman::p_load():\n\npacman::p_load(tidyr)\n\n\n\n\nReshape a simple dataset:\n\ndata &lt;- data.frame(\n    ID = 1:3,\n    Gender = c(\"M\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 35),\n    Age_2023 = c(26, 31, 36)\n)\n\n# Pivot longer\ndata |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nConverts wide data into long format, making columns into rows.\n\n\n\ntidyr::pivot_longer(data, cols, names_to, values_to, ...)\n\ndata: A data frame.\ncols: Columns to pivot.\nnames_to: Name of the new column for column names.\nvalues_to: Name of the new column for values.\n\n\n\n\n\ndata |&gt; tidyr::pivot_longer(cols = c(Age_2022, Age_2023), names_to = \"Year\", values_to = \"Age\")\n\n\n  \n\n\n\n\n\n\n\n\n\nConverts long data into wide format, making rows into columns.\n\n\n\ntidyr::pivot_wider(data, names_from, values_from, ...)\n\ndata: A data frame.\nnames_from: Column containing names for new columns.\nvalues_from: Column containing values for new columns.\n\n\n\n\n\nlong_data &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\nlong_data |&gt; tidyr::pivot_wider(names_from = Year, values_from = Age)\n\n\n  \n\n\n\n\n\n\n\n\n\nSplits a single column into multiple columns.\n\n\n\ntidyr::separate(data, col, into, sep = \" \")\n\ndata: A data frame.\ncol: Column to split.\ninto: Vector of new column names.\nsep: Separator to split on.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice_2022\", \"Bob_2023\"))\ndata |&gt; tidyr::separate(Name, into = c(\"Name\", \"Year\"), sep = \"_\")\n\n\n  \n\n\n\n\n\n\n\n\n\nCombines multiple columns into one.\n\n\n\ntidyr::unite(data, col, ..., sep = \"_\")\n\ndata: A data frame.\ncol: Name of the new column.\n...: Columns to combine.\nsep: Separator between combined values.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Year = c(2022, 2023))\ndata |&gt; tidyr::unite(\"Name_Year\", Name, Year, sep = \"_\")\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nSummarize the mean age by gender and year.\n\n\n\n\ndata &lt;- data.frame(\n    ID = 1:4,\n    Gender = c(\"M\", \"F\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 27, 35),\n    Age_2023 = c(26, 31, 28, 36)\n)\n\ndata_long &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\n\nsummary &lt;- data_long |&gt;\n    dplyr::group_by(Gender, Year) |&gt;\n    dplyr::summarize(MeanAge = mean(Age), .groups = \"drop\")\nsummary\n\n\n  \n\n\n\n\n\n\nA summary table of mean ages by gender and year.\n\n\n\n\n\n\nCombine year and ID into a unique identifier.\n\n\n\n\ndata |&gt;\n    tidyr::unite(\"UniqueID\", ID, starts_with(\"Age\"), sep = \"-\")\n\n\n  \n\n\n\n\n\n\nA new column UniqueID combines values.\n\n\n\n\n\n\n\n\n\n\ndata_long |&gt; dplyr::mutate(Year = paste0(\"Year_\", Year))\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\n\ndata_long |&gt;\n    ggplot(aes(x = Year, y = Age, fill = Gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    labs(title = \"Age Distribution by Year\", x = \"Year\", y = \"Age\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUse select() before pivoting to limit unnecessary columns.\nAvoid excessive pivoting by designing data collection appropriately.\n\n\n\n\n\n\n\n\n\nEnsure column names are unique before pivoting.\n\n\n\nCheck the data for inconsistencies or missing values.\n\n\n\n\n\nCan tidyr handle large datasets? Yes, but consider chunking with data.table for very large data.\nHow do I preserve column types? Use tidyr::pivot_longer() or pivot_wider() with values_transform.\n\n\n\n\n\n\nStart with well-structured raw data.\nUse descriptive column names and standard formats.\nAvoid overusing pivot_longer() or pivot_wider() for reversible tasks.\n\n\n\n\n\n\ntidyr simplifies reshaping and tidying data with a cohesive set of functions.\n\n\n\nExplore advanced tidying tasks with tidyr or integrate with other tidyverse tools like ggplot2 and dplyr.\n\n\n\n\n\ntidyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#introduction",
    "href": "appendix/r-packages/tidyr.html#introduction",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "The tidyr package, part of the tidyverse, is designed for data tidying—reshaping data into a tidy format where each variable is a column, each observation is a row, and each value is a single cell.\nKey functions include: - tidyr::pivot_longer(): Reshape wide data into long format. - tidyr::pivot_wider(): Reshape long data into wide format. - tidyr::separate(): Split a column into multiple columns. - tidyr::unite(): Combine multiple columns into one. - tidyr::drop_na(): Remove rows with missing values. - tidyr::replace_na(): Replace missing values.\n\n\n\n\nReshaping data for analysis or visualization.\nCleaning and preparing raw datasets.\nCombining or splitting columns for better data structure.\n\n\n\n\n\nSimplifies handling of messy or untidy data.\nEnsures compatibility with other tidyverse tools.\nOffers a consistent and intuitive syntax."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#getting-started",
    "href": "appendix/r-packages/tidyr.html#getting-started",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Load tidyr using pacman::p_load():\n\npacman::p_load(tidyr)\n\n\n\n\nReshape a simple dataset:\n\ndata &lt;- data.frame(\n    ID = 1:3,\n    Gender = c(\"M\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 35),\n    Age_2023 = c(26, 31, 36)\n)\n\n# Pivot longer\ndata |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#key-functions-and-features",
    "href": "appendix/r-packages/tidyr.html#key-functions-and-features",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Converts wide data into long format, making columns into rows.\n\n\n\ntidyr::pivot_longer(data, cols, names_to, values_to, ...)\n\ndata: A data frame.\ncols: Columns to pivot.\nnames_to: Name of the new column for column names.\nvalues_to: Name of the new column for values.\n\n\n\n\n\ndata |&gt; tidyr::pivot_longer(cols = c(Age_2022, Age_2023), names_to = \"Year\", values_to = \"Age\")\n\n\n  \n\n\n\n\n\n\n\n\n\nConverts long data into wide format, making rows into columns.\n\n\n\ntidyr::pivot_wider(data, names_from, values_from, ...)\n\ndata: A data frame.\nnames_from: Column containing names for new columns.\nvalues_from: Column containing values for new columns.\n\n\n\n\n\nlong_data &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\nlong_data |&gt; tidyr::pivot_wider(names_from = Year, values_from = Age)\n\n\n  \n\n\n\n\n\n\n\n\n\nSplits a single column into multiple columns.\n\n\n\ntidyr::separate(data, col, into, sep = \" \")\n\ndata: A data frame.\ncol: Column to split.\ninto: Vector of new column names.\nsep: Separator to split on.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice_2022\", \"Bob_2023\"))\ndata |&gt; tidyr::separate(Name, into = c(\"Name\", \"Year\"), sep = \"_\")\n\n\n  \n\n\n\n\n\n\n\n\n\nCombines multiple columns into one.\n\n\n\ntidyr::unite(data, col, ..., sep = \"_\")\n\ndata: A data frame.\ncol: Name of the new column.\n...: Columns to combine.\nsep: Separator between combined values.\n\n\n\n\n\ndata &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Year = c(2022, 2023))\ndata |&gt; tidyr::unite(\"Name_Year\", Name, Year, sep = \"_\")"
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#in-depth-examples",
    "href": "appendix/r-packages/tidyr.html#in-depth-examples",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Summarize the mean age by gender and year.\n\n\n\n\ndata &lt;- data.frame(\n    ID = 1:4,\n    Gender = c(\"M\", \"F\", \"F\", \"M\"),\n    Age_2022 = c(25, 30, 27, 35),\n    Age_2023 = c(26, 31, 28, 36)\n)\n\ndata_long &lt;- data |&gt; tidyr::pivot_longer(cols = starts_with(\"Age\"), names_to = \"Year\", values_to = \"Age\")\n\nsummary &lt;- data_long |&gt;\n    dplyr::group_by(Gender, Year) |&gt;\n    dplyr::summarize(MeanAge = mean(Age), .groups = \"drop\")\nsummary\n\n\n  \n\n\n\n\n\n\nA summary table of mean ages by gender and year.\n\n\n\n\n\n\nCombine year and ID into a unique identifier.\n\n\n\n\ndata |&gt;\n    tidyr::unite(\"UniqueID\", ID, starts_with(\"Age\"), sep = \"-\")\n\n\n  \n\n\n\n\n\n\nA new column UniqueID combines values."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#advanced-features",
    "href": "appendix/r-packages/tidyr.html#advanced-features",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "data_long |&gt; dplyr::mutate(Year = paste0(\"Year_\", Year))\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\n\ndata_long |&gt;\n    ggplot(aes(x = Year, y = Age, fill = Gender)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    labs(title = \"Age Distribution by Year\", x = \"Year\", y = \"Age\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUse select() before pivoting to limit unnecessary columns.\nAvoid excessive pivoting by designing data collection appropriately."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/tidyr.html#troubleshooting-and-faqs",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Ensure column names are unique before pivoting.\n\n\n\nCheck the data for inconsistencies or missing values.\n\n\n\n\n\nCan tidyr handle large datasets? Yes, but consider chunking with data.table for very large data.\nHow do I preserve column types? Use tidyr::pivot_longer() or pivot_wider() with values_transform."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#best-practices",
    "href": "appendix/r-packages/tidyr.html#best-practices",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "Start with well-structured raw data.\nUse descriptive column names and standard formats.\nAvoid overusing pivot_longer() or pivot_wider() for reversible tasks."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#conclusion",
    "href": "appendix/r-packages/tidyr.html#conclusion",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "tidyr simplifies reshaping and tidying data with a cohesive set of functions.\n\n\n\nExplore advanced tidying tasks with tidyr or integrate with other tidyverse tools like ggplot2 and dplyr."
  },
  {
    "objectID": "appendix/r-packages/tidyr.html#references-and-resources",
    "href": "appendix/r-packages/tidyr.html#references-and-resources",
    "title": "Using the tidyr Package in R",
    "section": "",
    "text": "tidyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html",
    "href": "appendix/r-packages/multcomp.html",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "",
    "text": "The multcomp package in R is designed for simultaneous inference in linear models, specifically focused on multiple comparisons. It provides tools to test hypotheses about multiple parameters in a model while controlling for family-wise error rates.\n\n\n\n\nComparing group means in ANOVA.\nSimultaneous confidence intervals.\nAdjusting p-values for multiple testing.\n\n\n\n\n\nHandles a variety of multiple comparison tests.\nProvides flexibility with user-defined contrasts.\nSupports many types of linear models, including generalized linear models and mixed-effects models."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#introduction",
    "href": "appendix/r-packages/multcomp.html#introduction",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "",
    "text": "The multcomp package in R is designed for simultaneous inference in linear models, specifically focused on multiple comparisons. It provides tools to test hypotheses about multiple parameters in a model while controlling for family-wise error rates.\n\n\n\n\nComparing group means in ANOVA.\nSimultaneous confidence intervals.\nAdjusting p-values for multiple testing.\n\n\n\n\n\nHandles a variety of multiple comparison tests.\nProvides flexibility with user-defined contrasts.\nSupports many types of linear models, including generalized linear models and mixed-effects models."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#getting-started",
    "href": "appendix/r-packages/multcomp.html#getting-started",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\nLoading the Package\n\n# Load the multcomp package\npacman::p_load(multcomp)\n\n\n\nBasic Usage\nLet’s start with a simple example:\n\n# Example dataset\ndata(warpbreaks)\n\n# Fit a linear model\nmodel &lt;- lm(breaks ~ wool + tension, data = warpbreaks)\n\n# Perform multiple comparisons\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\n\n# Summarize the results\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = breaks ~ wool + tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)   \nM - L == 0  -10.000      3.872  -2.582  0.03378 * \nH - L == 0  -14.722      3.872  -3.802  0.00112 **\nH - M == 0   -4.722      3.872  -1.219  0.44739   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nExplanation\n\nmcp: Specifies the type of multiple comparisons (e.g., Tukey’s test).\nglht: General linear hypothesis testing function.\nsummary: Displays adjusted p-values and confidence intervals."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#key-functions-and-features",
    "href": "appendix/r-packages/multcomp.html#key-functions-and-features",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "3. Key Functions and Features",
    "text": "3. Key Functions and Features\n\nglht()\nDescription: Performs multiple comparisons or general linear hypothesis tests.\nSyntax:\nmultcomp::glht(model, linfct = mcp(\"factor\"=\"method\"))\nArguments:\n\nmodel: A fitted linear model object.\nlinfct: Defines the hypotheses to test (e.g., Tukey or user-defined contrasts).\n\nExample:\n\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = breaks ~ wool + tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)   \nM - L == 0  -10.000      3.872  -2.582  0.03361 * \nH - L == 0  -14.722      3.872  -3.802  0.00121 **\nH - M == 0   -4.722      3.872  -1.219  0.44743   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\n\nmcp()\nDescription: Creates predefined contrasts for a factor.\nSyntax:\nmcp(factor = \"method\")\nExample:\nlinfct = mcp(tension = \"Tukey\")"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#in-depth-examples",
    "href": "appendix/r-packages/multcomp.html#in-depth-examples",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "4. In-Depth Examples",
    "text": "4. In-Depth Examples\n\nExample 1: ANOVA Multiple Comparisons\nObjective: Compare tension levels in warpbreaks dataset.\nSteps:\n\nFit the model:\n\n\nmodel &lt;- aov(breaks ~ tension, data = warpbreaks)\n\n\nPerform Tukey’s test:\n\n\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = breaks ~ tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)   \nM - L == 0  -10.000      3.960  -2.525   0.0383 * \nH - L == 0  -14.722      3.960  -3.718   0.0014 **\nH - M == 0   -4.722      3.960  -1.192   0.4631   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\nVisualize the results:\n\n\nplot(comparison)\n\n\n\n\n\n\n\n\nOutput: Provides adjusted p-values and confidence intervals."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#advanced-features",
    "href": "appendix/r-packages/multcomp.html#advanced-features",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "5. Advanced Features",
    "text": "5. Advanced Features\n\nCustom Contrasts\nCreate user-defined contrasts:\n\ncontrast_matrix &lt;- rbind(\n    \"M - L\" = c(-1, 1, 0),\n    \"H - L\" = c(-1, 0, 1),\n    \"H - M\" = c(0, -1, 1)\n)\ncomparison &lt;- glht(model, linfct = contrast_matrix)\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nFit: aov(formula = breaks ~ tension, data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error t value Pr(&gt;|t|)    \nM - L == 0  -46.389      6.262  -7.408   &lt;1e-04 ***\nH - L == 0  -51.111      6.262  -8.163   &lt;1e-04 ***\nH - M == 0   -4.722      3.960  -1.192    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\n\n\nIntegration with Mixed-Effects Models\nSupports models from lme4:\n\nlibrary(lme4)\nmodel &lt;- lmer(breaks ~ tension + (1 | wool), data = warpbreaks)\ncomparison &lt;- glht(model, linfct = mcp(tension = \"Tukey\"))\nsummary(comparison)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = breaks ~ tension + (1 | wool), data = warpbreaks)\n\nLinear Hypotheses:\n           Estimate Std. Error z value Pr(&gt;|z|)    \nM - L == 0  -10.000      3.872  -2.582  0.02638 *  \nH - L == 0  -14.722      3.872  -3.802  0.00039 ***\nH - M == 0   -4.722      3.872  -1.219  0.44161    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/multcomp.html#troubleshooting-and-faqs",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "6. Troubleshooting and FAQs",
    "text": "6. Troubleshooting and FAQs\n\nCommon Issues\n\nError: Non-conformable arguments\n\nEnsure the linfct matches the model’s factor levels.\n\nConfusion with model types\n\nUse glht with models that support linear hypotheses (e.g., lm, lmer).\n\n\n\n\nFAQs\n\nCan multcomp handle unbalanced data? Yes, but ensure your model accounts for this."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#best-practices",
    "href": "appendix/r-packages/multcomp.html#best-practices",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "7. Best Practices",
    "text": "7. Best Practices\n\nAlways check model diagnostics before multiple comparisons.\nUse visualizations like plot() to complement numerical results.\nFor large datasets, consider summarizing key results to avoid overloading the output."
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#conclusion",
    "href": "appendix/r-packages/multcomp.html#conclusion",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "8. Conclusion",
    "text": "8. Conclusion\n\nSummary\nThe multcomp package is a robust tool for conducting multiple comparisons in R, providing flexibility and accuracy.\n\n\nNext Steps\nExplore interactions in models and extend functionality with packages like emmeans.\n\n\nEncouragement\nExperiment with different datasets to build confidence in interpreting outputs!"
  },
  {
    "objectID": "appendix/r-packages/multcomp.html#references-and-resources",
    "href": "appendix/r-packages/multcomp.html#references-and-resources",
    "title": "Tutorial: Using the multcomp R Package",
    "section": "9. References and Resources",
    "text": "9. References and Resources\n\nPackage Documentation\nExamples on CRAN"
  },
  {
    "objectID": "appendix/r-packages/dplyr.html",
    "href": "appendix/r-packages/dplyr.html",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "The dplyr package is a powerful tool in the tidyverse suite designed for data manipulation. It simplifies and accelerates common data transformation tasks with an intuitive syntax.\nKey functions include:\n\ndplyr::select(): Select specific columns.\ndplyr::filter(): Filter rows based on conditions.\ndplyr::mutate(): Add or modify columns.\ndplyr::summarize(): Create summary statistics.\ndplyr::arrange(): Sort rows.\ndplyr::group_by(): Group data for grouped operations.\n\n\n\n\n\nFiltering large datasets for analysis.\nSummarizing data to extract insights.\nTransforming variables or creating new ones.\nSorting or organizing data for visualization or reporting.\n\n\n\n\n\nUser-friendly syntax, especially with the pipe operator |&gt;.\nOptimized for speed, making it suitable for large datasets.\nSeamless integration with other tidyverse packages.\n\n\n\n\n\n\n\nUse pacman::p_load() to load dplyr:\n\npacman::p_load(dplyr)\n\n\n\n\nCreate and manipulate a simple dataset:\n\ndata &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\n\n# Select columns\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n# Filter rows\ndata |&gt; dplyr::filter(Age &gt; 25)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nSelects specified columns from a data frame.\n\n\n\ndplyr::select(.data, ...)\n\n.data: A data frame.\n…: Columns to select.\n\n\n\n\n\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n\nThis extracts the Name and Score columns.\n\n\n\n\n\n\nFilters rows based on a logical condition.\n\n\n\ndplyr::filter(.data, ...)\n\n.data: A data frame.\n…: Logical conditions.\n\n\n\n\n\ndata |&gt; dplyr::filter(Age &gt; 25)\n\n\n  \n\n\n\nFilters rows where Age is greater than 25.\n\n\n\n\n\n\nCreates new columns or modifies existing ones.\n\n\n\ndplyr::mutate(.data, ...)\n\n.data: A data frame.\n…: Expressions for new or modified columns.\n\n\n\n\n\ndata |&gt; dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\n\n\n  \n\n\n\nAdds a Grade column based on Score.\n\n\n\n\n\n\n\n\n\nSummarize scores by age groups.\n\n\n\n\ndata |&gt;\n    dplyr::mutate(AgeGroup = ifelse(Age &gt; 25, \"Above 25\", \"25 and Below\")) |&gt;\n    dplyr::group_by(AgeGroup) |&gt;\n    dplyr::summarize(MeanScore = mean(Score), .groups = \"drop\")\n\n\n  \n\n\n\n\n\n\nA table showing average scores for each age group.\n\n\n\n\n\n\nFind the top scorer in each age group.\n\n\n\n\ndata |&gt;\n    dplyr::group_by(Age) |&gt;\n    dplyr::arrange(desc(Score)) |&gt;\n    dplyr::slice(1)\n\n\n  \n\n\n\n\n\n\nTop scorer details by age.\n\n\n\n\n\n\n\n\n\n\ndata |&gt; dplyr::rename(FullName = Name)\n\n\n  \n\n\n\n\n\n\n\ndata |&gt;\n    dplyr::mutate(\n        NormalizedScore = Score / max(Score),\n        Status = ifelse(Age &gt; 30, \"Senior\", \"Junior\")\n    )\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\ndata |&gt;\n    dplyr::mutate(Pass = Score &gt; 80) |&gt;\n    ggplot(aes(x = Age, y = Score, color = Pass)) +\n    geom_point() +\n    labs(title = \"Scores by Age\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse dplyr::filter() early in a pipeline to reduce rows.\nChain operations with |&gt; for clarity and efficiency.\n\n\n\n\n\n\n\n\n\nEnsure column names are correctly spelled and case-sensitive.\n\n\n\nThis occurs when filtering results in zero rows. Check filter conditions.\n\n\n\n\n\nCan dplyr handle large datasets? Yes, but for very large data, consider data.table or databases with dbplyr.\nHow to retain row names? Convert them to a column with tibble::rownames_to_column().\n\n\n\n\n\n\nUse consistent column naming conventions.\nApply group_by() only when summarizing or calculating grouped statistics.\nDocument pipelines for reproducibility.\n\n\n\n\n\n\n\ndplyr streamlines data manipulation with a readable syntax.\nKey functions cover selecting, filtering, mutating, and summarizing data.\n\n\n\n\nExplore dplyr advanced features like joins and window functions, or integrate with dbplyr for database operations.\n\n\n\n\n\ndplyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#introduction",
    "href": "appendix/r-packages/dplyr.html#introduction",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "The dplyr package is a powerful tool in the tidyverse suite designed for data manipulation. It simplifies and accelerates common data transformation tasks with an intuitive syntax.\nKey functions include:\n\ndplyr::select(): Select specific columns.\ndplyr::filter(): Filter rows based on conditions.\ndplyr::mutate(): Add or modify columns.\ndplyr::summarize(): Create summary statistics.\ndplyr::arrange(): Sort rows.\ndplyr::group_by(): Group data for grouped operations.\n\n\n\n\n\nFiltering large datasets for analysis.\nSummarizing data to extract insights.\nTransforming variables or creating new ones.\nSorting or organizing data for visualization or reporting.\n\n\n\n\n\nUser-friendly syntax, especially with the pipe operator |&gt;.\nOptimized for speed, making it suitable for large datasets.\nSeamless integration with other tidyverse packages."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#getting-started",
    "href": "appendix/r-packages/dplyr.html#getting-started",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "Use pacman::p_load() to load dplyr:\n\npacman::p_load(dplyr)\n\n\n\n\nCreate and manipulate a simple dataset:\n\ndata &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\n\n# Select columns\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n# Filter rows\ndata |&gt; dplyr::filter(Age &gt; 25)"
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#key-functions-and-features",
    "href": "appendix/r-packages/dplyr.html#key-functions-and-features",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "Selects specified columns from a data frame.\n\n\n\ndplyr::select(.data, ...)\n\n.data: A data frame.\n…: Columns to select.\n\n\n\n\n\ndata |&gt; dplyr::select(Name, Score)\n\n\n  \n\n\n\nThis extracts the Name and Score columns.\n\n\n\n\n\n\nFilters rows based on a logical condition.\n\n\n\ndplyr::filter(.data, ...)\n\n.data: A data frame.\n…: Logical conditions.\n\n\n\n\n\ndata |&gt; dplyr::filter(Age &gt; 25)\n\n\n  \n\n\n\nFilters rows where Age is greater than 25.\n\n\n\n\n\n\nCreates new columns or modifies existing ones.\n\n\n\ndplyr::mutate(.data, ...)\n\n.data: A data frame.\n…: Expressions for new or modified columns.\n\n\n\n\n\ndata |&gt; dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\n\n\n  \n\n\n\nAdds a Grade column based on Score."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#in-depth-examples",
    "href": "appendix/r-packages/dplyr.html#in-depth-examples",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "Summarize scores by age groups.\n\n\n\n\ndata |&gt;\n    dplyr::mutate(AgeGroup = ifelse(Age &gt; 25, \"Above 25\", \"25 and Below\")) |&gt;\n    dplyr::group_by(AgeGroup) |&gt;\n    dplyr::summarize(MeanScore = mean(Score), .groups = \"drop\")\n\n\n  \n\n\n\n\n\n\nA table showing average scores for each age group.\n\n\n\n\n\n\nFind the top scorer in each age group.\n\n\n\n\ndata |&gt;\n    dplyr::group_by(Age) |&gt;\n    dplyr::arrange(desc(Score)) |&gt;\n    dplyr::slice(1)\n\n\n  \n\n\n\n\n\n\nTop scorer details by age."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#advanced-features",
    "href": "appendix/r-packages/dplyr.html#advanced-features",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "data |&gt; dplyr::rename(FullName = Name)\n\n\n  \n\n\n\n\n\n\n\ndata |&gt;\n    dplyr::mutate(\n        NormalizedScore = Score / max(Score),\n        Status = ifelse(Age &gt; 30, \"Senior\", \"Junior\")\n    )\n\n\n  \n\n\n\n\n\n\n\nCombine with ggplot2 for visualization:\n\nlibrary(ggplot2)\ndata |&gt;\n    dplyr::mutate(Pass = Score &gt; 80) |&gt;\n    ggplot(aes(x = Age, y = Score, color = Pass)) +\n    geom_point() +\n    labs(title = \"Scores by Age\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse dplyr::filter() early in a pipeline to reduce rows.\nChain operations with |&gt; for clarity and efficiency."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/dplyr.html#troubleshooting-and-faqs",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "Ensure column names are correctly spelled and case-sensitive.\n\n\n\nThis occurs when filtering results in zero rows. Check filter conditions.\n\n\n\n\n\nCan dplyr handle large datasets? Yes, but for very large data, consider data.table or databases with dbplyr.\nHow to retain row names? Convert them to a column with tibble::rownames_to_column()."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#best-practices",
    "href": "appendix/r-packages/dplyr.html#best-practices",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "Use consistent column naming conventions.\nApply group_by() only when summarizing or calculating grouped statistics.\nDocument pipelines for reproducibility."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#conclusion",
    "href": "appendix/r-packages/dplyr.html#conclusion",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "dplyr streamlines data manipulation with a readable syntax.\nKey functions cover selecting, filtering, mutating, and summarizing data.\n\n\n\n\nExplore dplyr advanced features like joins and window functions, or integrate with dbplyr for database operations."
  },
  {
    "objectID": "appendix/r-packages/dplyr.html#references-and-resources",
    "href": "appendix/r-packages/dplyr.html#references-and-resources",
    "title": "Using the Dplyr Package in R",
    "section": "",
    "text": "dplyr Documentation\nR for Data Science\nTidyverse GitHub"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html",
    "href": "appendix/r-packages/emmeans.html",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "The emmeans package in R simplifies post-hoc analysis and estimation of marginal means from statistical models. It provides tools to estimate, compare, and test means across levels of predictors while accounting for the model structure."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#introduction",
    "href": "appendix/r-packages/emmeans.html#introduction",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "The emmeans package in R simplifies post-hoc analysis and estimation of marginal means from statistical models. It provides tools to estimate, compare, and test means across levels of predictors while accounting for the model structure."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#overview",
    "href": "appendix/r-packages/emmeans.html#overview",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Overview",
    "text": "Overview\n\nPurpose: Provides estimated marginal means (EMMs) or least-squares means.\nCore Features:\n\nCompute EMMs for any combination of factors.\nPerform pairwise comparisons or custom contrasts.\nIntegrate with popular visualization packages like ggplot2.\nEasily handle model interactions and transformations.\n\nKey Supported Models:\n\nLinear models (lm, aov).\nGeneralized linear models (glm).\nMixed-effects models (lme4, nlme)."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#what-are-marginal-means",
    "href": "appendix/r-packages/emmeans.html#what-are-marginal-means",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "What Are Marginal Means?",
    "text": "What Are Marginal Means?\nMarginal means are predicted values from a model, adjusted to account for the distribution of covariates or other factors. They provide a clearer interpretation of group effects compared to raw means."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#getting-started",
    "href": "appendix/r-packages/emmeans.html#getting-started",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Getting Started",
    "text": "2. Getting Started"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#loading-the-package",
    "href": "appendix/r-packages/emmeans.html#loading-the-package",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Loading the Package",
    "text": "Loading the Package\n\nlibrary(emmeans)"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#example-dataset",
    "href": "appendix/r-packages/emmeans.html#example-dataset",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Example Dataset",
    "text": "Example Dataset\nWe will use the warpbreaks dataset, which contains data on the number of breaks in yarn (response variable breaks) across two factors: wool type (wool) and tension level (tension)."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#basic-model",
    "href": "appendix/r-packages/emmeans.html#basic-model",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Basic Model",
    "text": "Basic Model\n\n# Fitting a linear model\nlm_model &lt;- lm(breaks ~ wool * tension, data = warpbreaks)"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#basic-marginal-means",
    "href": "appendix/r-packages/emmeans.html#basic-marginal-means",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Basic Marginal Means",
    "text": "Basic Marginal Means\n\n# Marginal means by wool\nemmeans(lm_model, ~ wool)\n\n wool emmean   SE df lower.CL upper.CL\n A      31.0 2.11 48     26.8     35.3\n B      25.3 2.11 48     21.0     29.5\n\nResults are averaged over the levels of: tension \nConfidence level used: 0.95"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#key-functions-in-emmeans",
    "href": "appendix/r-packages/emmeans.html#key-functions-in-emmeans",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Key Functions in emmeans",
    "text": "3. Key Functions in emmeans"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#emmeans",
    "href": "appendix/r-packages/emmeans.html#emmeans",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3.1 emmeans()",
    "text": "3.1 emmeans()\nThe emmeans() function is the foundation of the package, used to estimate marginal means.\nUsage:\nemmeans(object, specs, by = NULL, options = NULL, ...)\nArguments:\n\nobject: A fitted model (e.g., lm, glm, or lmer).\nspecs: Formula specifying factors for which marginal means are computed. Use | to specify conditions (e.g., ~ wool | tension).\nby: Variables to group results.\noptions: Adjustments like type of prediction (response, link, etc.).\n\nExamples:\n\n# Marginal means for wool\nemmeans(lm_model, ~wool)\n\n wool emmean   SE df lower.CL upper.CL\n A      31.0 2.11 48     26.8     35.3\n B      25.3 2.11 48     21.0     29.5\n\nResults are averaged over the levels of: tension \nConfidence level used: 0.95 \n\n# Marginal means of tension within each wool group\nemmeans(lm_model, ~ tension | wool)\n\nwool = A:\n tension emmean   SE df lower.CL upper.CL\n L         44.6 3.65 48     37.2     51.9\n M         24.0 3.65 48     16.7     31.3\n H         24.6 3.65 48     17.2     31.9\n\nwool = B:\n tension emmean   SE df lower.CL upper.CL\n L         28.2 3.65 48     20.9     35.6\n M         28.8 3.65 48     21.4     36.1\n H         18.8 3.65 48     11.4     26.1\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#contrast",
    "href": "appendix/r-packages/emmeans.html#contrast",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3.2 contrast()",
    "text": "3.2 contrast()\nThe contrast() function performs post-hoc comparisons between marginal means, such as pairwise comparisons or custom contrasts.\nUsage:\ncontrast(object, method = \"pairwise\", interaction = FALSE, by = NULL, ...)\nArguments:\n\nobject: An emmGrid object (output of emmeans()).\nmethod: Type of comparison. Common methods:\n\n\"pairwise\": Pairwise differences.\n\"poly\": Polynomial contrasts.\n\"eff\": Effect contrasts (deviation from overall mean).\n\ninteraction: Defines interaction contrasts, generating contrasts of contrasts for multi-factor interactions.\nby: Grouping variables for separate contrasts.\n\nExamples:\n\n# Pairwise comparisons of tension levels\nwarp.emm &lt;- emmeans(lm_model, ~ tension | wool)\ncontrast(warp.emm, method = \"pairwise\")\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L - M      20.556 5.16 48   3.986  0.0007\n L - H      20.000 5.16 48   3.878  0.0009\n M - H      -0.556 5.16 48  -0.108  0.9936\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L - M      -0.556 5.16 48  -0.108  0.9936\n L - H       9.444 5.16 48   1.831  0.1704\n M - H      10.000 5.16 48   1.939  0.1389\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n# Custom contrasts for tension levels\ncontrast(warp.emm, list(mid.vs.ends = c(-1, 2, -1) / 2, lo.vs.hi = c(1, 0, -1)))\n\nwool = A:\n contrast    estimate   SE df t.ratio p.value\n mid.vs.ends   -10.56 4.47 48  -2.363  0.0222\n lo.vs.hi       20.00 5.16 48   3.878  0.0003\n\nwool = B:\n contrast    estimate   SE df t.ratio p.value\n mid.vs.ends     5.28 4.47 48   1.182  0.2432\n lo.vs.hi        9.44 5.16 48   1.831  0.0733"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#pairs",
    "href": "appendix/r-packages/emmeans.html#pairs",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3.3 pairs()",
    "text": "3.3 pairs()\nThis function simplifies pairwise comparisons (shortcut for contrast(…, method = \"pairwise\")).\nUsage:\npairs(object, reverse = FALSE, ...)\nArguments:\n\nreverse: If TRUE, reverses the order of comparisons.\n\nExample:\n\n# Pairwise comparisons for tension\npairs(warp.emm)\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L - M      20.556 5.16 48   3.986  0.0007\n L - H      20.000 5.16 48   3.878  0.0009\n M - H      -0.556 5.16 48  -0.108  0.9936\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L - M      -0.556 5.16 48  -0.108  0.9936\n L - H       9.444 5.16 48   1.831  0.1704\n M - H      10.000 5.16 48   1.939  0.1389\n\nP value adjustment: tukey method for comparing a family of 3 estimates"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#advanced-functionality",
    "href": "appendix/r-packages/emmeans.html#advanced-functionality",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Advanced Functionality",
    "text": "4. Advanced Functionality"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#interaction-contrasts",
    "href": "appendix/r-packages/emmeans.html#interaction-contrasts",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4.1 Interaction Contrasts",
    "text": "4.1 Interaction Contrasts\nInteraction contrasts allow you to compare interactions between factors. The interaction argument in contrast() enables this.\nExample:\n\n# Interaction contrast for wool and tension\ninteraction_contrast &lt;- contrast(warp.emm, interaction = c(wool = \"eff\", tension = \"poly\"))\ninteraction_contrast\n\nwool = A:\n tension_eff estimate   SE df t.ratio p.value\n L effect       13.52 2.98 48   4.540  &lt;.0001\n M effect       -7.04 2.98 48  -2.363  0.0222\n H effect       -6.48 2.98 48  -2.177  0.0344\n\nwool = B:\n tension_eff estimate   SE df t.ratio p.value\n L effect        2.96 2.98 48   0.995  0.3247\n M effect        3.52 2.98 48   1.182  0.2432\n H effect       -6.48 2.98 48  -2.177  0.0344\n\ncoef(interaction_contrast)  # View coefficients"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#scaling-and-offsetting",
    "href": "appendix/r-packages/emmeans.html#scaling-and-offsetting",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4.2 Scaling and Offsetting",
    "text": "4.2 Scaling and Offsetting\nUse scale and offset to transform results for interpretation (e.g., converting units).\nExample:\n\n# Convert temperature from Celsius to Fahrenheit\nmod &lt;- lm(Water.Temp ~ poly(stack.loss, degree = 2), data = stackloss)\nemm &lt;- emmeans(mod, \"stack.loss\", at = list(stack.loss = 10 * (1:4)))\ncontrast(emm, \"identity\", scale = 9 / 5, offset = 32)\n\n contrast     estimate    SE df t.ratio p.value\n stack.loss10     65.9 0.833 18  79.138  &lt;.0001\n stack.loss20     72.1 1.020 18  71.059  &lt;.0001\n stack.loss30     76.8 1.160 18  66.067  &lt;.0001\n stack.loss40     80.0 1.720 18  46.389  &lt;.0001"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#adjusting-for-multiple-comparisons",
    "href": "appendix/r-packages/emmeans.html#adjusting-for-multiple-comparisons",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4.3 Adjusting for Multiple Comparisons",
    "text": "4.3 Adjusting for Multiple Comparisons\nUse the adjust argument to control corrections like Bonferroni or Tukey:\n\ncontrast(warp.emm, method = \"pairwise\", adjust = \"tukey\")\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L - M      20.556 5.16 48   3.986  0.0007\n L - H      20.000 5.16 48   3.878  0.0009\n M - H      -0.556 5.16 48  -0.108  0.9936\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L - M      -0.556 5.16 48  -0.108  0.9936\n L - H       9.444 5.16 48   1.831  0.1704\n M - H      10.000 5.16 48   1.939  0.1389\n\nP value adjustment: tukey method for comparing a family of 3 estimates"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#visualizing-results",
    "href": "appendix/r-packages/emmeans.html#visualizing-results",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Visualizing Results",
    "text": "5. Visualizing Results"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#interaction-plots",
    "href": "appendix/r-packages/emmeans.html#interaction-plots",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Interaction Plots",
    "text": "Interaction Plots\nThe emmip() function creates interaction plots to visualize EMMs:\n\n# Interaction plot for wool and tension\nlibrary(ggplot2)\nemmip(warp.emm, wool ~ tension)"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#pairwise-comparisons-plot",
    "href": "appendix/r-packages/emmeans.html#pairwise-comparisons-plot",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Pairwise Comparisons Plot",
    "text": "Pairwise Comparisons Plot\n\n# Pairwise comparison plot\nplot(pairs(warp.emm))"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#common-issues-and-troubleshooting",
    "href": "appendix/r-packages/emmeans.html#common-issues-and-troubleshooting",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "6. Common Issues and Troubleshooting",
    "text": "6. Common Issues and Troubleshooting"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#common-issues",
    "href": "appendix/r-packages/emmeans.html#common-issues",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Common Issues",
    "text": "Common Issues\n\nDegrees of freedom not available:\n\nSome models (e.g., Bayesian) require manual specification of degrees of freedom.\n\nInvalid contrast method:\n\nEnsure the method matches the structure of your emmGrid object."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#best-practices",
    "href": "appendix/r-packages/emmeans.html#best-practices",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways inspect the emmGrid object using summary() before applying contrasts.\nUse confint() to obtain confidence intervals.\nApply adjust for proper corrections in multiple comparisons."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#detailed-argument-reference",
    "href": "appendix/r-packages/emmeans.html#detailed-argument-reference",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "7. Detailed Argument Reference",
    "text": "7. Detailed Argument Reference"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#emmeans-specific-arguments",
    "href": "appendix/r-packages/emmeans.html#emmeans-specific-arguments",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "emmeans() Specific Arguments",
    "text": "emmeans() Specific Arguments\n\nspecs: Defines the terms of interest. For example:\n\n~ factor1: Marginal means for factor1.\n~ factor1 | factor2: Marginal means of factor1 grouped by factor2."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#contrast-specific-arguments",
    "href": "appendix/r-packages/emmeans.html#contrast-specific-arguments",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "contrast() Specific Arguments",
    "text": "contrast() Specific Arguments\n\ninteraction: Defines contrasts for interactions. Example:\n\n\ncontrast(warp.emm, interaction = list(wool = \"pairwise\", tension = \"eff\"))\n\nwool = A:\n tension_pairwise estimate   SE df t.ratio p.value\n L - M              20.556 5.16 48   3.986  0.0002\n L - H              20.000 5.16 48   3.878  0.0003\n M - H              -0.556 5.16 48  -0.108  0.9147\n\nwool = B:\n tension_pairwise estimate   SE df t.ratio p.value\n L - M              -0.556 5.16 48  -0.108  0.9147\n L - H               9.444 5.16 48   1.831  0.0733\n M - H              10.000 5.16 48   1.939  0.0584\n\n\n\nscale and offset: For transforming estimates. Example:\n\n\ncontrast(warp.emm, method = \"identity\", scale = 2, offset = 1)\n\nwool = A:\n contrast estimate   SE df t.ratio p.value\n L            90.1 7.29 48  12.355  &lt;.0001\n M            49.0 7.29 48   6.718  &lt;.0001\n H            50.1 7.29 48   6.871  &lt;.0001\n\nwool = B:\n contrast estimate   SE df t.ratio p.value\n L            57.4 7.29 48   7.876  &lt;.0001\n M            58.6 7.29 48   8.028  &lt;.0001\n H            38.6 7.29 48   5.286  &lt;.0001"
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#conclusion",
    "href": "appendix/r-packages/emmeans.html#conclusion",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "8. Conclusion",
    "text": "8. Conclusion\nThe emmeans package provides a robust framework for estimating and interpreting marginal means. With extensive support for complex models and versatile post-hoc analysis tools, it is essential for data analysis workflows."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#next-steps",
    "href": "appendix/r-packages/emmeans.html#next-steps",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Next Steps",
    "text": "Next Steps\n\nExplore advanced vignettes in the package documentation.\nIntegrate results into visualizations with ggplot2."
  },
  {
    "objectID": "appendix/r-packages/emmeans.html#references-and-resources",
    "href": "appendix/r-packages/emmeans.html#references-and-resources",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "9. References and Resources",
    "text": "9. References and Resources\n\nemmeans CRAN Page\nVignettes: Estimated Marginal Means\nGitHub Repository"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html",
    "href": "appendix/r-topics/r-subsetting-basics.html",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Subsetting data in R is a fundamental operation for data manipulation and analysis. It involves selecting specific rows, columns, or elements of a data structure based on conditions or indices. This tutorial explores subsetting techniques in R using Base R and tidyverse packages. It will provide comprehensive examples, tips, and nuances to master this essential skill.\nPackages and Functions Covered:\n\nBase R Functions: [], subset()\nTidyverse Functions: dplyr::filter(), dplyr::select(), dplyr::slice(), dplyr::pull()\n\nUse Cases:\n\nExtracting subsets of data for focused analysis.\nFiltering data for visualization or modeling.\nReducing dataset size for computation efficiency.\nHandling missing or specific data points.\n\nBenefits:\n\nBase R: Lightweight and does not require additional installations.\nTidyverse: Provides clean, readable syntax and integrates well with pipelines (|&gt;)."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#introduction",
    "href": "appendix/r-topics/r-subsetting-basics.html#introduction",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "",
    "text": "Subsetting data in R is a fundamental operation for data manipulation and analysis. It involves selecting specific rows, columns, or elements of a data structure based on conditions or indices. This tutorial explores subsetting techniques in R using Base R and tidyverse packages. It will provide comprehensive examples, tips, and nuances to master this essential skill.\nPackages and Functions Covered:\n\nBase R Functions: [], subset()\nTidyverse Functions: dplyr::filter(), dplyr::select(), dplyr::slice(), dplyr::pull()\n\nUse Cases:\n\nExtracting subsets of data for focused analysis.\nFiltering data for visualization or modeling.\nReducing dataset size for computation efficiency.\nHandling missing or specific data points.\n\nBenefits:\n\nBase R: Lightweight and does not require additional installations.\nTidyverse: Provides clean, readable syntax and integrates well with pipelines (|&gt;)."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#getting-started",
    "href": "appendix/r-topics/r-subsetting-basics.html#getting-started",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Package:\n\npacman::p_load(dplyr)  # Loads or installs the dplyr package\n\nBasic Usage: Select rows where mpg &gt; 20 using both approaches:\nBase R:\n\ndata(mtcars)\nmtcars[mtcars$mpg &gt; 20, ]\n\n\n  \n\n\n\nTidyverse:\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20)"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-subsetting-basics.html#key-functions-and-features",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Key Functions and Features",
    "text": "Key Functions and Features\n\nBase R: Subsetting with []\nDescription: The simplest and most versatile method for subsetting in R. Use [] to extract rows, columns, or individual elements from data frames or matrices.\nSyntax:\ndata[rows, columns]\nExamples:\n\nSelect rows where mpg &gt; 20:\n\n\nmtcars[mtcars$mpg &gt; 20, ]\n\n\n  \n\n\n\n\nSelect specific columns:\n\n\nmtcars[, c(\"mpg\", \"cyl\")]\n\n\n  \n\n\n\n\nCombine row and column conditions:\n\n\nmtcars[mtcars$mpg &gt; 20, c(\"mpg\", \"cyl\")]\n\n\n  \n\n\n\nNuances:\n\nOmitting rows or columns selects all:\n\n\nmtcars[mtcars$mpg &gt; 20, ]  # All columns selected\n\n\n  \n\n\nmtcars[, c(\"mpg\", \"cyl\")]  # All rows selected\n\n\n  \n\n\n\n\nNegative indexing excludes rows or columns:\n\n\nmtcars[-c(1, 2), ]  # Exclude first two rows\n\n\n  \n\n\n\n\n\nBase R: subset()\nDescription: Provides a more human-readable syntax for subsetting.\nSyntax:\nsubset(data, subset, select)\nExamples:\n\nSubset rows where mpg &gt; 20 and select mpg and cyl columns:\n\n\nsubset(mtcars, mpg &gt; 20, select = c(mpg, cyl))\n\n\n  \n\n\n\n\nSubset using multiple conditions:\n\n\nsubset(mtcars, mpg &gt; 20 & cyl == 6, select = c(mpg, cyl))\n\n\n  \n\n\n\nNuances:\n\nNon-standard evaluation allows simpler syntax but can lead to ambiguities. Use column names explicitly when necessary.\n\n\n\nTidyverse: dplyr::filter()\nDescription: Filters rows based on logical conditions.\nSyntax:\ndplyr::filter(data, condition)\nExamples:\n\nFilter rows with mpg &gt; 20:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20)\n\n\n  \n\n\n\n\nFilter with multiple conditions:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20 & cyl == 6)\n\n\n  \n\n\n\n\nUse dynamic variables in filtering:\n\n\nthreshold &lt;- 20\nmtcars |&gt;\n    dplyr::filter(mpg &gt; threshold)\n\n\n  \n\n\n\nNuances:\n\nUse | for “or” conditions:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; 20 | cyl == 6)\n\n\n  \n\n\n\n\nFilter with is.na() to handle missing values:\n\n\nmtcars |&gt;\n    dplyr::filter(!is.na(mpg))\n\n\n  \n\n\n\n\n\nTidyverse: dplyr::select()\nDescription: Selects specific columns from a data frame.\nSyntax:\ndplyr::select(data, columns)\nExamples:\n\nSelect specific columns:\n\n\nmtcars |&gt;\n    dplyr::select(mpg, cyl)\n\n\n  \n\n\n\n\nUse helpers for dynamic selection:\n\n\nmtcars |&gt;\n    dplyr::select(starts_with(\"d\"))\n\n\n  \n\n\n\nNuances:\n\nCombine helpers for advanced selection:\n\n\nmtcars |&gt;\n    dplyr::select(starts_with(\"d\"), ends_with(\"t\"))\n\n\n  \n\n\n\n\n\nTidyverse: dplyr::slice()\nDescription: Selects rows by position.\nSyntax:\ndplyr::slice(data, row_numbers)\nExamples:\n\nSelect the first five rows:\n\n\nmtcars |&gt;\n    dplyr::slice(1:5)\n\n\n  \n\n\n\n\nRandom sampling:\n\n\nmtcars |&gt;\n    dplyr::slice_sample(n = 5)\n\n\n  \n\n\n\n\nSelect rows by percentages:\n\n\nmtcars |&gt;\n    dplyr::slice_sample(prop = .1)  # 10% of rows\n\n\n  \n\n\n\n\n\nTidyverse: dplyr::pull()\nDescription: Extracts a single column as a vector.\nSyntax:\ndplyr::pull(data, column)\nExamples:\n\nPull the mpg column:\n\n\nmtcars |&gt;\n    dplyr::pull(mpg)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4\n[16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7\n[31] 15.0 21.4\n\n\n\nCombine with filtering:\n\n\nmtcars |&gt;\n    dplyr::filter(cyl == 6) |&gt;\n    dplyr::pull(mpg)\n\n[1] 21.0 21.0 21.4 18.1 19.2 17.8 19.7\n\n\nNuances:\n\nUseful for extracting data for external functions:\n\n\nmean(mtcars |&gt; dplyr::pull(mpg))\n\n[1] 20.09062"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-subsetting-basics.html#in-depth-examples",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "In-Depth Examples",
    "text": "In-Depth Examples\n\nExample 1: Dynamic Filtering with External Variables\nObjective: Filter rows based on user-defined thresholds for mpg and hp.\nSteps:\n\nDefine thresholds:\n\n\nmpg_threshold &lt;- 20\nhp_threshold &lt;- 100\n\n\nBase R:\n\n\nmtcars[mtcars$mpg &gt; mpg_threshold & mtcars$hp &gt; hp_threshold, ]\n\n\n  \n\n\n\n\nTidyverse:\n\n\nmtcars |&gt;\n    dplyr::filter(mpg &gt; mpg_threshold & hp &gt; hp_threshold)\n\n\n  \n\n\n\n\n\nExample 2: Visualizing Subsets\nObjective: Visualize cars with mpg &gt; 20.\nSteps:\n\nSubset data:\n\n\nfiltered_data &lt;- mtcars |&gt;\n    dplyr::filter(mpg &gt; 20)\n\n\nPlot:\n\n\nlibrary(ggplot2)\nggplot(filtered_data, aes(x = wt, y = mpg)) +\n    geom_point()"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#advanced-features",
    "href": "appendix/r-topics/r-subsetting-basics.html#advanced-features",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Advanced Features",
    "text": "Advanced Features\nCustomization:\n\nCreate reusable functions for subsetting:\n\n\nfilter_cars &lt;- function(data, mpg_threshold, cyl_threshold) {\n    data |&gt;\n        dplyr::filter(mpg &gt; mpg_threshold, cyl == cyl_threshold)\n}\n\nfilter_cars(mtcars, 20, 6)\n\n\n  \n\n\n\nIntegration:\n\nCombine tidyr for reshaping with subsetting:\n\n\nlibrary(tidyr)\nmtcars |&gt;\n    pivot_longer(cols = everything()) |&gt;\n    dplyr::filter(value &gt; 20)"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-subsetting-basics.html#troubleshooting-and-faqs",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Troubleshooting and FAQs",
    "text": "Troubleshooting and FAQs\n\nIssue: “undefined columns selected” Fix: Double-check column names.\nIssue: Subsetting returns no rows. Fix: Ensure logical conditions are met.\n\nFAQs:\n\nHow to subset with regex? Use grepl() in Base R or matches() in dplyr::select():\n\n\nmtcars |&gt;\n    dplyr::select(matches(\"^m\"))"
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#best-practices",
    "href": "appendix/r-topics/r-subsetting-basics.html#best-practices",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Best Practices",
    "text": "Best Practices\n\nPreview data before subsetting using head() or glimpse().\nChain operations for clarity in tidyverse.\nAvoid hard-coding indices; use dynamic variables."
  },
  {
    "objectID": "appendix/r-topics/r-subsetting-basics.html#conclusion",
    "href": "appendix/r-topics/r-subsetting-basics.html#conclusion",
    "title": "STAT 454/545 - Analysis of Variance and Experimental Design",
    "section": "Conclusion",
    "text": "Conclusion\nRecap: Subsetting is a powerful tool for extracting specific data. Base R provides flexibility, while tidyverse offers an elegant syntax for modern workflows."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html",
    "href": "appendix/r-topics/r-data-frames-basics.html",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Creating data frames is fundamental in R for managing and analyzing tabular data. This tutorial explores:\n\nBase R Functions: data.frame() and as.data.frame().\ntidyverse Functions: tibble::tibble() and dplyr::data_frame().\n\nData frames allow structured data storage, enabling row and column manipulations akin to spreadsheets.\n\n\n\n\nCleaning and structuring raw data.\nAggregating results from computations.\nPreparing data for visualization or modeling.\n\n\n\n\n\nBase R: Straightforward and widely supported.\ntidyverse: Intuitive syntax, compatibility with the pipe operator |&gt;, and enhanced features like printing.\n\n\n\n\n\n\n\n\n# Use pacman to load packages\npacman::p_load(tidyverse)\n\n\n\n\n\n# Create a simple data frame\ndata_base &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_base\n\n\n  \n\n\n\n\n\n\n\n# Create a tibble\ndata_tidy &lt;- tibble::tibble(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_tidy\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\ndata.frame() creates data frames from vectors or lists.\n\n\n\n#| echo: fenced\ndata.frame(x, ..., row.names = NULL, check.rows = FALSE, check.names = TRUE)\n\n\n\n\ndata_example &lt;- data.frame(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_example\n\n\n  \n\n\n\n\n\n\n\n\n\ntibble::tibble() constructs modern data frames with automatic column type detection.\n\n\n\ntibble::tibble(...)\n\n\n\n\ndata_tidy_example &lt;- tibble::tibble(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_tidy_example\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nCombine two data frames by a common key.\n\n\n\n\ndf1 &lt;- tibble::tibble(ID = 1:3, Value1 = c(10, 20, 30))\ndf2 &lt;- tibble::tibble(ID = 2:4, Value2 = c(40, 50, 60))\nmerged &lt;- dplyr::left_join(df1, df2, by = \"ID\")\nmerged\n\n\n  \n\n\n\n\n\n\nObserve how NA fills for unmatched rows.\n\n\n\n\n\n\nFilter rows where Value1 &gt; 15 and select specific columns.\n\n\n\n\nfiltered &lt;- merged |&gt;\n    dplyr::filter(Value1 &gt; 15) |&gt;\n    dplyr::select(ID, Value1)\nfiltered\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\ncolnames(data_base) &lt;- c(\"FullName\", \"Years\", \"ExamScore\")\ndata_base\n\n\n  \n\n\n\n\n\n\n\ndata_tidy &lt;- data_tidy |&gt;\n    dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\ndata_tidy\n\n\n  \n\n\n\n\n\n\n\nGenerate a plot from the data.\n\nggplot2::ggplot(data_tidy, ggplot2::aes(x = Age, y = Score)) +\n    ggplot2::geom_point() +\n    ggplot2::labs(title = \"Age vs. Score\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse data.table for large data sets.\nPredefine column types for better memory usage.\n\n\n\n\n\n\n\n\nData Mismatch: Ensure column lengths match when creating data frames.\n\n# Error example\n# data.frame(a = 1:3, b = 1:4)\n\nType Conversion Warnings: Avoid unintended type coercion by specifying stringsAsFactors = FALSE.\n\n\n\n\n\nWhy use tibble? tibble provides cleaner, more readable output, and better handling of column names and types.\n\n\n\n\n\n\nUse consistent column names and data types.\nValidate data integrity before further processing.\nLeverage tidyverse pipes for readability and maintainability.\n\n\n\n\n\n\n\nBase R is great for quick setups and legacy code.\ntidyverse excels for scalable, readable, and modern workflows.\n\n\n\n\nExplore advanced techniques like data.table or R6 for complex data operations.\n\n\n\n\n\nBase R Documentation\nTidyverse Guide\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#introduction",
    "href": "appendix/r-topics/r-data-frames-basics.html#introduction",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Creating data frames is fundamental in R for managing and analyzing tabular data. This tutorial explores:\n\nBase R Functions: data.frame() and as.data.frame().\ntidyverse Functions: tibble::tibble() and dplyr::data_frame().\n\nData frames allow structured data storage, enabling row and column manipulations akin to spreadsheets.\n\n\n\n\nCleaning and structuring raw data.\nAggregating results from computations.\nPreparing data for visualization or modeling.\n\n\n\n\n\nBase R: Straightforward and widely supported.\ntidyverse: Intuitive syntax, compatibility with the pipe operator |&gt;, and enhanced features like printing."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#getting-started",
    "href": "appendix/r-topics/r-data-frames-basics.html#getting-started",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "# Use pacman to load packages\npacman::p_load(tidyverse)\n\n\n\n\n\n# Create a simple data frame\ndata_base &lt;- data.frame(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_base\n\n\n  \n\n\n\n\n\n\n\n# Create a tibble\ndata_tidy &lt;- tibble::tibble(\n    Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    Age = c(25, 30, 35),\n    Score = c(85.5, 90.3, 78.9)\n)\ndata_tidy"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-data-frames-basics.html#key-functions-and-features",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "data.frame() creates data frames from vectors or lists.\n\n\n\n#| echo: fenced\ndata.frame(x, ..., row.names = NULL, check.rows = FALSE, check.names = TRUE)\n\n\n\n\ndata_example &lt;- data.frame(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_example\n\n\n  \n\n\n\n\n\n\n\n\n\ntibble::tibble() constructs modern data frames with automatic column type detection.\n\n\n\ntibble::tibble(...)\n\n\n\n\ndata_tidy_example &lt;- tibble::tibble(\n    ID = 1:3,\n    Category = c(\"A\", \"B\", \"A\"),\n    Value = c(100, 200, 150)\n)\ndata_tidy_example"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-data-frames-basics.html#in-depth-examples",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Combine two data frames by a common key.\n\n\n\n\ndf1 &lt;- tibble::tibble(ID = 1:3, Value1 = c(10, 20, 30))\ndf2 &lt;- tibble::tibble(ID = 2:4, Value2 = c(40, 50, 60))\nmerged &lt;- dplyr::left_join(df1, df2, by = \"ID\")\nmerged\n\n\n  \n\n\n\n\n\n\nObserve how NA fills for unmatched rows.\n\n\n\n\n\n\nFilter rows where Value1 &gt; 15 and select specific columns.\n\n\n\n\nfiltered &lt;- merged |&gt;\n    dplyr::filter(Value1 &gt; 15) |&gt;\n    dplyr::select(ID, Value1)\nfiltered"
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#advanced-features",
    "href": "appendix/r-topics/r-data-frames-basics.html#advanced-features",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "colnames(data_base) &lt;- c(\"FullName\", \"Years\", \"ExamScore\")\ndata_base\n\n\n  \n\n\n\n\n\n\n\ndata_tidy &lt;- data_tidy |&gt;\n    dplyr::mutate(Grade = ifelse(Score &gt; 80, \"Pass\", \"Fail\"))\ndata_tidy\n\n\n  \n\n\n\n\n\n\n\nGenerate a plot from the data.\n\nggplot2::ggplot(data_tidy, ggplot2::aes(x = Age, y = Score)) +\n    ggplot2::geom_point() +\n    ggplot2::labs(title = \"Age vs. Score\", x = \"Age\", y = \"Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nUse data.table for large data sets.\nPredefine column types for better memory usage."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-data-frames-basics.html#troubleshooting-and-faqs",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Data Mismatch: Ensure column lengths match when creating data frames.\n\n# Error example\n# data.frame(a = 1:3, b = 1:4)\n\nType Conversion Warnings: Avoid unintended type coercion by specifying stringsAsFactors = FALSE.\n\n\n\n\n\nWhy use tibble? tibble provides cleaner, more readable output, and better handling of column names and types."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#best-practices",
    "href": "appendix/r-topics/r-data-frames-basics.html#best-practices",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Use consistent column names and data types.\nValidate data integrity before further processing.\nLeverage tidyverse pipes for readability and maintainability."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#conclusion",
    "href": "appendix/r-topics/r-data-frames-basics.html#conclusion",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Base R is great for quick setups and legacy code.\ntidyverse excels for scalable, readable, and modern workflows.\n\n\n\n\nExplore advanced techniques like data.table or R6 for complex data operations."
  },
  {
    "objectID": "appendix/r-topics/r-data-frames-basics.html#references-and-resources",
    "href": "appendix/r-topics/r-data-frames-basics.html#references-and-resources",
    "title": "Creating Data Frames in R",
    "section": "",
    "text": "Base R Documentation\nTidyverse Guide\nR for Data Science"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html",
    "href": "appendix/r-base/r-interaction-basics.html",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function in R is used to combine two or more factors into a single factor. It is particularly useful in ANOVA or linear modeling when you need to analyze interactions between factors.\n\n\n\n\nExploring interaction effects between multiple factors in ANOVA.\nCreating unique combinations of factor levels.\nPreparing datasets for hierarchical or nested analyses.\n\n\n\n\n\nSimplifies the analysis of interactions between factors.\nEnsures unique identification of factor level combinations.\nSeamlessly integrates with ANOVA and modeling workflows in R.\n\n\n\n\n\n\n\nThe interaction function is part of base R, so no additional packages are required.\n\n\n\n\n# Define two factors\nfactor1 &lt;- gl(2, 3, labels = c(\"Low\", \"High\"))\nfactor2 &lt;- gl(3, 1, 6, labels = c(\"A\", \"B\", \"C\"))\n\n# Combine factors using interaction\ncombined &lt;- interaction(factor1, factor2)\nprint(combined)\n\n[1] Low.A  Low.B  Low.C  High.A High.B High.C\nLevels: Low.A High.A Low.B High.B Low.C High.C\n\n\nAnnotations:\n\ngl: Generates factors with specified levels and repetitions.\ninteraction: Combines multiple factors into a single factor.\n\n\n\n\n\n\n\n\n\nCreates a single factor by combining multiple factors. Each level of the new factor corresponds to a unique combination of levels from the input factors.\n\n\n\ninteraction(..., sep = \":\", lex.order = FALSE)\n\n…: Factors to be combined.\nsep: Separator for combined levels (default is \":\").\nlex.order: Logical; determines if levels should be ordered lexicographically.\n\n\n\n\n\n# Example with custom separator\ncombined &lt;- interaction(factor1, factor2, sep = \"-\")\nprint(combined)\n\n[1] Low-A  Low-B  Low-C  High-A High-B High-C\nLevels: Low-A High-A Low-B High-B Low-C High-C\n\n\n\n\n\n\n\n\n\n\n\nAnalyze the interaction between treatment and dose in an experiment.\n\n\n\n\nCreate a dataset with two factors and a response variable.\nUse interaction to generate a combined factor.\nPerform two-way ANOVA.\n\n\n\n\n\n# Create factors and response\nset.seed(42)\ntreatment &lt;- gl(2, 6, labels = c(\"Control\", \"Treatment\"))\ndose &lt;- gl(3, 2, 12, labels = c(\"Low\", \"Medium\", \"High\"))\nresponse &lt;- c(rnorm(6, 5), rnorm(6, 6))\n\n# Combine factors\ninteraction_factor &lt;- interaction(treatment, dose)\n\n# Perform ANOVA\nanova_model &lt;- aov(response ~ treatment * dose)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntreatment       1  9.835   9.835   9.872  0.020 *\ndose            2  0.349   0.174   0.175  0.843  \ntreatment:dose  2  1.064   0.532   0.534  0.612  \nResiduals       6  5.977   0.996                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nVisualize interaction effects using interaction and ggplot2.\n\n\n\n\nGenerate a dataset with interaction factors.\nCreate a plot of means by interaction levels.\n\n\n\n\n\nlibrary(ggplot2)\n\n# Create dataset\ndata &lt;- data.frame(\n    Treatment = treatment,\n    Dose = dose,\n    Response = response,\n    Interaction = interaction(treatment, dose)\n)\n\n# Plot\nggplot(data, aes(x = Interaction, y = Response, fill = Treatment)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = \"Interaction Effects\", x = \"Interaction\", y = \"Response\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize interaction labels using the sep argument.\n\ninteraction_factor &lt;- interaction(treatment, dose, sep = \"_\")\nprint(interaction_factor)\n\n [1] Control_Low      Control_Low      Control_Medium   Control_Medium  \n [5] Control_High     Control_High     Treatment_Low    Treatment_Low   \n [9] Treatment_Medium Treatment_Medium Treatment_High   Treatment_High  \n6 Levels: Control_Low Treatment_Low Control_Medium ... Treatment_High\n\n\n\n\n\nUse interaction directly in model formulas.\n\nanova_model &lt;- aov(response ~ interaction(treatment, dose))\nsummary(anova_model)\n\n                             Df Sum Sq Mean Sq F value Pr(&gt;F)\ninteraction(treatment, dose)  5 11.248  2.2495   2.258  0.175\nResiduals                     6  5.977  0.9962               \n\n\n\n\n\nFor large datasets, ensure factors are properly ordered to reduce computational overhead.\n\n\n\n\n\n\n\nMismatch in factor lengths:\n\nEnsure all input factors have the same length.\n\nUnexpected level combinations:\n\nUse lex.order = TRUE to enforce lexicographic ordering.\n\n\n\n\n\n\n\nUse descriptive factor labels for clarity.\nApply interaction within model formulas for cleaner workflows.\nVisualize interaction effects to better understand relationships between factors.\n\n\n\n\n\n\n\nThe interaction function is essential for analyzing and visualizing factor interactions.\nIt integrates seamlessly with ANOVA and modeling workflows.\n\n\n\n\n\nExplore interactions with more than two factors.\nCombine interaction with advanced visualization tools like ggplot2.\n\n\n\n\n\n\nR Documentation on interaction\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#introduction",
    "href": "appendix/r-base/r-interaction-basics.html#introduction",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function in R is used to combine two or more factors into a single factor. It is particularly useful in ANOVA or linear modeling when you need to analyze interactions between factors.\n\n\n\n\nExploring interaction effects between multiple factors in ANOVA.\nCreating unique combinations of factor levels.\nPreparing datasets for hierarchical or nested analyses.\n\n\n\n\n\nSimplifies the analysis of interactions between factors.\nEnsures unique identification of factor level combinations.\nSeamlessly integrates with ANOVA and modeling workflows in R."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#getting-started",
    "href": "appendix/r-base/r-interaction-basics.html#getting-started",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function is part of base R, so no additional packages are required.\n\n\n\n\n# Define two factors\nfactor1 &lt;- gl(2, 3, labels = c(\"Low\", \"High\"))\nfactor2 &lt;- gl(3, 1, 6, labels = c(\"A\", \"B\", \"C\"))\n\n# Combine factors using interaction\ncombined &lt;- interaction(factor1, factor2)\nprint(combined)\n\n[1] Low.A  Low.B  Low.C  High.A High.B High.C\nLevels: Low.A High.A Low.B High.B Low.C High.C\n\n\nAnnotations:\n\ngl: Generates factors with specified levels and repetitions.\ninteraction: Combines multiple factors into a single factor."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-interaction-basics.html#key-functions-and-features",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Creates a single factor by combining multiple factors. Each level of the new factor corresponds to a unique combination of levels from the input factors.\n\n\n\ninteraction(..., sep = \":\", lex.order = FALSE)\n\n…: Factors to be combined.\nsep: Separator for combined levels (default is \":\").\nlex.order: Logical; determines if levels should be ordered lexicographically.\n\n\n\n\n\n# Example with custom separator\ncombined &lt;- interaction(factor1, factor2, sep = \"-\")\nprint(combined)\n\n[1] Low-A  Low-B  Low-C  High-A High-B High-C\nLevels: Low-A High-A Low-B High-B Low-C High-C"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-interaction-basics.html#in-depth-examples",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Analyze the interaction between treatment and dose in an experiment.\n\n\n\n\nCreate a dataset with two factors and a response variable.\nUse interaction to generate a combined factor.\nPerform two-way ANOVA.\n\n\n\n\n\n# Create factors and response\nset.seed(42)\ntreatment &lt;- gl(2, 6, labels = c(\"Control\", \"Treatment\"))\ndose &lt;- gl(3, 2, 12, labels = c(\"Low\", \"Medium\", \"High\"))\nresponse &lt;- c(rnorm(6, 5), rnorm(6, 6))\n\n# Combine factors\ninteraction_factor &lt;- interaction(treatment, dose)\n\n# Perform ANOVA\nanova_model &lt;- aov(response ~ treatment * dose)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntreatment       1  9.835   9.835   9.872  0.020 *\ndose            2  0.349   0.174   0.175  0.843  \ntreatment:dose  2  1.064   0.532   0.534  0.612  \nResiduals       6  5.977   0.996                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nVisualize interaction effects using interaction and ggplot2.\n\n\n\n\nGenerate a dataset with interaction factors.\nCreate a plot of means by interaction levels.\n\n\n\n\n\nlibrary(ggplot2)\n\n# Create dataset\ndata &lt;- data.frame(\n    Treatment = treatment,\n    Dose = dose,\n    Response = response,\n    Interaction = interaction(treatment, dose)\n)\n\n# Plot\nggplot(data, aes(x = Interaction, y = Response, fill = Treatment)) +\n    geom_boxplot() +\n    theme_minimal() +\n    labs(title = \"Interaction Effects\", x = \"Interaction\", y = \"Response\")"
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#advanced-features",
    "href": "appendix/r-base/r-interaction-basics.html#advanced-features",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Customize interaction labels using the sep argument.\n\ninteraction_factor &lt;- interaction(treatment, dose, sep = \"_\")\nprint(interaction_factor)\n\n [1] Control_Low      Control_Low      Control_Medium   Control_Medium  \n [5] Control_High     Control_High     Treatment_Low    Treatment_Low   \n [9] Treatment_Medium Treatment_Medium Treatment_High   Treatment_High  \n6 Levels: Control_Low Treatment_Low Control_Medium ... Treatment_High\n\n\n\n\n\nUse interaction directly in model formulas.\n\nanova_model &lt;- aov(response ~ interaction(treatment, dose))\nsummary(anova_model)\n\n                             Df Sum Sq Mean Sq F value Pr(&gt;F)\ninteraction(treatment, dose)  5 11.248  2.2495   2.258  0.175\nResiduals                     6  5.977  0.9962               \n\n\n\n\n\nFor large datasets, ensure factors are properly ordered to reduce computational overhead."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-interaction-basics.html#troubleshooting-and-faqs",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Mismatch in factor lengths:\n\nEnsure all input factors have the same length.\n\nUnexpected level combinations:\n\nUse lex.order = TRUE to enforce lexicographic ordering."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#best-practices",
    "href": "appendix/r-base/r-interaction-basics.html#best-practices",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "Use descriptive factor labels for clarity.\nApply interaction within model formulas for cleaner workflows.\nVisualize interaction effects to better understand relationships between factors."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#conclusion",
    "href": "appendix/r-base/r-interaction-basics.html#conclusion",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "The interaction function is essential for analyzing and visualizing factor interactions.\nIt integrates seamlessly with ANOVA and modeling workflows.\n\n\n\n\n\nExplore interactions with more than two factors.\nCombine interaction with advanced visualization tools like ggplot2."
  },
  {
    "objectID": "appendix/r-base/r-interaction-basics.html#references-and-resources",
    "href": "appendix/r-base/r-interaction-basics.html#references-and-resources",
    "title": "R Tutorial: Using interaction and Related ANOVA Functions",
    "section": "",
    "text": "R Documentation on interaction\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html",
    "href": "appendix/r-base/r-gl-basics.html",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function in R is used to generate factor levels for balanced designs. It is particularly useful in creating factors for experimental designs, enabling users to set up data for analysis of variance (ANOVA) efficiently.\n\n\n\n\nConstructing balanced datasets for factorial experiments.\nSimplifying the creation of factor levels in simulations.\nPreparing data for linear modeling and ANOVA.\n\n\n\n\n\nStreamlines the creation of balanced datasets.\nReduces errors in data preparation.\nIntegrates seamlessly with ANOVA-related functions in R.\n\n\n\n\n\n\n\nThe gl function is part of base R, so no additional libraries are required.\n\n\n\n\n# Generate a factor with 3 levels, each repeated 5 times\nfactor_levels &lt;- gl(n = 3, k = 5, length = 15)\nprint(factor_levels)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\nLevels: 1 2 3\n\n\nAnnotations:\n\nn: Number of levels.\nk: Number of repetitions per level.\nlength: Total length of the factor.\n\n\n\n\n\n\n\n\n\nCreates balanced factors by specifying the number of levels, repetitions, and total length.\n\n\n\ngl(n, k, length = n*k, labels = NULL)\n\nn: Number of levels.\nk: Repetitions of each level.\nlength: Total length of the factor.\nlabels: Custom labels for the factor levels.\n\n\n\n\n\n# Example with custom labels\nfactor_levels &lt;- gl(n = 2, k = 4, labels = c(\"Control\", \"Treatment\"))\nprint(factor_levels)\n\n[1] Control   Control   Control   Control   Treatment Treatment Treatment\n[8] Treatment\nLevels: Control Treatment\n\n\n\n\n\n\n\n\n\n\n\nCreate factors for a 2x3 factorial design.\n\n\n\n\nDefine two factors using gl.\nCombine factors into a dataset.\n\n\n\n\n\n# Factor 1: Treatment with 2 levels\ntreatment &lt;- gl(n = 2, k = 3, labels = c(\"Low\", \"High\"))\n\n# Factor 2: Dose with 3 levels\ndose &lt;- gl(n = 3, k = 1, length = 6, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# Combine into a data frame\ndata &lt;- data.frame(Treatment = treatment, Dose = dose)\nprint(data)\n\n  Treatment   Dose\n1       Low    Low\n2       Low Medium\n3       Low   High\n4      High    Low\n5      High Medium\n6      High   High\n\n\n\n\n\n\n\n\nConduct a one-way ANOVA on a dataset.\n\n\n\n\nSimulate data.\nFit an ANOVA model.\n\n\n\n\n\n# Simulate data\nset.seed(123)\ngroup &lt;- gl(3, 10, labels = c(\"A\", \"B\", \"C\"))\nresponse &lt;- c(rnorm(10, mean = 5), rnorm(10, mean = 6), rnorm(10, mean = 7))\n\n# Create a data frame\ndata &lt;- data.frame(Group = group, Response = response)\n\n# Perform ANOVA\nanova_model &lt;- aov(Response ~ Group, data = data)\nsummary(anova_model)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nGroup        2  12.24   6.122   6.435 0.00518 **\nResiduals   27  25.68   0.951                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\ncustom_labels &lt;- gl(n = 3, k = 5, labels = c(\"Group1\", \"Group2\", \"Group3\"))\nprint(custom_labels)\n\n [1] Group1 Group1 Group1 Group1 Group1 Group2 Group2 Group2 Group2 Group2\n[11] Group3 Group3 Group3 Group3 Group3\nLevels: Group1 Group2 Group3\n\n\n\n\n\nCombine factors to create interaction terms.\n\ninteraction_factor &lt;- interaction(treatment, dose)\nprint(interaction_factor)\n\n[1] Low.Low     Low.Medium  Low.High    High.Low    High.Medium High.High  \nLevels: Low.Low High.Low Low.Medium High.Medium Low.High High.High\n\n\n\n\n\n\n\n\n\nMismatch in lengths: Ensure length matches n * k if specified.\nIncorrect labels: Verify the number of labels matches n.\n\n\n\n\n\n\nAlways use descriptive labels for factors for clarity.\nUse gl in combination with other functions like expand.grid for complex designs.\nValidate the structure of your factors using table() or str().\n\n\n\n\n\n\n\nThe gl function is a versatile tool for creating factors in R.\nIt simplifies the setup of balanced designs and integrates seamlessly with ANOVA workflows.\n\n\n\n\n\nExplore interactions between factors.\nApply gl in complex factorial experiments.\n\n\n\n\n\n\nR Documentation on gl\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#introduction",
    "href": "appendix/r-base/r-gl-basics.html#introduction",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function in R is used to generate factor levels for balanced designs. It is particularly useful in creating factors for experimental designs, enabling users to set up data for analysis of variance (ANOVA) efficiently.\n\n\n\n\nConstructing balanced datasets for factorial experiments.\nSimplifying the creation of factor levels in simulations.\nPreparing data for linear modeling and ANOVA.\n\n\n\n\n\nStreamlines the creation of balanced datasets.\nReduces errors in data preparation.\nIntegrates seamlessly with ANOVA-related functions in R."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#getting-started",
    "href": "appendix/r-base/r-gl-basics.html#getting-started",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function is part of base R, so no additional libraries are required.\n\n\n\n\n# Generate a factor with 3 levels, each repeated 5 times\nfactor_levels &lt;- gl(n = 3, k = 5, length = 15)\nprint(factor_levels)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\nLevels: 1 2 3\n\n\nAnnotations:\n\nn: Number of levels.\nk: Number of repetitions per level.\nlength: Total length of the factor."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#key-functions-and-features",
    "href": "appendix/r-base/r-gl-basics.html#key-functions-and-features",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Creates balanced factors by specifying the number of levels, repetitions, and total length.\n\n\n\ngl(n, k, length = n*k, labels = NULL)\n\nn: Number of levels.\nk: Repetitions of each level.\nlength: Total length of the factor.\nlabels: Custom labels for the factor levels.\n\n\n\n\n\n# Example with custom labels\nfactor_levels &lt;- gl(n = 2, k = 4, labels = c(\"Control\", \"Treatment\"))\nprint(factor_levels)\n\n[1] Control   Control   Control   Control   Treatment Treatment Treatment\n[8] Treatment\nLevels: Control Treatment"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#in-depth-examples",
    "href": "appendix/r-base/r-gl-basics.html#in-depth-examples",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Create factors for a 2x3 factorial design.\n\n\n\n\nDefine two factors using gl.\nCombine factors into a dataset.\n\n\n\n\n\n# Factor 1: Treatment with 2 levels\ntreatment &lt;- gl(n = 2, k = 3, labels = c(\"Low\", \"High\"))\n\n# Factor 2: Dose with 3 levels\ndose &lt;- gl(n = 3, k = 1, length = 6, labels = c(\"Low\", \"Medium\", \"High\"))\n\n# Combine into a data frame\ndata &lt;- data.frame(Treatment = treatment, Dose = dose)\nprint(data)\n\n  Treatment   Dose\n1       Low    Low\n2       Low Medium\n3       Low   High\n4      High    Low\n5      High Medium\n6      High   High\n\n\n\n\n\n\n\n\nConduct a one-way ANOVA on a dataset.\n\n\n\n\nSimulate data.\nFit an ANOVA model.\n\n\n\n\n\n# Simulate data\nset.seed(123)\ngroup &lt;- gl(3, 10, labels = c(\"A\", \"B\", \"C\"))\nresponse &lt;- c(rnorm(10, mean = 5), rnorm(10, mean = 6), rnorm(10, mean = 7))\n\n# Create a data frame\ndata &lt;- data.frame(Group = group, Response = response)\n\n# Perform ANOVA\nanova_model &lt;- aov(Response ~ Group, data = data)\nsummary(anova_model)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nGroup        2  12.24   6.122   6.435 0.00518 **\nResiduals   27  25.68   0.951                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#advanced-features",
    "href": "appendix/r-base/r-gl-basics.html#advanced-features",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "custom_labels &lt;- gl(n = 3, k = 5, labels = c(\"Group1\", \"Group2\", \"Group3\"))\nprint(custom_labels)\n\n [1] Group1 Group1 Group1 Group1 Group1 Group2 Group2 Group2 Group2 Group2\n[11] Group3 Group3 Group3 Group3 Group3\nLevels: Group1 Group2 Group3\n\n\n\n\n\nCombine factors to create interaction terms.\n\ninteraction_factor &lt;- interaction(treatment, dose)\nprint(interaction_factor)\n\n[1] Low.Low     Low.Medium  Low.High    High.Low    High.Medium High.High  \nLevels: Low.Low High.Low Low.Medium High.Medium Low.High High.High"
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-base/r-gl-basics.html#troubleshooting-and-faqs",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Mismatch in lengths: Ensure length matches n * k if specified.\nIncorrect labels: Verify the number of labels matches n."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#best-practices",
    "href": "appendix/r-base/r-gl-basics.html#best-practices",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "Always use descriptive labels for factors for clarity.\nUse gl in combination with other functions like expand.grid for complex designs.\nValidate the structure of your factors using table() or str()."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#conclusion",
    "href": "appendix/r-base/r-gl-basics.html#conclusion",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "The gl function is a versatile tool for creating factors in R.\nIt simplifies the setup of balanced designs and integrates seamlessly with ANOVA workflows.\n\n\n\n\n\nExplore interactions between factors.\nApply gl in complex factorial experiments."
  },
  {
    "objectID": "appendix/r-base/r-gl-basics.html#references-and-resources",
    "href": "appendix/r-base/r-gl-basics.html#references-and-resources",
    "title": "R Tutorial: Using gl and Related ANOVA Functions",
    "section": "",
    "text": "R Documentation on gl\nANOVA in R Tutorial"
  },
  {
    "objectID": "appendix/r-base/r-sample-basic.html",
    "href": "appendix/r-base/r-sample-basic.html",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "",
    "text": "The sample() function operates on vectors or integers and uses random number generation to produce randomized output.\n\n\n\nx:\n\nA vector of elements to sample from.\nIf x is a single positive integer, sample() assumes x as 1:x (i.e., the sequence from 1 to x).\n\nsize:\n\nSpecifies the number of elements to sample. Default is the length of x (i.e., it permutes the entire input if size is not specified).\n\nreplace:\n\nLogical, determines if sampling is with replacement (TRUE) or without replacement (FALSE).\nDefault: FALSE.\n\nprob:\n\nA vector of weights for sampling probabilities. Weights are applied proportionally for sampling elements.\nDefault: NULL (uniform probabilities).\n\nuseHash (in sample.int):\n\nOptimizes sampling for large datasets when replace = FALSE, prob = NULL, and size &lt;= n/2.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf replace = FALSE, size should not exceed the length of x. Otherwise, an error occurs."
  },
  {
    "objectID": "appendix/r-base/r-sample-basic.html#function-description",
    "href": "appendix/r-base/r-sample-basic.html#function-description",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "",
    "text": "The sample() function operates on vectors or integers and uses random number generation to produce randomized output.\n\n\n\nx:\n\nA vector of elements to sample from.\nIf x is a single positive integer, sample() assumes x as 1:x (i.e., the sequence from 1 to x).\n\nsize:\n\nSpecifies the number of elements to sample. Default is the length of x (i.e., it permutes the entire input if size is not specified).\n\nreplace:\n\nLogical, determines if sampling is with replacement (TRUE) or without replacement (FALSE).\nDefault: FALSE.\n\nprob:\n\nA vector of weights for sampling probabilities. Weights are applied proportionally for sampling elements.\nDefault: NULL (uniform probabilities).\n\nuseHash (in sample.int):\n\nOptimizes sampling for large datasets when replace = FALSE, prob = NULL, and size &lt;= n/2.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf replace = FALSE, size should not exceed the length of x. Otherwise, an error occurs."
  },
  {
    "objectID": "appendix/r-base/r-sample-basic.html#key-features",
    "href": "appendix/r-base/r-sample-basic.html#key-features",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "Key Features",
    "text": "Key Features\n\nRandom Permutation:\n\nIf no size is provided and replace = FALSE, the function shuffles all elements of x:\n\n\n\nset.seed(42) # For reproducibility\nsample(1:5)  # Outputs a random permutation, e.g., c(3, 5, 1, 4, 2)\n\n[1] 1 5 4 3 2\n\n\n\nWith Replacement:\n\nAllows sampling the same element multiple times:\n\n\n\nsample(1:3, size = 5, replace = TRUE)  # Outputs, e.g., c(1, 3, 2, 2, 1)\n\n[1] 2 2 1 3 3\n\n\n\nWeighted Sampling:\n\nSampling based on a specified probability vector:\n\n\n\nsample(1:3, size = 6, prob = c(0.1, 0.7, 0.2), replace = TRUE )  # Likely favors \"2\"\n\n[1] 2 2 1 1 2 2\n\n\n\nDefault Behavior with Single Numeric x:\n\nTreats x as 1:x:\n\n\n\nsample(5)  # Equivalent to sample(1:5)\n\n[1] 2 3 4 1 5\n\n\n\nEdge Cases:\n\nIf size = 0 and x is empty or has length zero, the result is a zero-length vector.\nNon-integer x or n is truncated to the nearest smaller integer."
  },
  {
    "objectID": "appendix/r-base/r-sample-basic.html#examples",
    "href": "appendix/r-base/r-sample-basic.html#examples",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "Examples",
    "text": "Examples\n\n1. Random Permutation\n\nx &lt;- c(\"A\", \"B\", \"C\", \"D\")\nsample(x)  # Randomly shuffles elements, e.g., c(\"C\", \"A\", \"D\", \"B\")\n\n[1] \"C\" \"A\" \"D\" \"B\"\n\n\n\n\n2. Sampling With Replacement\n\nsample(1:4, size = 6, replace = TRUE)\n\n[1] 1 4 2 4 4 3\n\n# Outputs, e.g., c(2, 4, 1, 3, 2, 4)\n\n\n\n3. Weighted Sampling\n\nsample(letters[1:5], size = 5, prob = c(0.5, 0.2, 0.1, 0.1, 0.1), replace = TRUE)\n\n[1] \"e\" \"a\" \"a\" \"d\" \"b\"\n\n# Heavily favors \"a\" in the output\n\n\n\n4. Default Behavior for Single Integer\n\nsample(5)  # Equivalent to sample(1:5)\n\n[1] 3 5 2 4 1\n\n\n\n\n5. Safeguarding Edge Cases with sample.int\n\n# Programmatically handle edge cases for length(x) &gt; 1:\nresample &lt;- function(x, ...) x[sample.int(length(x), ...)]\n\nx &lt;- 1:10\nresample(x[x &gt; 8])   # Properly handles cases where x has length &lt; 2\n\n[1] 10  9\n\nresample(x[x &gt; 10])  # Returns an empty vector safely\n\ninteger(0)"
  },
  {
    "objectID": "appendix/r-base/r-sample-basic.html#best-practices",
    "href": "appendix/r-base/r-sample-basic.html#best-practices",
    "title": "Randomizing Treatments in R Using sample()",
    "section": "Best Practices",
    "text": "Best Practices\n\nReproducibility: Use set.seed() to ensure reproducible randomization:\n\n\nset.seed(42)\nsample(1:5)\n\n[1] 1 5 4 3 2\n\n\n\nEdge Cases: Use sample.int for numeric sequences, especially in programmatic contexts, to avoid surprises with default sample() behavior.\nWeight Verification: Ensure that weights in prob are non-negative and not all zero.\nLarge Dataset Efficiency: Use useHash (via sample.int) for large n to optimize memory and performance."
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html",
    "href": "appendix/r-base/r-rep-basics.html",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "The rep function in R replicates the elements of vectors. It is commonly used to:\n\nRepeat single values or sequences.\nExpand data sets with repetitive patterns.\nCreate predictable test cases for analysis or debugging.\n\nIt is a versatile function that supports various ways to replicate elements, making it a fundamental tool in data manipulation.\n\n\n\nThe rep function has the following arguments:\n\nx:\n\nDescription: The vector to be replicated.\nType: Numeric, character, logical, or any other R object.\nRequired: Yes.\nExamples:\n\n\n\nrep(1:3, times = 2) # Output: 1 2 3 1 2 3\n\n[1] 1 2 3 1 2 3\n\n\n\ntimes:\n\nDescription: Specifies the number of times to repeat each element.\nType: Numeric vector (must be non-negative integers).\nDefault: None; must be explicitly defined if used.\nExamples:\n\n\n\nrep(1:3, times = c(2, 3, 1)) # Output: 1 1 2 2 2 3\n\n[1] 1 1 2 2 2 3\n\n\n\neach:\n\nDescription: Specifies the number of times to repeat each element individually.\nType: Numeric scalar (non-negative integer).\nDefault: 1.\nExamples:\n\n\n\nrep(1:3, each = 2) # Output: 1 1 2 2 3 3\n\n[1] 1 1 2 2 3 3\n\n\n\nlength.out:\n\nDescription: Specifies the total length of the resulting vector.\nType: Numeric scalar.\nDefault: NULL (calculated based on other arguments).\nExamples:\n\n\n\nrep(1:3, length.out = 5) # Output: 1 2 3 1 2\n\n[1] 1 2 3 1 2\n\n\n\n\n\n\nCombining each and times:\n\nWhen both each and times are specified, each operates first, followed by times.\n\n\n\nrep(1:2, each = 2, times = 2) # Output: 1 1 2 2 1 1 2 2\n\n[1] 1 1 2 2 1 1 2 2\n\n\n\nNon-integer inputs:\n\nNon-integer times, each, or length.out values are truncated to their integer part.\n\n\n\nrep(1:2, times = 2.7) # Output: 1 1 2 2\n\n[1] 1 2 1 2\n\n\n\nHandling NA values:\n\nWorks seamlessly with vectors containing NA.\n\n\n\nrep(c(NA, 1), each = 2) # Output: NA NA 1 1\n\n[1] NA NA  1  1\n\n\n\n\n\nBasic Usage:\n\n# Repeat the sequence twice\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\n# Output: 1 2 3 1 2 3\n\nUsing each:\n\n# Repeat each element twice\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3\n\nCustom Length:\n\n# Truncate the output to a specific length\nrep(1:3, length.out = 4)\n\n[1] 1 2 3 1\n\n# Output: 1 2 3 1\n\nCombining each and times:\n\n# A complex repetition pattern\nrep(1:3, each = 2, times = 3)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3 1 1 2 2 3 3\n\n\n\n\n\nNegative values for times or each:\n\nError: Error in rep.int: invalid 'times' argument\nFix: Ensure times and each are non-negative integers.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = -1) # Error\n```\n\n\nMismatched lengths for times:\n\nThe times vector length must match x or be recycled appropriately.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = c(1, 2)) # Warning: longer object length is not a multiple of shorter object length\n```\n\n\nNon-numeric inputs:\n\nEnsure that all arguments like length.out, each, and times are numeric.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = \"two\") # Error\n```\n\n\nRecycling:\n\nIf times or each is shorter than x, it generates an error.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = c(2, 3)) # Output: 1 1 2 2 2 3\n```"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#function-overview",
    "href": "appendix/r-base/r-rep-basics.html#function-overview",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "The rep function in R replicates the elements of vectors. It is commonly used to:\n\nRepeat single values or sequences.\nExpand data sets with repetitive patterns.\nCreate predictable test cases for analysis or debugging.\n\nIt is a versatile function that supports various ways to replicate elements, making it a fundamental tool in data manipulation."
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#arguments-explanation",
    "href": "appendix/r-base/r-rep-basics.html#arguments-explanation",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "The rep function has the following arguments:\n\nx:\n\nDescription: The vector to be replicated.\nType: Numeric, character, logical, or any other R object.\nRequired: Yes.\nExamples:\n\n\n\nrep(1:3, times = 2) # Output: 1 2 3 1 2 3\n\n[1] 1 2 3 1 2 3\n\n\n\ntimes:\n\nDescription: Specifies the number of times to repeat each element.\nType: Numeric vector (must be non-negative integers).\nDefault: None; must be explicitly defined if used.\nExamples:\n\n\n\nrep(1:3, times = c(2, 3, 1)) # Output: 1 1 2 2 2 3\n\n[1] 1 1 2 2 2 3\n\n\n\neach:\n\nDescription: Specifies the number of times to repeat each element individually.\nType: Numeric scalar (non-negative integer).\nDefault: 1.\nExamples:\n\n\n\nrep(1:3, each = 2) # Output: 1 1 2 2 3 3\n\n[1] 1 1 2 2 3 3\n\n\n\nlength.out:\n\nDescription: Specifies the total length of the resulting vector.\nType: Numeric scalar.\nDefault: NULL (calculated based on other arguments).\nExamples:\n\n\n\nrep(1:3, length.out = 5) # Output: 1 2 3 1 2\n\n[1] 1 2 3 1 2"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#nuances-and-advanced-features",
    "href": "appendix/r-base/r-rep-basics.html#nuances-and-advanced-features",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "Combining each and times:\n\nWhen both each and times are specified, each operates first, followed by times.\n\n\n\nrep(1:2, each = 2, times = 2) # Output: 1 1 2 2 1 1 2 2\n\n[1] 1 1 2 2 1 1 2 2\n\n\n\nNon-integer inputs:\n\nNon-integer times, each, or length.out values are truncated to their integer part.\n\n\n\nrep(1:2, times = 2.7) # Output: 1 1 2 2\n\n[1] 1 2 1 2\n\n\n\nHandling NA values:\n\nWorks seamlessly with vectors containing NA.\n\n\n\nrep(c(NA, 1), each = 2) # Output: NA NA 1 1\n\n[1] NA NA  1  1"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#examples",
    "href": "appendix/r-base/r-rep-basics.html#examples",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "Basic Usage:\n\n# Repeat the sequence twice\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\n# Output: 1 2 3 1 2 3\n\nUsing each:\n\n# Repeat each element twice\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3\n\nCustom Length:\n\n# Truncate the output to a specific length\nrep(1:3, length.out = 4)\n\n[1] 1 2 3 1\n\n# Output: 1 2 3 1\n\nCombining each and times:\n\n# A complex repetition pattern\nrep(1:3, each = 2, times = 3)\n\n [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3\n\n# Output: 1 1 2 2 3 3 1 1 2 2 3 3"
  },
  {
    "objectID": "appendix/r-base/r-rep-basics.html#common-errors-and-troubleshooting",
    "href": "appendix/r-base/r-rep-basics.html#common-errors-and-troubleshooting",
    "title": "Tutorial on rep function in R",
    "section": "",
    "text": "Negative values for times or each:\n\nError: Error in rep.int: invalid 'times' argument\nFix: Ensure times and each are non-negative integers.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = -1) # Error\n```\n\n\nMismatched lengths for times:\n\nThe times vector length must match x or be recycled appropriately.\n\n\n\n```{r}\n#| eval: false\n\nrep(1:3, times = c(1, 2)) # Warning: longer object length is not a multiple of shorter object length\n```\n\n\nNon-numeric inputs:\n\nEnsure that all arguments like length.out, each, and times are numeric.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = \"two\") # Error\n```\n\n\nRecycling:\n\nIf times or each is shorter than x, it generates an error.\n\n\n\n```{r}\n#| eval: false\nrep(1:3, times = c(2, 3)) # Output: 1 1 2 2 2 3\n```"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html",
    "href": "appendix/r-topics/r-data-transformation.html",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Data transformation and wrangling involve cleaning, reshaping, and modifying data to prepare it for analysis. This tutorial introduces:\n\nBase R Functions: subset(), transform(), merge().\ntidyverse Functions: dplyr::filter(), dplyr::mutate(), tidyr::pivot_longer(), and others.\n\n\n\n\n\nFiltering rows based on conditions.\nAdding or modifying columns.\nReshaping data between wide and long formats.\nCombining or joining datasets.\n\n\n\n\n\nBase R: Built into R, no additional installations required.\ntidyverse: Consistent syntax and powerful chaining with the pipe operator |&gt;.\n\n\n\n\n\n\n\n\n# Base R requires no additional loading.\n\n# Load tidyverse package using pacman\npacman::p_load(tidyverse)\n\n\n\n\n\n\n\n\n\nDescription: Extract rows based on a condition.\nSyntax:\nsubset(x, subset, select = NULL)\nArguments:\n\nx: A data frame or similar object.\nsubset: Logical expression indicating rows to keep.\nselect: Columns to retain, or NULL to keep all.\n\nExample:\n\ndata(iris)\n# Filter rows where Species is \"setosa\"\nsubset(iris, Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Add or modify columns in a data frame.\nSyntax:\ntransform(data, ...)\nArguments:\n\ndata: A data frame.\n…: Expressions specifying transformations.\n\nExample:\n\n# Add a new column for Sepal Area\niris &lt;- transform(iris, Sepal.Area = Sepal.Length * Sepal.Width)\nhead(iris)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets based on common columns (keys).\nSyntax:\nmerge(x, y, by, all)\nArguments:\n\nx, y: Data frames to merge.\nby: Column names to use as keys. Defaults to all common columns.\nall: Logical. TRUE for full outer join; FALSE for inner join.\n\nExample:\n\n# Create example datasets\ndf1 &lt;- data.frame(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- data.frame(ID = 2:4, Score = c(15, 25, 35))\n# Merge on ID\nmerge(df1, df2, by = \"ID\")\n\n\n  \n\n\n\n\n\n\n\n\n\nDescription: Select rows using conditions with dplyr::filter().\nSyntax:\ndplyr::filter(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Logical conditions. Rows where the condition is TRUE are retained.\n\nExample:\n\n# Filter rows where Species is \"setosa\"\niris |&gt;\n  dplyr::filter(Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Create or modify columns using dplyr::mutate().\nSyntax:\ndplyr::mutate(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Name-value pairs where names are new or existing column names, and values are expressions.\n\nExample:\n\n# Add Sepal.Area column\niris |&gt;\n  dplyr::mutate(Sepal.Area = Sepal.Length * Sepal.Width)\n\n\n  \n\n\n\n\n\n\nDescription: Convert data between wide and long formats with tidyr::pivot_longer().\nSyntax:\ntidyr::pivot_longer(data, cols, names_to, values_to)\nArguments:\n\ndata: A data frame or tibble.\ncols: Columns to pivot into rows.\nnames_to: Name of the new column containing the names of the pivoted variables.\nvalues_to: Name of the new column containing the values of the pivoted variables.\n\nExample:\n\n# Convert iris to long format\niris_long &lt;- iris |&gt;\n  tidyr::pivot_longer(\n    cols = starts_with(\"Sepal\"),\n    names_to = \"Measurement\",\n    values_to = \"Value\"\n  )\nhead(iris_long)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets using dplyr::inner_join().\nSyntax:\ndplyr::inner_join(x, y, by)\nArguments:\n\nx, y: Data frames or tibbles to join.\nby: Column names to join by. Defaults to common column names.\n\nExample:\n\ndf1 &lt;- tibble(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- tibble(ID = 2:4, Score = c(15, 25, 35))\ndf1 |&gt;\n  dplyr::inner_join(df2, by = \"ID\")\n\n\n  \n\n\n\n\n\n\n\n\n\n\nObjective: Prepare mtcars for analysis by filtering rows, creating a new column, and summarizing data.\nSteps:\n\nFilter cars with mpg &gt; 20.\nAdd a column Weight_kg converting wt to kilograms.\nSummarize average MPG by the number of cylinders.\n\nCode:\n\n# Using Base R\nmtcars_filtered &lt;- subset(mtcars, mpg &gt; 20)\nmtcars_filtered &lt;- transform(mtcars_filtered, Weight_kg = wt * 453.6)\naggregate(mpg ~ cyl, data = mtcars_filtered, mean)\n\n\n  \n\n\n# Using tidyverse\nmtcars |&gt;\n  dplyr::filter(mpg &gt; 20) |&gt;\n  dplyr::mutate(Weight_kg = wt * 453.6) |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))\n\n\n  \n\n\n\n\n\n\n\n\n\nBase R: Use tapply() for custom aggregations. tidyverse: Leverage dplyr::group_by() for grouped operations.\nExample:\n\n# Base R\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n# tidyverse\nmtcars |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntidyr::pivot_longer() errors:\n\nEnsure column names in cols exist.\nUse names_pattern for extracting variable parts.\n\nMerge Mismatches in Base R:\n\nVerify columns in by are consistent between datasets.\n\n\n\n\n\n\n\nUse descriptive column names.\nKeep data in tidy format for easier analysis.\nPrefer tidyverse for a consistent and readable workflow.\n\n\n\n\n\n\n\nBase R: Ideal for simple tasks.\ntidyverse: Superior for complex workflows with a consistent syntax.\n\n\n\n\nExplore additional tidyverse packages like lubridate (dates) or stringr (text manipulation).\n\n\n\n\n\nBase R Documentation\ntidyverse Documentation\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#introduction",
    "href": "appendix/r-topics/r-data-transformation.html#introduction",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Data transformation and wrangling involve cleaning, reshaping, and modifying data to prepare it for analysis. This tutorial introduces:\n\nBase R Functions: subset(), transform(), merge().\ntidyverse Functions: dplyr::filter(), dplyr::mutate(), tidyr::pivot_longer(), and others.\n\n\n\n\n\nFiltering rows based on conditions.\nAdding or modifying columns.\nReshaping data between wide and long formats.\nCombining or joining datasets.\n\n\n\n\n\nBase R: Built into R, no additional installations required.\ntidyverse: Consistent syntax and powerful chaining with the pipe operator |&gt;."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#getting-started",
    "href": "appendix/r-topics/r-data-transformation.html#getting-started",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "# Base R requires no additional loading.\n\n# Load tidyverse package using pacman\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#key-functions-and-features",
    "href": "appendix/r-topics/r-data-transformation.html#key-functions-and-features",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Description: Extract rows based on a condition.\nSyntax:\nsubset(x, subset, select = NULL)\nArguments:\n\nx: A data frame or similar object.\nsubset: Logical expression indicating rows to keep.\nselect: Columns to retain, or NULL to keep all.\n\nExample:\n\ndata(iris)\n# Filter rows where Species is \"setosa\"\nsubset(iris, Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Add or modify columns in a data frame.\nSyntax:\ntransform(data, ...)\nArguments:\n\ndata: A data frame.\n…: Expressions specifying transformations.\n\nExample:\n\n# Add a new column for Sepal Area\niris &lt;- transform(iris, Sepal.Area = Sepal.Length * Sepal.Width)\nhead(iris)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets based on common columns (keys).\nSyntax:\nmerge(x, y, by, all)\nArguments:\n\nx, y: Data frames to merge.\nby: Column names to use as keys. Defaults to all common columns.\nall: Logical. TRUE for full outer join; FALSE for inner join.\n\nExample:\n\n# Create example datasets\ndf1 &lt;- data.frame(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- data.frame(ID = 2:4, Score = c(15, 25, 35))\n# Merge on ID\nmerge(df1, df2, by = \"ID\")\n\n\n  \n\n\n\n\n\n\n\n\n\nDescription: Select rows using conditions with dplyr::filter().\nSyntax:\ndplyr::filter(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Logical conditions. Rows where the condition is TRUE are retained.\n\nExample:\n\n# Filter rows where Species is \"setosa\"\niris |&gt;\n  dplyr::filter(Species == \"setosa\")\n\n\n  \n\n\n\n\n\n\nDescription: Create or modify columns using dplyr::mutate().\nSyntax:\ndplyr::mutate(.data, ...)\nArguments:\n\n.data: A data frame or tibble.\n…: Name-value pairs where names are new or existing column names, and values are expressions.\n\nExample:\n\n# Add Sepal.Area column\niris |&gt;\n  dplyr::mutate(Sepal.Area = Sepal.Length * Sepal.Width)\n\n\n  \n\n\n\n\n\n\nDescription: Convert data between wide and long formats with tidyr::pivot_longer().\nSyntax:\ntidyr::pivot_longer(data, cols, names_to, values_to)\nArguments:\n\ndata: A data frame or tibble.\ncols: Columns to pivot into rows.\nnames_to: Name of the new column containing the names of the pivoted variables.\nvalues_to: Name of the new column containing the values of the pivoted variables.\n\nExample:\n\n# Convert iris to long format\niris_long &lt;- iris |&gt;\n  tidyr::pivot_longer(\n    cols = starts_with(\"Sepal\"),\n    names_to = \"Measurement\",\n    values_to = \"Value\"\n  )\nhead(iris_long)\n\n\n  \n\n\n\n\n\n\nDescription: Combine datasets using dplyr::inner_join().\nSyntax:\ndplyr::inner_join(x, y, by)\nArguments:\n\nx, y: Data frames or tibbles to join.\nby: Column names to join by. Defaults to common column names.\n\nExample:\n\ndf1 &lt;- tibble(ID = 1:3, Value = c(10, 20, 30))\ndf2 &lt;- tibble(ID = 2:4, Score = c(15, 25, 35))\ndf1 |&gt;\n  dplyr::inner_join(df2, by = \"ID\")"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#in-depth-examples",
    "href": "appendix/r-topics/r-data-transformation.html#in-depth-examples",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Objective: Prepare mtcars for analysis by filtering rows, creating a new column, and summarizing data.\nSteps:\n\nFilter cars with mpg &gt; 20.\nAdd a column Weight_kg converting wt to kilograms.\nSummarize average MPG by the number of cylinders.\n\nCode:\n\n# Using Base R\nmtcars_filtered &lt;- subset(mtcars, mpg &gt; 20)\nmtcars_filtered &lt;- transform(mtcars_filtered, Weight_kg = wt * 453.6)\naggregate(mpg ~ cyl, data = mtcars_filtered, mean)\n\n\n  \n\n\n# Using tidyverse\nmtcars |&gt;\n  dplyr::filter(mpg &gt; 20) |&gt;\n  dplyr::mutate(Weight_kg = wt * 453.6) |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#advanced-features",
    "href": "appendix/r-topics/r-data-transformation.html#advanced-features",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Base R: Use tapply() for custom aggregations. tidyverse: Leverage dplyr::group_by() for grouped operations.\nExample:\n\n# Base R\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n# tidyverse\nmtcars |&gt;\n  dplyr::group_by(cyl) |&gt;\n  dplyr::summarize(Average_MPG = mean(mpg))"
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-data-transformation.html#troubleshooting-and-faqs",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "tidyr::pivot_longer() errors:\n\nEnsure column names in cols exist.\nUse names_pattern for extracting variable parts.\n\nMerge Mismatches in Base R:\n\nVerify columns in by are consistent between datasets."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#best-practices",
    "href": "appendix/r-topics/r-data-transformation.html#best-practices",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Use descriptive column names.\nKeep data in tidy format for easier analysis.\nPrefer tidyverse for a consistent and readable workflow."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#conclusion",
    "href": "appendix/r-topics/r-data-transformation.html#conclusion",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Base R: Ideal for simple tasks.\ntidyverse: Superior for complex workflows with a consistent syntax.\n\n\n\n\nExplore additional tidyverse packages like lubridate (dates) or stringr (text manipulation)."
  },
  {
    "objectID": "appendix/r-topics/r-data-transformation.html#references-and-resources",
    "href": "appendix/r-topics/r-data-transformation.html#references-and-resources",
    "title": "Data Transformation and Wrangling in R",
    "section": "",
    "text": "Base R Documentation\ntidyverse Documentation\nR for Data Science"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html",
    "href": "appendix/r-topics/r-factors-basics.html",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Factors in R are data structures used to handle categorical data effectively. Factors allow you to store, manipulate, and analyze data with distinct levels, making them essential for statistical modeling and visualization. This tutorial covers functions from the base R package and the forcats package.\n\n\n\n\nBase R Functions: factor(), levels(), nlevels(), as.factor(), is.factor(), relevel()\nForcats Functions: forcats::fct_relevel(), forcats::fct_infreq(), forcats::fct_collapse()\n\n\n\n\n\nRepresenting categorical variables (e.g., gender, regions).\nEncoding ordinal variables (e.g., education levels, survey responses).\nSimplifying group-wise statistical analysis.\nReordering levels for meaningful visualization.\n\n\n\n\n\nEfficient storage of categorical data.\nSeamless integration with statistical models in R.\nImproved clarity and readability of data.\n\n\n\n\n\n\n\nUse pacman::p_load() to load necessary packages:\n\npacman::p_load(forcats, dplyr)\n\n\n\n\nCreate a basic factor and explore its properties:\n\n# Creating a factor\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\n\n# Display the factor\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n# Check the levels\nlevels(colors)\n\n[1] \"blue\"  \"green\" \"red\"  \n\n\n\n\n\n\n\n\n\n\nCreates a factor from a vector of values.\n\n\n\nfactor(x, levels = NULL, labels = NULL, ordered = FALSE)\n\nx: Input vector.\nlevels: Custom levels.\nlabels: Labels for levels.\nordered: Logical flag for ordinal factors.\n\n\n\n\n\ngrades &lt;- factor(c(\"A\", \"B\", \"C\", \"A\", \"B\"), levels = c(\"A\", \"B\", \"C\"), ordered = TRUE)\ngrades\n\n[1] A B C A B\nLevels: A &lt; B &lt; C\n\n\n\n\n\n\n\n\nReorders the levels of a factor to make a specific level the reference level.\n\n\n\nrelevel(factor_variable, ref)\n\nfactor_variable: The factor to be modified.\nref: The new reference level.\n\n\n\n\n\n# Releveling to make \"blue\" the reference level\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\ncolors &lt;- relevel(colors, ref = \"blue\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n\n\n\n\n\n\n\nReorders factor levels.\n\n\n\nforcats::fct_relevel(factor_variable, new_order)\n\n\n\n\neducation &lt;- factor(c(\"High School\", \"Masters\", \"Bachelors\", \"PhD\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] High School Masters     Bachelors   PhD        \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\n\n\n\nReordering education levels for a survey dataset.\n\n\n\nRearrange education levels from lowest to highest for meaningful analysis.\n\n\n\n\neducation &lt;- factor(c(\"Masters\", \"PhD\", \"High School\", \"Bachelors\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nChange the reference level of a factor for a statistical model.\n\n\n\n\n# Relevel \"High School\" as the reference level\neducation &lt;- relevel(education, ref = \"High School\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nSimplify factor levels by collapsing related categories.\n\n\n\n\nfruit &lt;- factor(c(\"apple\", \"banana\", \"pear\", \"apple\", \"orange\", \"pear\"))\nfruit &lt;- forcats::fct_collapse(fruit, Citrus = c(\"orange\"), Other = c(\"apple\", \"banana\", \"pear\"))\nfruit\n\n[1] Other  Other  Other  Other  Citrus Other \nLevels: Other Citrus\n\n\n\n\n\n\n\n\n\nAdjust levels dynamically:\n\nlevels(colors) &lt;- c(levels(colors), \"yellow\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red yellow\n\n\n\n\n\nCombine factors with other R packages:\n\ndata &lt;- tibble(category = factor(c(\"A\", \"B\", \"A\")), value = c(10, 20, 30))\ndata |&gt; group_by(category) |&gt; summarize(mean_value = mean(value))\n\n\n  \n\n\n\n\n\n\n\n\n\n\nUnused Levels: Use droplevels() to remove unused levels.\nIncorrect Order: Use forcats::fct_relevel() to reorder.\n\n\n\n\n\nQ: How to convert a factor back to a character vector?\nA: Use as.character(factor_variable).\nQ: How to check if a variable is a factor?\nA: Use is.factor(variable).\n\n\n\n\n\n\nAlways specify levels for consistency.\nUse ordered factors for ordinal data.\nUse forcats functions for enhanced factor manipulation.\n\n\n\n\n\n\nFactors are powerful tools for handling categorical data in R. They are versatile and integrate well with R’s modeling and visualization ecosystem.\n\n\n\nExplore advanced functionalities in the forcats package, such as forcats::fct_infreq().\n\n\n\n\n\nR Factor Documentation\nForcats Documentation\nTidyverse Guide to Factors"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#introduction",
    "href": "appendix/r-topics/r-factors-basics.html#introduction",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Factors in R are data structures used to handle categorical data effectively. Factors allow you to store, manipulate, and analyze data with distinct levels, making them essential for statistical modeling and visualization. This tutorial covers functions from the base R package and the forcats package.\n\n\n\n\nBase R Functions: factor(), levels(), nlevels(), as.factor(), is.factor(), relevel()\nForcats Functions: forcats::fct_relevel(), forcats::fct_infreq(), forcats::fct_collapse()\n\n\n\n\n\nRepresenting categorical variables (e.g., gender, regions).\nEncoding ordinal variables (e.g., education levels, survey responses).\nSimplifying group-wise statistical analysis.\nReordering levels for meaningful visualization.\n\n\n\n\n\nEfficient storage of categorical data.\nSeamless integration with statistical models in R.\nImproved clarity and readability of data."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#getting-started",
    "href": "appendix/r-topics/r-factors-basics.html#getting-started",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Use pacman::p_load() to load necessary packages:\n\npacman::p_load(forcats, dplyr)\n\n\n\n\nCreate a basic factor and explore its properties:\n\n# Creating a factor\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\n\n# Display the factor\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n# Check the levels\nlevels(colors)\n\n[1] \"blue\"  \"green\" \"red\""
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#key-functions-and-features",
    "href": "appendix/r-topics/r-factors-basics.html#key-functions-and-features",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Creates a factor from a vector of values.\n\n\n\nfactor(x, levels = NULL, labels = NULL, ordered = FALSE)\n\nx: Input vector.\nlevels: Custom levels.\nlabels: Labels for levels.\nordered: Logical flag for ordinal factors.\n\n\n\n\n\ngrades &lt;- factor(c(\"A\", \"B\", \"C\", \"A\", \"B\"), levels = c(\"A\", \"B\", \"C\"), ordered = TRUE)\ngrades\n\n[1] A B C A B\nLevels: A &lt; B &lt; C\n\n\n\n\n\n\n\n\nReorders the levels of a factor to make a specific level the reference level.\n\n\n\nrelevel(factor_variable, ref)\n\nfactor_variable: The factor to be modified.\nref: The new reference level.\n\n\n\n\n\n# Releveling to make \"blue\" the reference level\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\"))\ncolors &lt;- relevel(colors, ref = \"blue\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red\n\n\n\n\n\n\n\n\nReorders factor levels.\n\n\n\nforcats::fct_relevel(factor_variable, new_order)\n\n\n\n\neducation &lt;- factor(c(\"High School\", \"Masters\", \"Bachelors\", \"PhD\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] High School Masters     Bachelors   PhD        \nLevels: High School Bachelors Masters PhD"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#in-depth-examples",
    "href": "appendix/r-topics/r-factors-basics.html#in-depth-examples",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Reordering education levels for a survey dataset.\n\n\n\nRearrange education levels from lowest to highest for meaningful analysis.\n\n\n\n\neducation &lt;- factor(c(\"Masters\", \"PhD\", \"High School\", \"Bachelors\"))\neducation &lt;- forcats::fct_relevel(education, \"High School\", \"Bachelors\", \"Masters\", \"PhD\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nChange the reference level of a factor for a statistical model.\n\n\n\n\n# Relevel \"High School\" as the reference level\neducation &lt;- relevel(education, ref = \"High School\")\neducation\n\n[1] Masters     PhD         High School Bachelors  \nLevels: High School Bachelors Masters PhD\n\n\n\n\n\n\n\n\nSimplify factor levels by collapsing related categories.\n\n\n\n\nfruit &lt;- factor(c(\"apple\", \"banana\", \"pear\", \"apple\", \"orange\", \"pear\"))\nfruit &lt;- forcats::fct_collapse(fruit, Citrus = c(\"orange\"), Other = c(\"apple\", \"banana\", \"pear\"))\nfruit\n\n[1] Other  Other  Other  Other  Citrus Other \nLevels: Other Citrus"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#advanced-features",
    "href": "appendix/r-topics/r-factors-basics.html#advanced-features",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Adjust levels dynamically:\n\nlevels(colors) &lt;- c(levels(colors), \"yellow\")\ncolors\n\n[1] red   blue  green red   blue \nLevels: blue green red yellow\n\n\n\n\n\nCombine factors with other R packages:\n\ndata &lt;- tibble(category = factor(c(\"A\", \"B\", \"A\")), value = c(10, 20, 30))\ndata |&gt; group_by(category) |&gt; summarize(mean_value = mean(value))"
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#troubleshooting-and-faqs",
    "href": "appendix/r-topics/r-factors-basics.html#troubleshooting-and-faqs",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Unused Levels: Use droplevels() to remove unused levels.\nIncorrect Order: Use forcats::fct_relevel() to reorder.\n\n\n\n\n\nQ: How to convert a factor back to a character vector?\nA: Use as.character(factor_variable).\nQ: How to check if a variable is a factor?\nA: Use is.factor(variable)."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#best-practices",
    "href": "appendix/r-topics/r-factors-basics.html#best-practices",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Always specify levels for consistency.\nUse ordered factors for ordinal data.\nUse forcats functions for enhanced factor manipulation."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#conclusion",
    "href": "appendix/r-topics/r-factors-basics.html#conclusion",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "Factors are powerful tools for handling categorical data in R. They are versatile and integrate well with R’s modeling and visualization ecosystem.\n\n\n\nExplore advanced functionalities in the forcats package, such as forcats::fct_infreq()."
  },
  {
    "objectID": "appendix/r-topics/r-factors-basics.html#references-and-resources",
    "href": "appendix/r-topics/r-factors-basics.html#references-and-resources",
    "title": "Factors and Categorical Variables in R",
    "section": "",
    "text": "R Factor Documentation\nForcats Documentation\nTidyverse Guide to Factors"
  },
  {
    "objectID": "appendix/r-packages/ggplot2.html",
    "href": "appendix/r-packages/ggplot2.html",
    "title": "ggplot2: A Comprehensive Guide",
    "section": "",
    "text": "Introduction\nOverview The ggplot2 package (part of the Tidyverse) provides a powerful system for creating graphics in R by implementing the Grammar of Graphics. It uses layers to gradually build complex visualizations from simple components. Packages and Key Functions:\n\nggplot2 (key functions include ggplot(), aes(), geom_point(), geom_bar(), geom_line(), geom_histogram(), facet_wrap(), theme(), etc.)\n\nUse Cases\n\nExploratory data analysis: Quick visualization of relationships in data.\nPublication-ready plots: Highly customizable appearance and layout.\nInteractive or iterative workflows: Build complex visuals step-by-step.\n\nBenefits\n\nLayered approach allows modular building of plots.\nConsistent grammar for different plot types.\nPowerful customizations (labels, scales, themes, etc.).\n\nggplot2::ggplot(data, aes(x, y)) + geom_*()\n(The skeleton structure above demonstrates how layers are added; it is not meant to run by itself.)\nBelow is a schematic (using Mermaid) illustrating the layered approach in ggplot2:\n\ngraph LR\n    A[\"Begin with ggplot()\"] --&gt; B[\"Add aes() layer\"]\n    B --&gt; C[\"Add geom_*() layer\"]\n    C --&gt; D[\"Optional: Add facet or theme layers\"]\n\n\n\n\ngraph LR\n    A[\"Begin with ggplot()\"] --&gt; B[\"Add aes() layer\"]\n    B --&gt; C[\"Add geom_*() layer\"]\n    C --&gt; D[\"Optional: Add facet or theme layers\"]\n\n\n\n\n\n\n\n\nGetting Started\nLoading the Package You can load ggplot2 with pacman::p_load():\n\npacman::p_load(ggplot2)\n\nBasic Usage Below is a simple example to illustrate the core functionality of ggplot2. We will use the built-in mtcars dataset to create a scatter plot of mpg (miles per gallon) vs. hp (horsepower).\n\n# Scatter plot of mpg vs. hp\nmtcars |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = hp, y = mpg)) +\n    ggplot2::geom_point() +\n    ggplot2::labs(\n        title = \"Basic Scatter Plot\",\n        x = \"Horsepower\",\n        y = \"Miles Per Gallon\"\n    )\n\n\n\n\n\n\n\n\nIn this code:\n\nmtcars is piped into ggplot2::ggplot().\nggplot2::aes() maps hp to the x-axis and mpg to the y-axis.\nggplot2::geom_point() adds the point geometry to create a scatter plot.\nggplot2::labs() customizes labels.\n\n\n\nKey Functions and Features\nggplot2::ggplot() Description\n\nThe main function to initialize a ggplot object, where you specify the dataset and general aesthetic mappings.\n\nSyntax\nggplot2::ggplot(data, ggplot2::aes(...))\n\ndata: A data frame.\nggplot2::aes(…): Aesthetic mappings for x, y, color, size, etc.\n\nExample\n\n# Basic template (with mpg dataset from ggplot2)\nggplot2::ggplot(data = ggplot2::mpg, ggplot2::aes(x = displ, y = hwy)) +\n    ggplot2::geom_point()\n\n\n\n\n\n\n\n\nggplot2::aes() Description\n\nDefines the mapping of variables to visual properties (aesthetics) such as x, y, color, fill, size, shape, and more.\n\nSyntax\nggplot2::aes(x, y, color = ..., fill = ..., shape = ...)\nExample\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = cyl, y = hwy, color = drv)) +\n    ggplot2::geom_point()\n\n\n\n\n\n\n\n\n\nHere, drv is mapped to color, creating different point colors by drive type.\n\nggplot2::geom_*() Description\n\nA family of geometric functions to specify how data points are represented (points, lines, bars, etc.).\n\nSyntax\nggplot2::geom_point(...)\nggplot2::geom_line(...)\nggplot2::geom_bar(...)\nggplot2::geom_histogram(...)\nArguments often include statistical transformations, bin widths, position adjustments, etc.\nExample: Bar Plot\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = class)) +\n    ggplot2::geom_bar()\n\n\n\n\n\n\n\n\n\nDisplays the count of each vehicle class in the mpg dataset.\n\nggplot2::facet_wrap() Description\n\nSplits data into subplots (facets) based on one or more categorical variables.\n\nSyntax\nggplot2::facet_wrap(~ variable, ncol = ...)\nExample\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = displ, y = hwy)) +\n    ggplot2::geom_point() +\n    ggplot2::facet_wrap(~drv)\n\n\n\n\n\n\n\n\n\nCreates separate panels for each drive type (drv).\n\nggplot2::theme() Description\n\nAdjusts non-data components (text size, legend placement, background, etc.).\n\nSyntax\nggplot2::theme(\n  panel.background = element_rect(...),\n  axis.text.x = element_text(...),\n  ...\n)\nExample\n\nggplot2::ggplot(ggplot2::mpg, ggplot2::aes(x = displ, y = hwy)) +\n    ggplot2::geom_point() +\n    ggplot2::theme_minimal()\n\n\n\n\n\n\n\n\n\nApplies a minimal theme to reduce chart ink and highlight data.\n\n\n\nIn-Depth Examples\nTitle: Comparing Engine Displacement and Fuel Efficiency Objective\n\nVisualize relationships between engine displacement (displ) and highway fuel efficiency (hwy) for different car classes.\n\nSteps\n\nData Preparation We will use the built-in mpg dataset and select relevant columns: displ, hwy, class.\nFunction Application\n\nInitialize the plot with ggplot2::ggplot().\nMap displ and hwy using ggplot2::aes().\nAdd geom_point().\nFacet the plot by car class using ggplot2::facet_wrap().\nAdd a custom theme.\n\nVisualization\n\n\n# Step-by-step example\nfiltered_mpg &lt;- ggplot2::mpg\n\nfiltered_mpg |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = displ, y = hwy, color = class)) +\n    ggplot2::geom_point(size = 3) +\n    ggplot2::facet_wrap(~class) +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Engine Displacement vs. Highway MPG by Class\",\n        subtitle = \"Faceted by Vehicle Class\",\n        x = \"Engine Displacement (Liters)\",\n        y = \"Highway Miles per Gallon\",\n        color = \"Vehicle Class\"\n    )\n\n\n\n\n\n\n\n\nOutput\n\nYou will see a facetted scatter plot showing how different classes compare in the displ-hwy relationship. The color indicates each class, and each facet displays data for one class.\n\n\n\nAdvanced Features\nCustomization\n\nModify legends, labels, and scales with functions like ggplot2::labs(), ggplot2::scale_color_manual(), or ggplot2::theme().\nAdjust axis transformations (e.g., log scale) with ggplot2::scale_x_log10() or ggplot2::scale_y_log10().\n\nIntegration\n\nCombine ggplot2 with dplyr for data wrangling:\n\n\nlibrary(dplyr)\n\nggplot2::mpg |&gt;\n    dplyr::filter(displ &lt; 5) |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = displ, y = cty)) +\n    ggplot2::geom_point() +\n    ggplot2::geom_smooth(method = \"lm\") +\n    ggplot2::theme_minimal()\n\n\n\n\n\n\n\n\n\nThis uses dplyr::filter() to remove rows with displacement &gt;= 5, then plots with ggplot2.\n\nOptimization\n\nFor large datasets, consider sampling or binning (geom_bin2d, geom_hex from other packages).\nUse efficient data handling (data.table or arrow) before feeding data to ggplot2 for plotting.\nLimit layering of overly detailed geoms when performance is a concern.\n\n\n\nTroubleshooting and FAQs\nCommon Issues\n\n“Error in ggplot(…): object not found”: Ensure the dataset or column name matches correctly and is in scope.\n“Discrete value supplied to continuous scale”: A variable mapped to x or y might be character instead of numeric; convert if necessary.\nMissing + sign between layers: Each layer is added with the + operator; forgetting it can break the chain of commands.\n\nFAQs\n\nHow do I rotate x-axis labels?\n\nUse ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, hjust = 1)).\n\nCan I save my plots?\n\nYes, with ggplot2::ggsave(“plot.png”, width = …, height = …).\n\nHow do I create interactive plots?\n\nUse packages like plotly or ggiraph that build on ggplot2.\n\n\n\n\nBest Practices\n\nKeep code modular: Prepare and clean data first, then pipe into ggplot2 functions.\nUse meaningful variable and aesthetic mappings for readability.\nWrite reproducible scripts by incorporating set.seed() for sampling or random processes.\nWhen presenting results (including power analysis in broader data workflows), ensure the data, code, and figure output are clearly documented.\n\n\n\nConclusion\nSummary We have explored the ggplot2 package’s layered approach to building plots, introduced key functions, and provided examples for customizing, faceting, and integrating with other data pipelines.\nNext Steps\n\nExperiment with specialized geoms (e.g., boxplots, violin plots).\nExplore advanced theming options (e.g., ggthemes package).\nInvestigate extension packages like gganimate for animated graphics or patchwork for arranging multiple ggplots.\n\n\n\nReferences and Resources\nDocumentation\n\nggplot2 Official Documentation\n\nLearning Materials\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nOnline tutorials on the RStudio Community\n\nSupport Channels\n\nGitHub Issues for ggplot2\nStack Overflow (r + ggplot2 tag)\nLocal R User Groups or R-Ladies meetups for community support"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html",
    "href": "appendix/r-packages/emmeans-v1.html",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "emmeans (Estimated Marginal Means) is an R package designed for the analysis of linear models. It provides tools for obtaining and visualizing adjusted means for factors in statistical models, allowing users to interpret complex model outputs more easily.\n\n\n\n\nPost-hoc Comparisons: Evaluating differences between group means after fitting a model.\nVisualization: Creating clear and informative plots of marginal means.\nModel Checking: Assessing the fit and assumptions of linear models.\n\n\n\n\n\nSimplifies the interpretation of model outputs.\nFacilitates the comparison of means across different factors.\nIntegrates seamlessly with various types of models in R.\n\n\n\n\n\n\n\nFirst, load the emmeans package into your R session:\n\nlibrary(emmeans)\n\n\n\n\nHere’s a simple example using a linear model:\n\n# Sample data\ndata(mtcars)\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n# Obtain estimated marginal means\nemm &lt;- emmeans(model, ~cyl)\nsummary(emm)\n\n\n  \n\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n\nAnnotation: This will provide the estimated marginal means of miles per gallon (mpg) adjusted for the number of cylinders (cyl).\n\n\n\n\n\n\n\nDescription: Computes estimated marginal means for specified factors in a model.\nSyntax:\n\nemmeans(object, specs, ...)\n\nobject: A fitted model object.\nspecs: Terms for which to compute marginal means.\nExample:\n\n\n# Example of emmeans\nemm_cyl &lt;- emmeans(model, ~cyl)\nprint(emm_cyl)\n\n cyl emmean    SE df lower.CL upper.CL\n   4   26.7 0.972 29     24.7     28.7\n   6   19.7 1.220 29     17.3     22.2\n   8   15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n\nThis will display the estimated marginal means for each cylinder category.\n\n\n\n\nDescription: Computes pairwise comparisons among estimated marginal means.\nSyntax:\npairs(x, ...)\n\nx: An object of class emmeans.\n\nExample:\n\n\n# Pairwise comparisons\npairs(emm_cyl)\n\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  &lt;.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThis gives pairwise comparisons between the cylinder levels.\n\n\n\n\n\n\n\n\nTo evaluate how the number of cylinders affects the miles per gallon (mpg) in cars.\n\n\n\n\nData Preparation:\n\n\ndata(mtcars)\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n\nFunction Application:\n\n\nemm_cyl &lt;- emmeans(model, ~cyl)\n\n\nVisualization:\n\n\nplot(emm_cyl)\n\n\n\n\n\n\n\n\n\nOutput: The plot will show the estimated marginal means of mpg for each cylinder group, allowing for quick visual comparisons.\n\n\n\n\n\n\n\n\nYou can customize the output of emmeans using additional arguments:\n\nemm_cyl &lt;- emmeans(model, ~cyl, at = list(cyl = c(4, 6, 8)))\n\n\n\n\nemmeans can be used with packages like ggplot2 for enhanced visualizations:\n\nlibrary(ggplot2)\nemm_df &lt;- as.data.frame(emm_cyl)\nggplot(emm_df, aes(x = cyl, y = emmean)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError: Object not found: Ensure the model object exists and is correctly specified.\nWarnings about convergence: Check the model fitting process for issues related to data or model complexity.\n\n\n\n\n\nQ: Can emmeans be used with non-linear models?\n\nA: Yes, emmeans supports various model types, including generalized linear models.\n\n\n\n\n\n\n\nUse clear and descriptive variable names when fitting models.\nRegularly check model diagnostics to ensure assumptions are met.\nDocument your analysis steps for reproducibility.\n\n\n\n\n\n\nIn this tutorial, we explored the emmeans package, covering its key functions, in-depth examples, and advanced features.\n\n\n\nTry applying emmeans to your own datasets or explore more advanced statistical models.\n\n\n\nDive into your data analysis projects with confidence, utilizing the power of emmeans!\n\n\n\n\n\nDocumentation: emmeans Documentation\nLearning Materials: R for Data Science\nSupport Channels: GitHub Issues, Stack Overflow\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];\n\n\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#introduction",
    "href": "appendix/r-packages/emmeans-v1.html#introduction",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "emmeans (Estimated Marginal Means) is an R package designed for the analysis of linear models. It provides tools for obtaining and visualizing adjusted means for factors in statistical models, allowing users to interpret complex model outputs more easily.\n\n\n\n\nPost-hoc Comparisons: Evaluating differences between group means after fitting a model.\nVisualization: Creating clear and informative plots of marginal means.\nModel Checking: Assessing the fit and assumptions of linear models.\n\n\n\n\n\nSimplifies the interpretation of model outputs.\nFacilitates the comparison of means across different factors.\nIntegrates seamlessly with various types of models in R."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#getting-started",
    "href": "appendix/r-packages/emmeans-v1.html#getting-started",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "First, load the emmeans package into your R session:\n\nlibrary(emmeans)\n\n\n\n\nHere’s a simple example using a linear model:\n\n# Sample data\ndata(mtcars)\n\n# Fit a linear model\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n# Obtain estimated marginal means\nemm &lt;- emmeans(model, ~cyl)\nsummary(emm)\n\n\n  \n\n\ntapply(mtcars$mpg, mtcars$cyl, mean)\n\n       4        6        8 \n26.66364 19.74286 15.10000 \n\n\nAnnotation: This will provide the estimated marginal means of miles per gallon (mpg) adjusted for the number of cylinders (cyl)."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#key-functions-and-features",
    "href": "appendix/r-packages/emmeans-v1.html#key-functions-and-features",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Description: Computes estimated marginal means for specified factors in a model.\nSyntax:\n\nemmeans(object, specs, ...)\n\nobject: A fitted model object.\nspecs: Terms for which to compute marginal means.\nExample:\n\n\n# Example of emmeans\nemm_cyl &lt;- emmeans(model, ~cyl)\nprint(emm_cyl)\n\n cyl emmean    SE df lower.CL upper.CL\n   4   26.7 0.972 29     24.7     28.7\n   6   19.7 1.220 29     17.3     22.2\n   8   15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n\nThis will display the estimated marginal means for each cylinder category.\n\n\n\n\nDescription: Computes pairwise comparisons among estimated marginal means.\nSyntax:\npairs(x, ...)\n\nx: An object of class emmeans.\n\nExample:\n\n\n# Pairwise comparisons\npairs(emm_cyl)\n\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  &lt;.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThis gives pairwise comparisons between the cylinder levels."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#in-depth-examples",
    "href": "appendix/r-packages/emmeans-v1.html#in-depth-examples",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "To evaluate how the number of cylinders affects the miles per gallon (mpg) in cars.\n\n\n\n\nData Preparation:\n\n\ndata(mtcars)\nmodel &lt;- lm(mpg ~ factor(cyl), data = mtcars)\n\n\nFunction Application:\n\n\nemm_cyl &lt;- emmeans(model, ~cyl)\n\n\nVisualization:\n\n\nplot(emm_cyl)\n\n\n\n\n\n\n\n\n\nOutput: The plot will show the estimated marginal means of mpg for each cylinder group, allowing for quick visual comparisons."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#advanced-features",
    "href": "appendix/r-packages/emmeans-v1.html#advanced-features",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "You can customize the output of emmeans using additional arguments:\n\nemm_cyl &lt;- emmeans(model, ~cyl, at = list(cyl = c(4, 6, 8)))\n\n\n\n\nemmeans can be used with packages like ggplot2 for enhanced visualizations:\n\nlibrary(ggplot2)\nemm_df &lt;- as.data.frame(emm_cyl)\nggplot(emm_df, aes(x = cyl, y = emmean)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL))"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/emmeans-v1.html#troubleshooting-and-faqs",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Error: Object not found: Ensure the model object exists and is correctly specified.\nWarnings about convergence: Check the model fitting process for issues related to data or model complexity.\n\n\n\n\n\nQ: Can emmeans be used with non-linear models?\n\nA: Yes, emmeans supports various model types, including generalized linear models."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#best-practices",
    "href": "appendix/r-packages/emmeans-v1.html#best-practices",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Use clear and descriptive variable names when fitting models.\nRegularly check model diagnostics to ensure assumptions are met.\nDocument your analysis steps for reproducibility."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "In this tutorial, we explored the emmeans package, covering its key functions, in-depth examples, and advanced features.\n\n\n\nTry applying emmeans to your own datasets or explore more advanced statistical models.\n\n\n\nDive into your data analysis projects with confidence, utilizing the power of emmeans!"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#references-and-resources",
    "href": "appendix/r-packages/emmeans-v1.html#references-and-resources",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "",
    "text": "Documentation: emmeans Documentation\nLearning Materials: R for Data Science\nSupport Channels: GitHub Issues, Stack Overflow\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];\n\n\n\n\ngraph TD;\n    A[Start] --&gt; B[Load Package];\n    B --&gt; C[Fit Model];\n    C --&gt; D[Compute EMM];\n    D --&gt; E[Visualize Results];\n    E --&gt; F[Interpret Output];\n    F --&gt; G[Explore Further];\n    G --&gt; H[End];"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#understanding-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#understanding-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "1. Understanding Interactions",
    "text": "1. Understanding Interactions\nInteractions occur when the effect of one predictor variable on the response variable depends on the level of another predictor. For example, the relationship between treatment and outcome may vary depending on gender."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Fitting a Model with Interactions",
    "text": "2. Fitting a Model with Interactions\n\nExample\nLet’s create a model with an interaction between two factors, treatment and gender, on the response variable outcome.\n\n# Sample data\ndata(mtcars)\n\n# Create a categorical variable for the example\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\"))\n\n# Fit a linear model with interaction\nmodel &lt;- lm(mpg ~ am * cyl, data = mtcars)"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Using emmeans for Interactions",
    "text": "3. Using emmeans for Interactions\n\nCompute Estimated Marginal Means\nTo compute estimated marginal means for the interaction, use the emmeans() function with the interaction term specified.\n\n# Obtain estimated marginal means for the interaction\nemm_interaction &lt;- emmeans(model, ~ am * cyl)\nsummary(emm_interaction)\n\n\n  \n\n\n\n\n\nInterpret the Output\nThe output will give you the estimated marginal means for each combination of the levels of am and cyl. This allows for examining how the means differ based on the interaction."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#visualizing-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#visualizing-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Visualizing Interactions",
    "text": "4. Visualizing Interactions\nVisualizing interactions can help in understanding how the effects vary. Use the plot() function to create interaction plots.\n\n# Interaction plot\nplot(emm_interaction)\n\n\n\n\n\n\n\n\n\nCustomizing the Plot\nYou can further customize the plot using ggplot2 for enhanced visual representation:\n\nlibrary(ggplot2)\n\n# Convert emmeans object to a data frame\nemm_df &lt;- as.data.frame(emm_interaction)\n\n# Create a ggplot\nggplot(emm_df, aes(x = cyl, y = emmean, color = am)) +\n    geom_point() +\n    geom_line() +\n    labs(\n        title = \"Interaction between Treatment and Cylinder\",\n        x = \"Number of Cylinders\",\n        y = \"Estimated Marginal Mean MPG\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons",
    "href": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Pairwise Comparisons",
    "text": "5. Pairwise Comparisons\nYou can also perform pairwise comparisons for the interaction effects:\n\n# Pairwise comparisons for the interaction\npairs(emm_interaction)\n\n contrast                               estimate   SE df t.ratio p.value\n Automatic cyl6.1875 - Manual cyl6.1875     -2.1 1.27 28  -1.658  0.1085\n\n\nThis will give you insights into which combinations of am and cyl are significantly different from one another."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion-1",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Conclusion",
    "text": "Conclusion\nHandling interactions in emmeans is straightforward and provides valuable insights into how different factors influence the response variable. By fitting models that include interactions, computing estimated marginal means, and visualizing the results, you can effectively interpret complex relationships in your data."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "1. Understanding Multi-Factor Interactions",
    "text": "1. Understanding Multi-Factor Interactions\nMulti-factor interactions occur when the effect of one factor on the response variable depends on the levels of two or more other factors. For example, you may want to investigate how the effect of treatment varies across different levels of gender and age group."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Fitting a Model with Multiple Interactions",
    "text": "2. Fitting a Model with Multiple Interactions\n\nExample\nLet’s create a hypothetical dataset and fit a model that includes interactions among three factors: treatment, gender, and age_group.\n\n# Load necessary library\nlibrary(dplyr)\n\n# Simulated dataset\nset.seed(123)\ndata &lt;- data.frame(\n    treatment = factor(rep(c(\"A\", \"B\"), each = 30)),\n    gender = factor(rep(c(\"Male\", \"Female\"), times = 30)),\n    age_group = factor(rep(c(\"Young\", \"Old\"), each = 15, times = 2)),\n    outcome = rnorm(60, 50, 10)\n)\n\n# Fit a linear model with interactions\nmodel &lt;- lm(outcome ~ treatment * gender * age_group, data = data)"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Using emmeans for Multi-Factor Interactions",
    "text": "3. Using emmeans for Multi-Factor Interactions\n\nCompute Estimated Marginal Means\nTo compute estimated marginal means for the interaction among the three factors, specify the interaction in the emmeans() function.\n\n# Obtain estimated marginal means for the interaction\nemm_multi &lt;- emmeans(model, ~ treatment * gender * age_group)\nsummary(emm_multi)\n\n\n  \n\n\n\n\n\nInterpret the Output\nThe output will display the estimated marginal means for each combination of the levels of treatment, gender, and age_group. This helps you understand how the outcome varies across these groups."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions",
    "href": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Visualizing Multi-Factor Interactions",
    "text": "4. Visualizing Multi-Factor Interactions\nVisualizing interactions with multiple factors can be complex, but it can be done effectively using ggplot2. Here’s how to create a plot:\n\nlibrary(ggplot2)\n\n# Convert emmeans object to a data frame\nemm_df &lt;- as.data.frame(emm_multi)\n\n# Create a ggplot\nggplot(emm_df, aes(x = treatment, y = emmean, group = interaction(gender, age_group), color = interaction(gender, age_group))) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5)) +\n    labs(\n        title = \"Multi-Factor Interaction\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCustomizing the Plot\nYou can adjust the aesthetics of the plot to improve clarity, such as adding facet_wrap() to separate plots by one of the factors:\n\nggplot(emm_df, aes(x = treatment, y = emmean, color = gender)) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5), aes(group = age_group)) +\n    facet_wrap(~age_group) +\n    labs(\n        title = \"Multi-Factor Interaction by Age Group\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-1",
    "href": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Pairwise Comparisons",
    "text": "5. Pairwise Comparisons\nYou can perform pairwise comparisons for the multi-factor interactions as well:\n\n# Pairwise comparisons for the multi-factor interaction\npairs(emm_multi)\n\n contrast                        estimate   SE df t.ratio p.value\n A Female Old - B Female Old      -4.8285 4.68 52  -1.031  0.9674\n A Female Old - A Male Old         0.2514 4.85 52   0.052  1.0000\n A Female Old - B Male Old        -0.8298 4.85 52  -0.171  1.0000\n A Female Old - A Female Young    -2.7988 4.85 52  -0.577  0.9990\n A Female Old - B Female Young    -6.3348 4.85 52  -1.306  0.8923\n A Female Old - A Male Young      -4.8118 4.68 52  -1.027  0.9681\n A Female Old - B Male Young      -4.3973 4.68 52  -0.939  0.9806\n B Female Old - A Male Old         5.0800 4.85 52   1.048  0.9645\n B Female Old - B Male Old         3.9987 4.85 52   0.825  0.9909\n B Female Old - A Female Young     2.0297 4.85 52   0.419  0.9999\n B Female Old - B Female Young    -1.5063 4.85 52  -0.311  1.0000\n B Female Old - A Male Young       0.0167 4.68 52   0.004  1.0000\n B Female Old - B Male Young       0.4312 4.68 52   0.092  1.0000\n A Male Old - B Male Old          -1.0812 5.01 52  -0.216  1.0000\n A Male Old - A Female Young      -3.0503 5.01 52  -0.609  0.9986\n A Male Old - B Female Young      -6.5862 5.01 52  -1.315  0.8889\n A Male Old - A Male Young        -5.0633 4.85 52  -1.044  0.9651\n A Male Old - B Male Young        -4.6487 4.85 52  -0.959  0.9782\n B Male Old - A Female Young      -1.9690 5.01 52  -0.393  0.9999\n B Male Old - B Female Young      -5.5050 5.01 52  -1.099  0.9541\n B Male Old - A Male Young        -3.9820 4.85 52  -0.821  0.9911\n B Male Old - B Male Young        -3.5675 4.85 52  -0.736  0.9954\n A Female Young - B Female Young  -3.5360 5.01 52  -0.706  0.9965\n A Female Young - A Male Young    -2.0130 4.85 52  -0.415  0.9999\n A Female Young - B Male Young    -1.5985 4.85 52  -0.330  1.0000\n B Female Young - A Male Young     1.5230 4.85 52   0.314  1.0000\n B Female Young - B Male Young     1.9375 4.85 52   0.400  0.9999\n A Male Young - B Male Young       0.4145 4.68 52   0.088  1.0000\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\nThis will provide insights into significant differences among the combinations of the three factors."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion-2",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion-2",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Conclusion",
    "text": "Conclusion\nHandling interactions with more than two factors in emmeans allows you to explore complex relationships in your data effectively. By fitting models that include multiple interactions, computing estimated marginal means, and visualizing the results, you can gain a deeper understanding of how different factors influence the response variable."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#understanding-multi-factor-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "1. Understanding Multi-Factor Interactions",
    "text": "1. Understanding Multi-Factor Interactions\nMulti-factor interactions occur when the effect of one factor on the response variable depends on the levels of two or more other factors. For example, you may want to investigate how the effect of treatment varies across different levels of gender and age group."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#fitting-a-model-with-multiple-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "2. Fitting a Model with Multiple Interactions",
    "text": "2. Fitting a Model with Multiple Interactions\n\nExample\nLet’s create a hypothetical dataset and fit a model that includes interactions among three factors: treatment, gender, and age_group.\n\n# Load necessary library\nlibrary(dplyr)\n\n# Simulated dataset\nset.seed(123)\ndata &lt;- data.frame(\n    treatment = factor(rep(c(\"A\", \"B\"), each = 30)),\n    gender = factor(rep(c(\"Male\", \"Female\"), times = 30)),\n    age_group = factor(rep(c(\"Young\", \"Old\"), each = 15, times = 2)),\n    outcome = rnorm(60, 50, 10)\n)\n\n# Fit a linear model with interactions\nmodel &lt;- lm(outcome ~ treatment * gender * age_group, data = data)"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#using-emmeans-for-multi-factor-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "3. Using emmeans for Multi-Factor Interactions",
    "text": "3. Using emmeans for Multi-Factor Interactions\n\nCompute Estimated Marginal Means\nTo compute estimated marginal means for the interaction among the three factors, specify the interaction in the emmeans() function.\n\n# Obtain estimated marginal means for the interaction\nemm_multi &lt;- emmeans(model, ~ treatment * gender * age_group)\nsummary(emm_multi)\n\n\n  \n\n\n\n\n\nInterpret the Output\nThe output will display the estimated marginal means for each combination of the levels of treatment, gender, and age_group. This helps you understand how the outcome varies across these groups."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions-1",
    "href": "appendix/r-packages/emmeans-v1.html#visualizing-multi-factor-interactions-1",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "4. Visualizing Multi-Factor Interactions",
    "text": "4. Visualizing Multi-Factor Interactions\nVisualizing interactions with multiple factors can be complex, but it can be done effectively using ggplot2. Here’s how to create a plot:\n\nlibrary(ggplot2)\n\n# Convert emmeans object to a data frame\nemm_df &lt;- as.data.frame(emm_multi)\n\n# Create a ggplot\nggplot(emm_df, aes(x = treatment, y = emmean, group = interaction(gender, age_group), color = interaction(gender, age_group))) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5)) +\n    labs(\n        title = \"Multi-Factor Interaction\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nCustomizing the Plot\nYou can adjust the aesthetics of the plot to improve clarity, such as adding facet_wrap() to separate plots by one of the factors:\n\nggplot(emm_df, aes(x = treatment, y = emmean, color = gender)) +\n    geom_point(position = position_dodge(width = 0.5)) +\n    geom_line(position = position_dodge(width = 0.5), aes(group = age_group)) +\n    facet_wrap(~age_group) +\n    labs(\n        title = \"Multi-Factor Interaction by Age Group\",\n        x = \"Treatment\",\n        y = \"Estimated Marginal Mean Outcome\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-2",
    "href": "appendix/r-packages/emmeans-v1.html#pairwise-comparisons-2",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "5. Pairwise Comparisons",
    "text": "5. Pairwise Comparisons\nYou can perform pairwise comparisons for the multi-factor interactions as well:\n\n# Pairwise comparisons for the multi-factor interaction\npairs(emm_multi)\n\n contrast                        estimate   SE df t.ratio p.value\n A Female Old - B Female Old      -4.8285 4.68 52  -1.031  0.9674\n A Female Old - A Male Old         0.2514 4.85 52   0.052  1.0000\n A Female Old - B Male Old        -0.8298 4.85 52  -0.171  1.0000\n A Female Old - A Female Young    -2.7988 4.85 52  -0.577  0.9990\n A Female Old - B Female Young    -6.3348 4.85 52  -1.306  0.8923\n A Female Old - A Male Young      -4.8118 4.68 52  -1.027  0.9681\n A Female Old - B Male Young      -4.3973 4.68 52  -0.939  0.9806\n B Female Old - A Male Old         5.0800 4.85 52   1.048  0.9645\n B Female Old - B Male Old         3.9987 4.85 52   0.825  0.9909\n B Female Old - A Female Young     2.0297 4.85 52   0.419  0.9999\n B Female Old - B Female Young    -1.5063 4.85 52  -0.311  1.0000\n B Female Old - A Male Young       0.0167 4.68 52   0.004  1.0000\n B Female Old - B Male Young       0.4312 4.68 52   0.092  1.0000\n A Male Old - B Male Old          -1.0812 5.01 52  -0.216  1.0000\n A Male Old - A Female Young      -3.0503 5.01 52  -0.609  0.9986\n A Male Old - B Female Young      -6.5862 5.01 52  -1.315  0.8889\n A Male Old - A Male Young        -5.0633 4.85 52  -1.044  0.9651\n A Male Old - B Male Young        -4.6487 4.85 52  -0.959  0.9782\n B Male Old - A Female Young      -1.9690 5.01 52  -0.393  0.9999\n B Male Old - B Female Young      -5.5050 5.01 52  -1.099  0.9541\n B Male Old - A Male Young        -3.9820 4.85 52  -0.821  0.9911\n B Male Old - B Male Young        -3.5675 4.85 52  -0.736  0.9954\n A Female Young - B Female Young  -3.5360 5.01 52  -0.706  0.9965\n A Female Young - A Male Young    -2.0130 4.85 52  -0.415  0.9999\n A Female Young - B Male Young    -1.5985 4.85 52  -0.330  1.0000\n B Female Young - A Male Young     1.5230 4.85 52   0.314  1.0000\n B Female Young - B Male Young     1.9375 4.85 52   0.400  0.9999\n A Male Young - B Male Young       0.4145 4.68 52   0.088  1.0000\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\nThis will provide insights into significant differences among the combinations of the three factors."
  },
  {
    "objectID": "appendix/r-packages/emmeans-v1.html#conclusion-3",
    "href": "appendix/r-packages/emmeans-v1.html#conclusion-3",
    "title": "Tutorial: Using the emmeans R Package",
    "section": "Conclusion",
    "text": "Conclusion\nHandling interactions with more than two factors in emmeans allows you to explore complex relationships in your data effectively. By fitting models that include multiple interactions, computing estimated marginal means, and visualizing the results, you can gain a deeper understanding of how different factors influence the response variable."
  },
  {
    "objectID": "appendix/r-packages/pwr.html",
    "href": "appendix/r-packages/pwr.html",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package in R is a powerful tool for conducting power analysis. Power analysis is essential in determining the sample size needed to detect an effect of a given size with a certain level of confidence. It is widely used in experimental design and statistical hypothesis testing.\n\n\n\n\nDetermining the required sample size for an experiment.\nCalculating the power of a test given sample size and effect size.\nComparing the sensitivity of different statistical tests.\n\n\n\n\n\nSimplifies power analysis for various statistical tests.\nProvides built-in functions for common tests such as t-tests, ANOVA, and correlation.\nEasy integration with R workflows for reproducibility.\n\n\n\n\n\n\n\n\npacman::p_load(pwr)\n\n\n\n\nHere is a simple example of using pwr.t.test to calculate the sample size needed for a one-sample t-test:\n\n# Calculate required sample size for a one-sample t-test\nresult &lt;- pwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, type = \"one.sample\")\nprint(result)\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThis calculates the required sample size to achieve a power of 0.8 for detecting an effect size of 0.5 at a significance level of 0.05.\n\n\n\n\n\n\n\n\nCalculates power, effect size, or sample size for t-tests.\n\n\n\npwr.t.test(n = NULL, d = NULL, sig.level = NULL, power = NULL, type = \"two.sample\", alternative = \"two.sided\")\n\nn: Sample size.\nd: Effect size.\nsig.level: Significance level (default is 0.05).\npower: Desired power (1 - Type II error probability).\ntype: Type of t-test (\"one.sample\", \"two.sample\", or \"paired\").\nalternative: Specifies whether the test is \"two.sided\" or \"greater\"/\"less\".\n\n\n\n\n\n# Power analysis for a two-sample t-test\npwr.t.test(n = 30, d = 0.5, sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.5\n      sig.level = 0.05\n          power = 0.4778965\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\nPerforms power analysis for balanced one-way ANOVA.\n\n\n\npwr.anova.test(k = NULL, n = NULL, f = NULL, sig.level = NULL, power = NULL)\n\nk: Number of groups.\nn: Sample size per group.\nf: Effect size.\nsig.level: Significance level.\npower: Desired power.\n\n\n\n\n\n# Power analysis for a one-way ANOVA\npwr.anova.test(k = 4, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n\n\n\n\n\nCalculates power for correlation tests.\n\n\n\npwr.r.test(n = NULL, r = NULL, sig.level = NULL, power = NULL, alternative = \"two.sided\")\n\nr: Correlation coefficient.\n\n\n\n\n\n# Power analysis for correlation\npwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\n\n\n\n\nObjective: Determine the power of a study with a sample size of 40 per group, an effect size of 0.5, and a significance level of 0.05.\nCode:\n\nresult &lt;- pwr.t.test(n = 40, d = 0.5, sig.level = 0.05, type = \"two.sample\")\nprint(result)\n\n\n     Two-sample t test power calculation \n\n              n = 40\n              d = 0.5\n      sig.level = 0.05\n          power = 0.5981469\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOutput Interpretation: The output provides the calculated power for the test.\n\n\n\nObjective: Calculate the required sample size to detect a correlation of 0.4 with 80% power at a 5% significance level.\nCode:\n\nresult &lt;- pwr.r.test(r = 0.4, power = 0.8, sig.level = 0.05)\nprint(result)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 45.91614\n              r = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nOutput Interpretation: The result specifies the required sample size.\n\n\n\n\n\n\nThe pwr package allows you to create custom plots of power curves:\n\nlibrary(ggplot2)\n\neffect_sizes &lt;- seq(0.1, 0.5, 0.01)\npowers &lt;- sapply(effect_sizes, function(d) {\n    pwr.t.test(d = d, n = 30, sig.level = 0.05, type = \"two.sample\")$power\n})\n\ndata &lt;- data.frame(effect_sizes, powers)\n\nggplot(data, aes(x = effect_sizes, y = powers)) +\n    geom_line() +\n    labs(title = \"Power Curve\", x = \"Effect Size\", y = \"Power\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nThe pwr package can work seamlessly with simulation packages to validate power calculations.\n\n\n\n\n\nCommon Error: “Error in pwr.t.test(): Not enough parameters specified.” Fix: Ensure you provide at least three of the four parameters: n, d, power, or sig.level.\nHow to choose effect size? Use guidelines such as Cohen’s benchmarks: small (0.2), medium (0.5), and large (0.8).\n\n\n\n\n\nAlways use realistic estimates for effect size based on prior studies or pilot data.\nValidate your power calculations through simulations if possible.\n\n\n\n\n\n\nThe pwr package is a versatile and user-friendly tool for power analysis in R, covering various statistical tests and scenarios.\n\n\n\nExplore the official documentation or combine pwr with simulation-based methods for complex experimental designs.\n\n\n\nStart applying power analysis in your projects to optimize experimental designs and improve the validity of your results.\n\n\n\n\n\npwr Package Documentation\nR Power Analysis Tutorial\nComprehensive R Archive Network (CRAN)"
  },
  {
    "objectID": "appendix/r-packages/pwr.html#introduction",
    "href": "appendix/r-packages/pwr.html#introduction",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package in R is a powerful tool for conducting power analysis. Power analysis is essential in determining the sample size needed to detect an effect of a given size with a certain level of confidence. It is widely used in experimental design and statistical hypothesis testing.\n\n\n\n\nDetermining the required sample size for an experiment.\nCalculating the power of a test given sample size and effect size.\nComparing the sensitivity of different statistical tests.\n\n\n\n\n\nSimplifies power analysis for various statistical tests.\nProvides built-in functions for common tests such as t-tests, ANOVA, and correlation.\nEasy integration with R workflows for reproducibility."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#getting-started",
    "href": "appendix/r-packages/pwr.html#getting-started",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "pacman::p_load(pwr)\n\n\n\n\nHere is a simple example of using pwr.t.test to calculate the sample size needed for a one-sample t-test:\n\n# Calculate required sample size for a one-sample t-test\nresult &lt;- pwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, type = \"one.sample\")\nprint(result)\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThis calculates the required sample size to achieve a power of 0.8 for detecting an effect size of 0.5 at a significance level of 0.05."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#key-functions-and-features",
    "href": "appendix/r-packages/pwr.html#key-functions-and-features",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Calculates power, effect size, or sample size for t-tests.\n\n\n\npwr.t.test(n = NULL, d = NULL, sig.level = NULL, power = NULL, type = \"two.sample\", alternative = \"two.sided\")\n\nn: Sample size.\nd: Effect size.\nsig.level: Significance level (default is 0.05).\npower: Desired power (1 - Type II error probability).\ntype: Type of t-test (\"one.sample\", \"two.sample\", or \"paired\").\nalternative: Specifies whether the test is \"two.sided\" or \"greater\"/\"less\".\n\n\n\n\n\n# Power analysis for a two-sample t-test\npwr.t.test(n = 30, d = 0.5, sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.5\n      sig.level = 0.05\n          power = 0.4778965\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\nPerforms power analysis for balanced one-way ANOVA.\n\n\n\npwr.anova.test(k = NULL, n = NULL, f = NULL, sig.level = NULL, power = NULL)\n\nk: Number of groups.\nn: Sample size per group.\nf: Effect size.\nsig.level: Significance level.\npower: Desired power.\n\n\n\n\n\n# Power analysis for a one-way ANOVA\npwr.anova.test(k = 4, f = 0.25, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n\n\n\n\n\nCalculates power for correlation tests.\n\n\n\npwr.r.test(n = NULL, r = NULL, sig.level = NULL, power = NULL, alternative = \"two.sided\")\n\nr: Correlation coefficient.\n\n\n\n\n\n# Power analysis for correlation\npwr.r.test(r = 0.3, sig.level = 0.05, power = 0.8)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 84.07364\n              r = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided"
  },
  {
    "objectID": "appendix/r-packages/pwr.html#in-depth-examples",
    "href": "appendix/r-packages/pwr.html#in-depth-examples",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Objective: Determine the power of a study with a sample size of 40 per group, an effect size of 0.5, and a significance level of 0.05.\nCode:\n\nresult &lt;- pwr.t.test(n = 40, d = 0.5, sig.level = 0.05, type = \"two.sample\")\nprint(result)\n\n\n     Two-sample t test power calculation \n\n              n = 40\n              d = 0.5\n      sig.level = 0.05\n          power = 0.5981469\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOutput Interpretation: The output provides the calculated power for the test.\n\n\n\nObjective: Calculate the required sample size to detect a correlation of 0.4 with 80% power at a 5% significance level.\nCode:\n\nresult &lt;- pwr.r.test(r = 0.4, power = 0.8, sig.level = 0.05)\nprint(result)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 45.91614\n              r = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nOutput Interpretation: The result specifies the required sample size."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#advanced-features",
    "href": "appendix/r-packages/pwr.html#advanced-features",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package allows you to create custom plots of power curves:\n\nlibrary(ggplot2)\n\neffect_sizes &lt;- seq(0.1, 0.5, 0.01)\npowers &lt;- sapply(effect_sizes, function(d) {\n    pwr.t.test(d = d, n = 30, sig.level = 0.05, type = \"two.sample\")$power\n})\n\ndata &lt;- data.frame(effect_sizes, powers)\n\nggplot(data, aes(x = effect_sizes, y = powers)) +\n    geom_line() +\n    labs(title = \"Power Curve\", x = \"Effect Size\", y = \"Power\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nThe pwr package can work seamlessly with simulation packages to validate power calculations."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#troubleshooting-and-faqs",
    "href": "appendix/r-packages/pwr.html#troubleshooting-and-faqs",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Common Error: “Error in pwr.t.test(): Not enough parameters specified.” Fix: Ensure you provide at least three of the four parameters: n, d, power, or sig.level.\nHow to choose effect size? Use guidelines such as Cohen’s benchmarks: small (0.2), medium (0.5), and large (0.8)."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#best-practices",
    "href": "appendix/r-packages/pwr.html#best-practices",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "Always use realistic estimates for effect size based on prior studies or pilot data.\nValidate your power calculations through simulations if possible."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#conclusion",
    "href": "appendix/r-packages/pwr.html#conclusion",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "The pwr package is a versatile and user-friendly tool for power analysis in R, covering various statistical tests and scenarios.\n\n\n\nExplore the official documentation or combine pwr with simulation-based methods for complex experimental designs.\n\n\n\nStart applying power analysis in your projects to optimize experimental designs and improve the validity of your results."
  },
  {
    "objectID": "appendix/r-packages/pwr.html#references-and-resources",
    "href": "appendix/r-packages/pwr.html#references-and-resources",
    "title": "Tutorial: Power Analysis with the pwr Package in R",
    "section": "",
    "text": "pwr Package Documentation\nR Power Analysis Tutorial\nComprehensive R Archive Network (CRAN)"
  },
  {
    "objectID": "assignments/assignment7_analysis_of_covariance.html",
    "href": "assignments/assignment7_analysis_of_covariance.html",
    "title": "Assignment 7: Analysis of Covariance (ANCOVA)",
    "section": "",
    "text": "Objective: Explore the analysis of covariance (ANCOVA) and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 155"
  },
  {
    "objectID": "assignments/assignment7_analysis_of_covariance.html#instructions",
    "href": "assignments/assignment7_analysis_of_covariance.html#instructions",
    "title": "Assignment 7: Analysis of Covariance (ANCOVA)",
    "section": "",
    "text": "Objective: Explore the analysis of covariance (ANCOVA) and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 155"
  },
  {
    "objectID": "assignments/assignment7_analysis_of_covariance.html#exercise-7.7-coating-experiment",
    "href": "assignments/assignment7_analysis_of_covariance.html#exercise-7.7-coating-experiment",
    "title": "Assignment 7: Analysis of Covariance (ANCOVA)",
    "section": "Exercise 7.7: Coating Experiment",
    "text": "Exercise 7.7: Coating Experiment\nThis exercise examines the effects of spray parameters on the thermal spray coating properties of alumina (\\(\\text{Al}_2\\text{O}_3\\)). The experimental factors and levels are as follows:\n\nFuel ratio (A): 1:2.8 and 1:2.0\nCarrier gas flow rate (B): 1.33 and 3.21 \\(\\text{L/s}^{-1}\\)\nFrequency of detonations (C): 2 and 4 Hz\nSpray distance (D): 180 and 220 mm\n\nThe response variable is porosity (vol. %). Data are shown in Table 7.19.\nQuestions:\n\nDesign an Analysis\n\nAssuming negligible threeand four-factor interactions, outline the steps required to analyze the data (refer to Step (g) of the checklist in Chapter 2).\n\nCheck Model Assumptions\n\nEvaluate whether the assumptions underlying the model are valid.\n\nPerform the Analysis\n\nConduct the analysis you outlined in part (1), including interaction plots if appropriate. Clearly state your conclusions.\nTable 7.19: Data for the Coating Experiment\n\n\n\nA\nB\nC\nD\n\\(y_{ijkl}\\)\n\n\n\n\n2\n2\n2\n2\n5.95\n\n\n2\n2\n2\n1\n4.57\n\n\n2\n2\n1\n2\n4.03\n\n\n2\n2\n1\n1\n2.17\n\n\n2\n1\n2\n2\n3.43\n\n\n…\n…\n…\n…\n…\n\n\n\nSource: Adapted from Saravanan et al. (2001), Journal of Physics D: Applied Physics."
  },
  {
    "objectID": "assignments/assignment7_analysis_of_covariance.html#exercise-7.11-antifungal-antibiotic-experiment",
    "href": "assignments/assignment7_analysis_of_covariance.html#exercise-7.11-antifungal-antibiotic-experiment",
    "title": "Assignment 7: Analysis of Covariance (ANCOVA)",
    "section": "Exercise 7.11: Antifungal Antibiotic Experiment",
    "text": "Exercise 7.11: Antifungal Antibiotic Experiment\nThis exercise investigates the effects of incubation conditions on the yield of an antifungal antibiotic.\n\nFactors and Levels:\nIncubation temperature (A): 25, 30, and 37 \\(^\\circ \\text{C}\\)\nCarbon concentration (B): 2%, 5%, and 7.5%\nNitrogen concentration (C): 0.5%, 1%, and 3%\n\nThe response variable is the antifungal yield (measured in activity against Candida albicans). Data are shown in Table 7.23.\nQuestions:\n\nAssess Main Effects\n\nConstruct plots to assess the significance of main effects of \\(A\\), \\(B\\), and \\(C\\) on the response. What are your conclusions?\n\nInteraction Assumptions\n\nState the assumptions you made regarding interactions while analyzing main effects in part (1).\n\nTwo-Way Interactions\n\nConstruct plots to assess the significance of two-way interactions. Do they alter your conclusions from part (1)?\n\nFit a Model\n\nAssuming no three-way interaction, fit a model with all main effects and two-way interactions. Discuss the significance of the effects and compare with your conclusions from part (3).\n\nModel Diagnostics\n\nEvaluate whether the assumptions of normality and equal error variances are satisfied. Identify potential outliers.\nTable 7.23: Data for Antifungal Antibiotic Experiment\n\n\n\nA\nB\nC\n\\(y_{ijk}\\)\n\n\n\n\n25\n2\n0.5\n25.84\n\n\n25\n2\n1\n51.86\n\n\n25\n2\n3\n32.59\n\n\n30\n5\n1\n41.11\n\n\n37\n7.5\n0.5\n51.86\n\n\n…\n…\n…\n…\n\n\n\nSource: Gupte and Kulkarni (2003), Journal of Chemical Technology and Biotechnology."
  },
  {
    "objectID": "assignments/assignment7_analysis_of_covariance.html#exercise-7.12-antifungal-antibiotic-experiment-extended",
    "href": "assignments/assignment7_analysis_of_covariance.html#exercise-7.12-antifungal-antibiotic-experiment-extended",
    "title": "Assignment 7: Analysis of Covariance (ANCOVA)",
    "section": "Exercise 7.12: Antifungal Antibiotic Experiment (Extended)",
    "text": "Exercise 7.12: Antifungal Antibiotic Experiment (Extended)\nFor this exercise, consider the data from Table 7.23 but assume no negligible three-factor interaction. Modify levels of \\(A\\) and \\(B\\) so their third levels are 35 and 8, respectively, to ensure equal spacing.\nQuestions:\n\nTrend Contrasts\n\nCreate a table listing the 27 treatment combinations. Include contrast coefficients for:\n\nLinear and quadratic trends in \\(A\\) and \\(B\\)\nInteraction trends (\\(A \\times B\\): Linear-Linear, Linear-Quadratic, etc.)\n\n\nNormalize Contrasts\n\nIdentify divisors to normalize each contrast and manually compute least squares estimates for normalized Linear \\(A\\) and Quadratic \\(A\\) contrasts.\n\nOrthogonal Contrasts for \\(C\\)\n\nPropose two orthogonal contrasts for \\(C\\) to address its uneven spacing and add them to the table in part (1).\n\nOrthogonal Contrasts Analysis\n\nUse software to calculate least squares estimates for all 26 orthogonal contrasts. Generate a half-normal probability plot and interpret the results.\n\nAlternative Analysis\n\nApply the Voss and Wang method to analyze the orthogonal contrasts. Compare these findings to part (4).\n\nSummary Table of Grades and Rubric\n\n\n\n\n\n\n\n\n\nExercise\nCriteria\nMax Points\nDescription\n\n\n\n\nExercise 7.7\nPart (a): Analysis Design\n10\nOutline the analysis steps, assumptions, and appropriate statistical methods (e.g., ANOVA, interaction plots).\n\n\n\nPart (b): Model Assumptions\n10\nAssess normality, homoscedasticity, and independence of residuals using statistical tests and plots.\n\n\n\nPart (c): Analysis and Plots\n20\nConduct ANOVA, interpret significant effects, create interaction plots, and draw meaningful conclusions.\n\n\nTotal for Exercise 7.7\n\n40\n\n\n\nExercise 7.11\nPart (a): Main Effects\n10\nConstruct main effects plots and evaluate their significance.\n\n\n\nPart (b): Interaction Assumptions\n5\nClearly state and justify interaction assumptions.\n\n\n\nPart (c): Two-Way Interactions\n15\nConstruct interaction plots, interpret results, and update conclusions from main effects analysis.\n\n\n\nPart (d): Model Fitting\n15\nFit a model including all main effects and two-way interactions, interpret significant effects.\n\n\n\nPart (e): Diagnostics\n10\nEvaluate residual diagnostics for normality, homoscedasticity, and outliers.\n\n\nTotal for Exercise 7.11\n\n55\n\n\n\nExercise 7.12\nPart (a): Trend Contrasts\n15\nCreate a table with contrast coefficients for linear, quadratic, and interaction terms.\n\n\n\nPart (b): Normalization\n10\nCompute normalization divisors and least squares estimates.\n\n\n\nPart (c): Orthogonal Contrasts\n10\nPropose and justify two orthogonal contrasts for CC.\n\n\n\nPart (d): Computer Analysis\n15\nCompute estimates, create a half-normal probability plot, and interpret results.\n\n\n\nPart (e): Voss and Wang Method\n10\nApply the method to examine orthogonal contrasts and compare with results from the half-normal plot.\n\n\nTotal for Exercise 7.12\n\n60\n\n\n\n\n\n\nGrade Summary Table\n\n\n\n\n\n\n\n\n\nExercise\nMax Points\nEarned Points\nComments\n\n\n\n\nExercise 7.7\n40\n\n\n\n\nExercise 7.11\n55\n\n\n\n\nExercise 7.12\n60\n\n\n\n\nOverall Total\n155"
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html",
    "href": "assignments/assignment5_two_factor_anova.html",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "",
    "text": "Objective: Explore the two-way complete model and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 95"
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#instructions",
    "href": "assignments/assignment5_two_factor_anova.html#instructions",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "",
    "text": "Objective: Explore the two-way complete model and its applications in experimental designs.\nUse R to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal points: 95"
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.2-interaction-contrast",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.2-interaction-contrast",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.2: Interaction Contrast",
    "text": "Exercise 6.2: Interaction Contrast\nVerify that the expression\n\\[\n\\tau_{ij} - \\tau_{i.} - \\tau_{.j} + \\tau_{..}\n\\]\nis an interaction contrast for the two-way complete model.\n\nWrite the list of contrast coefficients in terms of the \\(\\tau_{ij}\\)’s under the following conditions:\n\nFactor \\(A\\) has \\(a = 3\\) levels.\nFactor \\(B\\) has \\(b = 4\\) levels."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.3-functions-of-parameters",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.3-functions-of-parameters",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.3: Functions of Parameters",
    "text": "Exercise 6.3: Functions of Parameters\nConsider the following functions under the two-way complete model:\n\n\\(\\{\\alpha^{*}_{1} - \\alpha^{*}_{2}\\}\\).\n\\(\\{(\\alpha \\beta)_{11} - (\\alpha \\beta)_{21} - (\\alpha \\beta)_{12} + (\\alpha \\beta)_{22}\\}\\).\n\n\nTasks\n\nVerify that these functions are estimable contrasts.\nDiscuss the meaning of each contrast for:\n\n\nPlot (d) from Figure 6.1 on page 140.\nPlot (g) from Figure 6.2 on page 141.\n\n\nFor \\(a = b = 3\\), provide:\n\n\nThe list of contrast coefficients for the parameters involved in the contrast.\nThe equivalent list of contrast coefficients in terms of the \\(\\tau_{ij}\\) parameters of the cell-means model."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.7-weld-strength-experiment",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.7-weld-strength-experiment",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.7: Weld Strength Experiment",
    "text": "Exercise 6.7: Weld Strength Experiment\nThe data in Table 6.22, obtained from Anderson and McLean (1974), show the weld strength of a steel bar under varying conditions of two factors:\n\nGage bar setting: The distance traveled by the weld die during the automatic weld cycle.\nTime of welding: The total time of the automatic weld cycle.\n\n\n\n\nTime of Welding (j)\n1\n2\n3\n4\n5\n\n\n\n\nGage Bar 1\n10, 12\n13, 17\n21, 30\n18, 16\n17, 21\n\n\nGage Bar 2\n15, 19\n14, 12\n30, 38\n15, 11\n14, 12\n\n\nGage Bar 3\n10, 8\n12, 9\n10, 5\n14, 15\n19, 11\n\n\n\n\nTasks\n\nUsing the cell-means model, test the hypothesis that there is no difference in the effects of the treatment combinations on weld strength.\nWrite contrasts in terms of \\(\\tau_{ij}\\) for:\n\n\nAll pairwise comparisons.\nThe difference between gage bar setting 3 and the average of the other two.\n\n\nSuggest a strategy to calculate all intervals at an overall confidence level of at least 98%.\n\n\nCompute confidence intervals for:\n\n\n\\(\\tau_{13} - \\tau_{15}\\): The difference in true mean strengths at the 3rd and 5th welding times for the first gage bar setting.\nThe difference between gage bar setting 3 and the average of the other two gage bar settings.\n\n\nCalculate an upper 90% confidence limit for \\(\\sigma^2\\).\nDetermine the total number of observations needed if the pairwise comparison intervals in part (b) must have a maximum width of 8."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.12-memory-experiment",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.12-memory-experiment",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.12: Memory Experiment",
    "text": "Exercise 6.12: Memory Experiment\nThe memory experiment investigates the effects of two factors:\n\nWord type: Fruits, nouns, and mixed types.\nType of distraction: No distraction, constant distraction, or changing distraction.\n\nThe response variable is the number of words remembered, with variance approximated by \\(30p(1-p)\\).\n\nTask\nDetermine the number of subjects required to reject the following hypotheses with a power of 0.9 at \\(\\alpha = 0.05\\):\n\n\\(H_A\\): The memorization rate is identical for the three word types.\n\\(H_B\\): The distraction types have no effect on memorization.\n\nAssume the minimum difference of interest in memorization rates is 4 words."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.16-survival-experiment",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.16-survival-experiment",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.16: Survival Experiment",
    "text": "Exercise 6.16: Survival Experiment\nThe following table provides survival times (in units of 10 hours) for animals exposed to three poisons and four treatments.\n\n\n\n\n\n\n\n\n\n\nPoison/Treatment\n1\n2\n3\n4\n\n\n\n\nI\n0.31, 0.45, 0.46, 0.43\n0.82, 1.10, 0.88, 0.72\n0.43, 0.45, 0.63, 0.76\n0.45, 0.71, 0.66, 0.62\n\n\nII\n0.36, 0.29, 0.40, 0.23\n0.92, 0.61, 0.49, 1.24\n0.44, 0.35, 0.31, 0.40\n0.56, 1.02, 0.71, 0.38\n\n\nIII\n0.22, 0.21, 0.18, 0.23\n0.30, 0.37, 0.38, 0.29\n0.23, 0.25, 0.24, 0.22\n0.30, 0.36, 0.31, 0.33\n\n\n\n\nTasks\n\nVerify assumptions of the two-way complete model using the original data. Analyze the data if the assumptions are satisfied.\nTransform the data using \\(y^{-1}\\) (reciprocal). Reassess model assumptions and reanalyze the transformed data.\nCreate interaction plots for the original and transformed data. Discuss how the interaction between poison and treatment varies with each scale."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#exercise-6.19-main-effects-model-verification",
    "href": "assignments/assignment5_two_factor_anova.html#exercise-6.19-main-effects-model-verification",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Exercise 6.19: Main-Effects Model Verification",
    "text": "Exercise 6.19: Main-Effects Model Verification\nFor the two-way main-effects model with equal sample sizes, perform the following:\n\nVerify the computational formula for \\(SS_E\\) as given in equation (6.5.38).\nDemonstrate that \\(E[SSE] = (n - a - b + 1)\\sigma^2\\).\n\nHint: Use the formula \\(E[X^2] = \\text{Var}(X) + E[X]^2\\)."
  },
  {
    "objectID": "assignments/assignment5_two_factor_anova.html#summary-of-the-grades-and-rubric",
    "href": "assignments/assignment5_two_factor_anova.html#summary-of-the-grades-and-rubric",
    "title": "Assignment 5: Two-Factor ANOVA",
    "section": "Summary of the Grades and Rubric",
    "text": "Summary of the Grades and Rubric\n\n\n\n\n\n\n\n\n\nExercise\nTask\nPoints\nDescription\n\n\n\n\nExercise 6.2\nDerive contrast coefficients and verify interaction contrast property.\n10\n- 4 points for correct derivation of coefficients. - 3 points for verifying the interaction contrast property. - 3 points for clarity and completeness.\n\n\nExercise 6.3\nVerify estimability, interpret contrasts, and compute coefficients.\n15\n- 5 points for verifying estimability. - 5 points for interpreting contrasts in provided plots. - 5 points for accurate and clear computation of coefficients.\n\n\nExercise 6.7\nPerform hypothesis testing, compute contrasts, intervals, and variance limits.\n25\n- 5 points each for parts (a) to (e), including hypothesis testing, contrast definitions, confidence interval computations, upper confidence limits for \\(\\sigma^2\\), and sample size needs.\n\n\nExercise 6.12\nCompute sample size for given power and significance level.\n15\n- 8 points for accurate computation of sample size. - 7 points for interpretation and justification of the calculations.\n\n\nExercise 6.16\nVerify model assumptions, perform transformation, and create interaction plots.\n20\n- 7 points for verifying assumptions. - 7 points for transformation analysis. - 6 points for interaction plots and interpretation.\n\n\nExercise 6.19\nVerify \\(SS_E\\) formula and derive expected value of \\(SSE\\).\n10\n- 5 points for verifying \\(SS_E\\) computational formula. - 5 points for deriving \\(E[SSE] = (n - a - b + 1)\\sigma^2\\)."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Objective: Perform multiple comparisons and contrasts for experimental data, including Scheffé’s method and simultaneous confidence intervals.\nUse R or a similar tool to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#instructions",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#instructions",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Objective: Perform multiple comparisons and contrasts for experimental data, including Scheffé’s method and simultaneous confidence intervals.\nUse R or a similar tool to conduct the analyses for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 100"
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#buoyancy-experiment",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#buoyancy-experiment",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "1. Buoyancy Experiment",
    "text": "1. Buoyancy Experiment\nInvestigate the question: “Is the buoyancy of an object in water affected by different concentrations of salt in the water?”\nTasks: (a) Address steps (a)–(d) from the experimental planning checklist in detail:\n\nDefine the objectives of the experiment.\nIdentify sources of variation, including treatment factors, experimental units, and nuisance factors.\nChoose an assignment rule for the experimental units.\nSpecify measurements, procedures, and any anticipated difficulties.\nSpecify preplanned contrasts or functions to estimate. Justify your choices.\nDetermine which multiple comparison methods, if any, are appropriate and explain why.\n\n\nConduct a small pilot experiment to estimate the preliminary variance (\\(\\sigma^2\\)). Use R to simulate data if needed and summarize findings.\nRevisit and complete the checklist by evaluating findings from the pilot study and revising experimental decisions if necessary."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#cotton-spinning-experiment-continuation-from-section-2.3-p.-13",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#cotton-spinning-experiment-continuation-from-section-2.3-p.-13",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "2. Cotton-Spinning Experiment (Continuation from Section 2.3, P. 13)",
    "text": "2. Cotton-Spinning Experiment (Continuation from Section 2.3, P. 13)\nFor the cotton-spinning experiment:\n\nIdentify and define contrasts or functions of interest. Specify which effects or comparisons will provide insight into the experimental objectives.\nProvide contrast coefficients for your selected contrasts."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#meat-cooking-experiment-continuation-from-exercise-14-chapter-3",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#meat-cooking-experiment-continuation-from-exercise-14-chapter-3",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "3. Meat Cooking Experiment (Continuation from Exercise 14, Chapter 3)",
    "text": "3. Meat Cooking Experiment (Continuation from Exercise 14, Chapter 3)\nThis experiment involved six treatments with data in Table 3.14, p. 68. Address the following:\n\nConduct pairwise comparisons for the six treatments using Scheffé’s method with a 95% overall confidence level. Present your results with appropriate statistical summaries and visualizations.\nAnalyze the expressions \\(\\mu + \\frac{(\\tau_1 + \\tau_4)}{2}\\), \\(\\mu + \\frac{(\\tau_2 + \\tau_5)}{2}\\), and \\(\\mu + \\frac{(\\tau_3 + \\tau_6)}{2}\\):\n\n\nInterpret what these expressions represent in the context of the experimental setup.\nPerform pairwise comparisons among these three treatment averages using Scheffé’s method with a 95% confidence level.\nSummarize and interpret the findings in terms of practical significance and experimental insights."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#reaction-time-experiment",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#reaction-time-experiment",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "4. Reaction Time Experiment",
    "text": "4. Reaction Time Experiment\nSource: (L. Cai, T. Li, Nishant, and A. van der Kouwe, 1996)\nThis experiment investigates the effects of auditory and visual cues on the speed of response of a human subject. A personal computer presents a stimulus to the subject, and the reaction time required to press a key is monitored. The subject is warned of the forthcoming stimulus through auditory or visual cues. The study also considers different elapsed times between cue and stimulus as a factor.\nFactors and Levels:\n\nCue Stimulus: Auditory or Visual\nElapsed Time: 5, 10, or 15 seconds\n\nTreatment Combinations and Codes:\n\n1 = Auditory, 5 seconds\n2 = Auditory, 10 seconds\n3 = Auditory, 15 seconds\n4 = Visual, 5 seconds\n5 = Visual, 10 seconds\n6 = Visual, 15 seconds\n\nThe pilot experiment data, involving one subject, are shown in Table 4.4.\nHere is the recreated Table 4.4 in a structured format:\nTable 4.4: Reaction times, in seconds, for the reaction time experiment (order of collection in parentheses).\n\n\n\n\n\n\n\n\n\nTreatment Combination\nReaction Time 1 (s) (Order)\nReaction Time 2 (s) (Order)\nReaction Time 3 (s) (Order)\n\n\n\n\n1\n0.181 (18)\n0.204 (9)\n0.170 (10)\n\n\n2\n0.187 (12)\n0.167 (3)\n0.182 (5)\n\n\n3\n0.236 (17)\n0.202 (13)\n0.198 (16)\n\n\n4\n0.269 (15)\n0.257 (7)\n0.279 (14)\n\n\n5\n0.260 (11)\n0.283 (6)\n0.235 (8)\n\n\n6\n0.258 (4)\n0.256 (1)\n0.281 (2)\n\n\n\nTasks: (a) Identify Preplanned Contrasts:\n\nDetermine contrasts of interest, such as comparing auditory versus visual treatments. Specify the contrasts mathematically, including coefficients for the comparisons.\n\n\nPlot the Data:\n\n\nVisualize the reaction times using an appropriate plot (e.g., bar or line plot). Summarize what the plot reveals about the treatment effects.\n\n\nHypothesis Testing:\n\n\nTest the null hypothesis \\(H_0\\): The treatments have no effect on reaction time against the alternative \\(H_1\\): The treatments have an effect on reaction time. Include a detailed explanation of the statistical test used and the results obtained.\n\n\nSimultaneous Confidence Intervals:\n\n\nCompute simultaneous 90% confidence intervals for the preplanned contrasts using a chosen method (e.g., Tukey’s or Scheffé’s method). Interpret these intervals and state your conclusions about the treatment effects."
  },
  {
    "objectID": "assignments/assignment3_multiple_comparisons_and_contrasts.html#grading-rubric-for-the-assignment",
    "href": "assignments/assignment3_multiple_comparisons_and_contrasts.html#grading-rubric-for-the-assignment",
    "title": "Assignment 3: Multiple Comparisons and Contrasts",
    "section": "Grading Rubric for the Assignment",
    "text": "Grading Rubric for the Assignment\n\n\n\n\n\n\n\n\n\n\nTask\nExcellent (100%)\nGood (85%)\nSatisfactory (70%)\nNeeds Improvement (50%)\n\n\n\n\n1. Buoyancy Experiment\nComplete and detailed checklist, well-justified preplanned contrasts, proper simulation and analysis in R.\nDetailed checklist but minor omissions in preplanned contrasts or R simulation.\nChecklist addressed but lacks sufficient detail or justification. Simulation and analysis incomplete.\nMajor gaps in checklist completion, no simulation, or preplanned contrasts missing.\n\n\n2. Cotton-Spinning Experiment\nAll contrasts identified clearly, and coefficients are mathematically accurate and fully explained.\nMost contrasts identified, and coefficients mostly correct. Explanation somewhat unclear.\nContrasts partially correct; coefficients missing or inaccurate. Explanation lacks detail or clarity.\nIncorrect or missing contrasts; coefficients absent or mathematically incorrect.\n\n\n3. Meat Cooking Experiment\nAll pairwise comparisons and interpretations are correct and well-presented with visualizations and context.\nMost pairwise comparisons correct, with minor errors in calculations or interpretations.\nComparisons incomplete or partially correct. Visualizations or interpretations are unclear.\nIncorrect or missing pairwise comparisons, no visualizations, or failure to interpret findings.\n\n\n4. Reaction Time Experiment\nAccurate contrasts with coefficients, clear plots, correct hypothesis testing, and well-justified confidence intervals.\nCorrect contrasts and coefficients, minor issues with plots or hypothesis testing, intervals partially correct.\nPartial contrasts or coefficients; plots unclear, hypothesis testing or intervals incomplete.\nContrasts missing or incorrect, no plots, hypothesis testing or intervals omitted or inaccurate.\n\n\n\n\nGrading Allocation\nEach question is worth 25 points, distributed as follows:\n\nCompleteness (10 points): Address all aspects of the question.\nAccuracy (10 points): Provide correct methods, calculations, and R code.\nClarity (5 points): Present results in a clear, logical, and concise manner."
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html",
    "href": "assignments/assignment2_completely_randomized_design.html",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "",
    "text": "Complete all questions below.\nUse the R programming language for computational problems. Include annotated R code and relevant output.\nProvide clear and concise explanations and interpretations for all tasks.\n\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#instructions",
    "href": "assignments/assignment2_completely_randomized_design.html#instructions",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "",
    "text": "Complete all questions below.\nUse the R programming language for computational problems. Include annotated R code and relevant output.\nProvide clear and concise explanations and interpretations for all tasks.\n\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#set-1-foundational-questions-suitable-for-undergraduate-and-graduate-students",
    "href": "assignments/assignment2_completely_randomized_design.html#set-1-foundational-questions-suitable-for-undergraduate-and-graduate-students",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "Set 1: Foundational Questions (Suitable for Undergraduate and Graduate Students)",
    "text": "Set 1: Foundational Questions (Suitable for Undergraduate and Graduate Students)\n\n1.1 Balloon Experiment\nData Source: Dean et al. (2017), Chapter 3, Exercise 12\nData Overview: The data for this experiment is provided below:\n\nBalloon inflation times for different colors\n\n\nTime Order\nCoded Color\nInflation Time (seconds)\n\n\n\n\n1\n1\n22.0\n\n\n2\n3\n24.6\n\n\n3\n1\n20.3\n\n\n4\n4\n19.8\n\n\n5\n3\n24.3\n\n\n6\n2\n22.2\n\n\n7\n2\n28.5\n\n\n8\n2\n25.7\n\n\n9\n3\n20.2\n\n\n10\n1\n19.6\n\n\n11\n2\n28.8\n\n\n12\n4\n24.0\n\n\n13\n4\n17.1\n\n\n14\n4\n19.3\n\n\n15\n3\n24.2\n\n\n16\n1\n15.8\n\n\n17\n2\n18.3\n\n\n18\n1\n17.5\n\n\n19\n4\n18.7\n\n\n20\n3\n22.9\n\n\n21\n1\n16.3\n\n\n22\n4\n14.0\n\n\n23\n4\n16.6\n\n\n24\n2\n18.1\n\n\n25\n2\n18.9\n\n\n26\n4\n16.0\n\n\n27\n2\n20.1\n\n\n28\n3\n22.5\n\n\n29\n3\n16.0\n\n\n30\n1\n19.3\n\n\n31\n1\n15.9\n\n\n32\n3\n20.3\n\n\n\nThe codes for balloon colors are as follows:\n\n1 = Pink\n2 = Yellow\n3 = Orange\n4 = Blue\n\nTasks:\n\n\nPlot inflation time versus balloon color and comment on the results.\n\n\nEstimate the mean inflation time for each balloon color and add these estimates to the plot from part (a).\n\n\nConstruct an analysis of variance (ANOVA) table and test the hypothesis that balloon color has no effect on inflation time (\\(\\alpha = 0.05\\)).\n\n\nPlot the data for each color in the order it was collected. Discuss whether the assumptions of the ANOVA model (e.g., normality, independence) appear satisfied.\n\n\nEvaluate whether the analysis in part (c) is adequate. If not, suggest improvements.\n\n\n\n\n1.2 Conceptual Question\n\nDiscuss how increasing the number of replicates affects the power of a one-way ANOVA test.\n\n\n\n1.3 Graphical Analysis\n\nCreate a boxplot of inflation times for each balloon color. Based on the plot, comment on visual evidence of differences among the treatments."
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#set-2-advanced-computational-and-proof-based-questions-graduate-level",
    "href": "assignments/assignment2_completely_randomized_design.html#set-2-advanced-computational-and-proof-based-questions-graduate-level",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "Set 2: Advanced Computational and Proof-Based Questions (Graduate Level)",
    "text": "Set 2: Advanced Computational and Proof-Based Questions (Graduate Level)\n\n2.1 Derivation of \\(t\\)-Statistic for Contrasts in ANOVA\nData Source: Christensen (2018)., Exercise 12.7.12\nIn a one-way ANOVA, the sum of squares for error (\\(SSE\\)) divided by the population variance (\\(\\sigma^2\\)) follows a chi-squared distribution with degrees of freedom equal to \\(df_E\\):\n\\[\n\\frac{SSE}{\\sigma^2} \\sim \\chi^2(df_E),\n\\]\nand the mean square error (\\(MSE\\)) is independent of all sample means (\\(\\bar{y}_i\\)). Show that the following expression follows a \\(t\\)-distribution with \\(df_E\\) degrees of freedom:\n\\[\n\\frac{\\sum_{i=1}^a \\lambda_i \\bar{y}_i - \\sum_{i=1}^a \\lambda_i \\mu_i}{\\sqrt{MSE \\sum_{i=1}^a \\frac{\\lambda_i^2}{N_i}}} \\sim t(df_E).\n\\]\nTasks:\n\nDerive the above expression step-by-step, starting with the variance of the contrast \\(\\sum_{i=1}^a \\lambda_i \\bar{y}_i\\).\nExplain why \\(MSE\\) serves as an unbiased estimate of \\(\\sigma^2\\).\nUsing the result, construct a hypothesis test for a contrast in a one-way ANOVA and provide a practical interpretation of the result.\n\n\n\n2.2 Implications of Assumption Violations\n\nDiscuss the implications of violating the assumption of homogeneity of variances in a one-way ANOVA. Suggest two methods to address such violations.\n\n\n\n2.3 Power Analysis\n\nCalculate the sample size required for a one-way ANOVA with four groups to detect a medium effect size (\\(f = 0.25\\)) at a significance level of \\(\\alpha = 0.05\\) and a power of \\(1 - \\beta = 0.8\\)."
  },
  {
    "objectID": "assignments/assignment2_completely_randomized_design.html#grading-rubric",
    "href": "assignments/assignment2_completely_randomized_design.html#grading-rubric",
    "title": "Assignment 2: Completely Randomized Design",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\nTask\nPoints\n\n\n\n\nSet 1: Foundational Questions\n50\n\n\n1.1 Balloon Experiment\n30\n\n\n1.2 Conceptual Question\n10\n\n\n1.3 Graphical Analysis\n10\n\n\nSet 2: Advanced Questions\n50\n\n\n2.1 Derivation of \\(t\\)-Statistic\n20\n\n\n2.2 Implications of Assumption Violations\n10\n\n\n2.3 Power Analysis\n20\n\n\nTotal\n100"
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html",
    "href": "assignments/assignment13_split_plot_designs.html",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "",
    "text": "Objective: Analyze split-plot designs, including whole-plot and split-plot effects, interaction plots, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\ntotal: 100 points"
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html#instructions",
    "href": "assignments/assignment13_split_plot_designs.html#instructions",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "",
    "text": "Objective: Analyze split-plot designs, including whole-plot and split-plot effects, interaction plots, and post-hoc tests.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\ntotal: 100 points"
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html#drug-experiment",
    "href": "assignments/assignment13_split_plot_designs.html#drug-experiment",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "19.1 Drug Experiment",
    "text": "19.1 Drug Experiment\nData Source: Dean et al. (2017), Chapter 19, Exercise 1\nAn experiment designed as a split-plot design evaluated eight drugs (Factor \\(A\\)) for treating arthritis. The second factor was dose (Factor \\(B\\)) at 2 levels, and the third was time (Factor \\(C\\)) at 2 levels. The experimental units were 64 rats, and the response was the amount of fluid (in mL) measured in the pleural cavity. The experiment was structured into blocks, whole plots, and split plots:\n\nBlocks: Two blocks of size 32, measured on two separate days.\nWhole Plots: Each block was divided into four whole plots of size 8.\nSplit Plots: Each whole plot was subdivided, with one drug assigned to each rat.\n\nThe logarithms of the response data were used for analysis.\n\n\n\n\n\n\n\n\n\n\n\nWhole Plot\nBlock\nDose (\\(B\\))\nTime (\\(C\\))\nDrug (\\(A\\))\nData\n\n\n\n\n1\nBlock I\n1\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.7, 8.6, 6.9, 6.6, 6.7, 7.4, 5.7, 6.7\\)\n\n\n2\nBlock I\n1\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(8.4, 9.6, 9.3, 11.1, 12.5, 8.7, 9.3, 9.5\\)\n\n\n3\nBlock I\n2\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.1, 7.2, 6.8, 6.4, 6.6, 8.7, 6.7, 7.0\\)\n\n\n4\nBlock I\n2\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(7.3, 8.7, 7.9, 6.9, 8.9, 9.5, 8.3, 11.3\\)\n\n\n5\nBlock II\n1\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.8, 6.8, 7.0, 8.5, 7.8, 7.3, 6.4, 8.5\\)\n\n\n6\nBlock II\n1\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(9.1, 10.8, 6.9, 12.2, 9.9, 10.4, 10.6, 10.5\\)\n\n\n7\nBlock II\n2\n1\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.4, 7.9, 8.0, 6.4, 8.4, 7.1, 6.4, 7.2\\)\n\n\n8\nBlock II\n2\n2\n1, 2, 3, 4, 5, 6, 7, 8\n\\(5.3, 10.4, 8.2, 8.1, 10.9, 9.8, 8.4, 14.6\\)\n\n\n\n\nQuestions:\n\nWrite out a model for this experiment.\nCalculate an analysis of variance table using the logarithms of the data. Distinguish between the effects measured on the whole plots and those measured on the split plots. Identify the whole-plot error and split-plot error.\nTest any hypotheses of interest and state your conclusions clearly.\nExamine interaction plots of any important interactions. Calculate a set of 95% confidence intervals for the differences between pairs of drugs. State your conclusions."
  },
  {
    "objectID": "assignments/assignment13_split_plot_designs.html#fishing-line-experiment",
    "href": "assignments/assignment13_split_plot_designs.html#fishing-line-experiment",
    "title": "Assignment 13 - Split Plot Designs",
    "section": "19.2 Fishing Line Experiment",
    "text": "19.2 Fishing Line Experiment\nData Source: Dean et al. (2017), Chapter 19, Exercise 2\nThe experiment compared the strengths of two brands of fishing line exposed to two stress levels (stressed, \\(S\\); non-stressed, \\(N\\)). Each brand corresponded to two reels (whole plots), and each reel provided four sections of fishing line (split plots). The response was the weight (lbs) required to break the line.\n\n\n\n\n\n\n\n\n\nWhole Plot\nBrand (\\(A\\))\nStress Level (\\(B\\))\nData (Weight in lbs)\n\n\n\n\n1\n1\nN, S, S, N\n\\(6.70, 6.40, 7.20, 7.00\\)\n\n\n2\n2\nS, S, N, N\n\\(8.10, 8.90, 8.00, 6.10\\)\n\n\n3\n2\nS, S, N, N\n\\(8.00, 8.00, 8.75, 8.50\\)\n\n\n4\n1\nN, S, N, S\n\\(8.50, 9.50, 9.70, 9.40\\)\n\n\n\n\nQuestions:\n\nWrite down a model for this experiment.\nConstruct an analysis of variance table and test the hypotheses of interest. State your conclusions.\nIf you were to repeat this experiment, suggest improvements.\n\n\n\nGrading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nDrug Experiment\n(a)\n10\n\n\n\n(b)\n15\n\n\n\n(c)\n10\n\n\n\n(d)\n15\n\n\nFishing Line Experiment\n(a)\n10\n\n\n\n(b)\n15\n\n\n\n(c)\n10\n\n\n\nTotal Score: 100%"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html",
    "href": "assignments/assignment10_row_column_designs.html",
    "title": "Assignment 10: Row and Column Designs",
    "section": "",
    "text": "Objective: Analyze row and column designs, including row-column interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 150"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#instructions",
    "href": "assignments/assignment10_row_column_designs.html#instructions",
    "title": "Assignment 10: Row and Column Designs",
    "section": "",
    "text": "Objective: Analyze row and column designs, including row-column interactions, post-hoc tests, and trend analyses.\nUse R to conduct the statistical analysis for each exercise. Submit your results, including code and explanations, in a clear format.\nClearly explain your analysis process for full credit.\nSubmit your assignment as a single PDF file.\n\nTotal Points: 150"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#latin-squares",
    "href": "assignments/assignment10_row_column_designs.html#latin-squares",
    "title": "Assignment 10: Row and Column Designs",
    "section": "12.2 Latin Squares",
    "text": "12.2 Latin Squares\nData Source: Dean et al. (2017), Chapter 12, Exercise 2\n\nShow that there is only one standard \\(3 \\times 3\\) Latin square.\n\n(Hint: Given the letters in the first row and the first column, show that there is only one way to complete the Latin square.)\n\nShow that there are exactly four standard \\(4 \\times 4\\) Latin squares."
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#video-game-experiment",
    "href": "assignments/assignment10_row_column_designs.html#video-game-experiment",
    "title": "Assignment 10: Row and Column Designs",
    "section": "12.6 Video Game Experiment",
    "text": "12.6 Video Game Experiment\nData Source: Dean et al. (2017), Chapter 12, Exercise 6\nProfessor Robert Wardrop conducted an experiment in 1991 to evaluate which of five sound modes helped him play a certain video game best. The sound modes included:\n\nModes 1–3: Three different types of background music with game sounds expected to enhance play.\nMode 4: Game sounds only, no background music.\nMode 5: No music or game sounds.\n\nThe experiment used a Latin square design with two blocking factors: day and time order of the game. The response variable was the game score (higher scores indicate better performance). The design and data are provided in Table 12.16.\n\nLatin square design showing treatments and data for the video game experiment\n\n\n\n\n\n\n\n\n\n\nTime Order\nDay 1\nDay 2\nDay 3\nDay 4\nDay 5\n\n\n\n\n1\n1 (94)\n3 (100)\n4 (98)\n2 (101)\n5 (112)\n\n\n2\n3 (103)\n2 (111)\n1 (51)\n5 (110)\n4 (90)\n\n\n3\n4 (114)\n1 (75)\n5 (94)\n3 (85)\n2 (107)\n\n\n4\n5 (100)\n4 (74)\n2 (70)\n1 (93)\n3 (106)\n\n\n5\n2 (106)\n5 (95)\n3 (81)\n4 (90)\n1 (73)\n\n\n\n\nQuestions\n\nWrite down a possible model for the data and check the model assumptions. If the assumptions appear to be approximately satisfied, proceed with parts (b)–(f).\nPlot the adjusted data and discuss the plot.\nComplete an analysis of variance table.\nEvaluate whether blocking (day and time order) was effective.\nConstruct simultaneous 95% confidence intervals for:\n\nAll pairwise comparisons of the five sound modes.\nThe “music versus no music” contrast:\n\n\n\\[\n\\frac{1}{3}(\\tau_1 + \\tau_2 + \\tau_3) - \\frac{1}{2}(\\tau_4 + \\tau_5)\n\\] 3. The “game sound versus no game sound” contrast:\n\\[\n\\frac{1}{4}(\\tau_1 + \\tau_2 + \\tau_3 + \\tau_4) - \\tau_5\n\\]\n\nSummarize the conclusions of the experiment. Which sound mode(s) should Professor Wardrop use for optimal performance?"
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#quantity-perception-experiment",
    "href": "assignments/assignment10_row_column_designs.html#quantity-perception-experiment",
    "title": "Assignment 10: Row and Column Designs",
    "section": "12.10 Quantity Perception Experiment",
    "text": "12.10 Quantity Perception Experiment\nData Source: Dean et al. (2017), Chapter 12, Exercise 10\nAn experiment conducted in 1996 investigated how color influences the perception of quantity. Subjects were asked to guess the number of candies on a tray after viewing it for 3 seconds. The response was the difference between the actual number of candies and the guessed number.\n\nFactors:\n\nNumber of candies (levels): 17, 29, 41\nColor (levels): Yellow, Orange, Brown\n\n\nEach subject viewed all nine treatment combinations based on a \\(9 \\times 9\\) Latin square. The data provided in Table 12.18 represents a 2-replicate Latin square.\n\nTreatment Combinations Coding:\n\n\\(1 = (17, \\text{Yellow})\\), \\(2 = (17, \\text{Orange})\\), \\(3 = (17, \\text{Brown})\\)\n\\(4 = (29, \\text{Yellow})\\), \\(5 = (29, \\text{Orange})\\), \\(6 = (29, \\text{Brown})\\)\n\\(7 = (41, \\text{Yellow})\\), \\(8 = (41, \\text{Orange})\\), \\(9 = (41, \\text{Brown})\\)\n\n\nHere is the correct recreation of Table 12.18 based on the provided text:\n\n2-replicate Latin square design and data in parentheses for the quantity perception experiment—data are ‘true number’ minus ‘guessed number’\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n1\n23 (4)\n22 (−3)\n11 (0)\n12 (−3)\n32 (−1)\n13 (−3)\n31 (−6)\n33 (−9)\n21 (−1)\n\n\n2\n12 (2)\n31 (16)\n32 (21)\n21 (9)\n22 (4)\n33 (16)\n23 (9)\n11 (2)\n13 (2)\n\n\n3\n21 (4)\n23 (−1)\n12 (7)\n13 (−5)\n33 (−1)\n11 (−13)\n32 (−9)\n31 (−19)\n22 (−16)\n\n\n4\n32 (21)\n12 (4)\n22 (3)\n31 (11)\n13 (0)\n21 (4)\n11 (0)\n23 (4)\n33 (11)\n\n\n5\n31 (7)\n11 (−2)\n21 (2)\n33 (3)\n12 (−3)\n23 (3)\n13 (−4)\n22 (−5)\n32 (−7)\n\n\n6\n11 (3)\n33 (7)\n31 (14)\n23 (11)\n21 (12)\n32 (17)\n22 (10)\n13 (5)\n12 (0)\n\n\n7\n22 (11)\n21 (14)\n13 (0)\n11 (1)\n31 (16)\n12 (1)\n33 (13)\n32 (14)\n23 (7)\n\n\n8\n13 (7)\n32 (16)\n33 (16)\n22 (4)\n23 (4)\n31 (16)\n21 (14)\n12 (7)\n11 (2)\n\n\n9\n33 (21)\n13 (2)\n23 (10)\n32 (24)\n11 (6)\n22 (13)\n12 (2)\n21 (8)\n31 (20)\n\n\n10\n33 (16)\n31 (20)\n22 (6)\n21 (6)\n11 (7)\n23 (6)\n12 (2)\n13 (3)\n32 (14)\n\n\n11\n12 (2)\n22 (4)\n32 (11)\n13 (2)\n21 (9)\n33 (1)\n23 (4)\n31 (−4)\n11 (7)\n\n\n12\n13 (−4)\n23 (−11)\n33 (−4)\n11 (−3)\n22 (4)\n31 (11)\n21 (−1)\n32 (1)\n12 (−3)\n\n\n13\n21 (4)\n12 (−1)\n11 (2)\n32 (11)\n31 (11)\n13 (−3)\n33 (1)\n22 (−1)\n23 (11)\n\n\n14\n22 (2)\n13 (−7)\n12 (−9)\n33 (8)\n32 (−2)\n11 (−6)\n31 (−9)\n23 (4)\n21 (2)\n\n\n15\n31 (21)\n32 (21)\n23 (14)\n22 (14)\n12 (4)\n21 (16)\n13 (0)\n11 (5)\n33 (11)\n\n\n16\n11 (2)\n21 (9)\n31 (21)\n12 (6)\n23 (9)\n32 (18)\n22 (9)\n33 (16)\n13 (2)\n\n\n17\n32 (6)\n33 (6)\n21 (−1)\n23 (−1)\n13 (2)\n22 (4)\n11 (−3)\n12 (−1)\n31 (6)\n\n\n18\n23 (4)\n11 (2)\n13 (2)\n31 (11)\n33 (6)\n12 (2)\n32 (6)\n21 (−1)\n22 (4)\n\n\n\n\nQuestions\n\nDiscuss whether the subjects in this study are likely to represent a larger population of interest. Are the conclusions relevant to the general population of noncolorblind individuals?\nFit a model including the effects of:\n\n\nTwo blocking factors (Subject and Time Order)\nThe treatment effect\nThe treatment × time-order interaction.\n\nVerify that the interaction (adjusted for subjects) can be measured with \\((v-1)^2 = 64\\) degrees of freedom.\n\nExamine whether the residuals are approximately normally distributed and whether variances are homogeneous across treatments.\n\nConsider transformations of the response variable, such as:\n\nOriginal response: “Guessed number”\n“Square root of guessed number”\n“(True number − guessed number) / (True number)”\nOther appropriate transformations.\n\n\nPresent an analysis of variance table and test any relevant hypotheses. Summarize your conclusions.\nRewrite the model in terms of main effects and interactions of the two treatment factors (Number and Color). Redo the analysis of variance and interpret the results.\nPropose a plan for a follow-up experiment, including a checklist of key considerations."
  },
  {
    "objectID": "assignments/assignment10_row_column_designs.html#grading-allocation",
    "href": "assignments/assignment10_row_column_designs.html#grading-allocation",
    "title": "Assignment 10: Row and Column Designs",
    "section": "Grading Allocation",
    "text": "Grading Allocation\n\n\n\nExercise Title\nPart\nGrade Weight (%)\n\n\n\n\nLatin Squares\n(a)\n5\n\n\n\n(b)\n5\n\n\nVideo Game Experiment\n(a)\n10\n\n\n\n(b)\n10\n\n\n\n(c)\n15\n\n\n\n(d)\n10\n\n\n\n(e)\n20\n\n\n\n(f)\n10\n\n\nQuantity Perception\n(a)\n5\n\n\n\n(b)\n10\n\n\n\n(c)\n10\n\n\n\n(d)\n15\n\n\n\n(e)\n15\n\n\n\n(f)\n10"
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html",
    "href": "assignments/assignment1_basics_of_experimental_design.html",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "",
    "text": "Objective: Demonstrate understanding of key concepts in experimental design and apply them to real-world scenarios.\nSubmission: Create a document that addresses the prompts below. Submit your responses as a PDF or Markdown file.\nFormat: Use clear headings and subheadings to organize your responses. Include any code or calculations as needed.\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#instructions",
    "href": "assignments/assignment1_basics_of_experimental_design.html#instructions",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "",
    "text": "Objective: Demonstrate understanding of key concepts in experimental design and apply them to real-world scenarios.\nSubmission: Create a document that addresses the prompts below. Submit your responses as a PDF or Markdown file.\nFormat: Use clear headings and subheadings to organize your responses. Include any code or calculations as needed.\nTotal points: 100"
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-1-key-concepts-of-experimental-design",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-1-key-concepts-of-experimental-design",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 1: Key Concepts of Experimental Design",
    "text": "Part 1: Key Concepts of Experimental Design\n\nDefining Objectives:\n\nWhat are the primary reasons for conducting experiments as outlined in Chapter 1? Provide an example for each purpose.\n\nExperimental vs. Observational Studies:\n\nExplain the differences between experimental and observational studies, emphasizing their strengths and limitations in establishing causality. Use the factory machine quality example in Chapter 1 for illustration.\n\nBasic Techniques:\n\nDescribe replication, blocking, and randomization. For each technique, provide an example that demonstrates how it improves the validity or precision of an experiment."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-2-checklist-for-planning-experiments",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-2-checklist-for-planning-experiments",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 2: Checklist for Planning Experiments",
    "text": "Part 2: Checklist for Planning Experiments\n\nDefining Objectives:\n\nList and briefly describe the steps from Chapter 2’s checklist for planning experiments. Emphasize why defining objectives and identifying sources of variation are critical.\n\nPilot Experiments:\n\nDescribe the purpose of running a pilot experiment and how it aids in refining the experimental design.\n\nHypothetical Scenario:\n\nImagine you are designing an experiment to test the effects of different fertilizers (F1, F2, F3) on plant growth. Outline your approach to:\n\nDefine objectives.\nIdentify sources of variation.\nChoose an assignment rule for treatments."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-3-real-world-applications",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-3-real-world-applications",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 3: Real-World Applications",
    "text": "Part 3: Real-World Applications\n\nCotton-Spinning Experiment (Chapter 2):\n\nSummarize the objectives and the experimental design used in the cotton-spinning experiment.\nExplain how blocking was used to account for variability and improve precision.\n\nImprovements:\n\nPropose one way to improve the cotton-spinning experiment for broader applicability or reduced cost."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#part-4-numerical-application",
    "href": "assignments/assignment1_basics_of_experimental_design.html#part-4-numerical-application",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Part 4: Numerical Application",
    "text": "Part 4: Numerical Application\nYou are tasked with analyzing a hypothetical dataset to practice experimental design concepts. Use the following dataset where three treatments (A, B, C) are applied to five blocks (1–5). The response variable represents a numerical outcome.\n\n\n\nBlock\nTreatment A\nTreatment B\nTreatment C\n\n\n\n\n1\n10.1\n9.8\n10.2\n\n\n2\n10.5\n9.7\n10.3\n\n\n3\n10.3\n10.1\n10.4\n\n\n4\n10.4\n9.9\n10.0\n\n\n5\n10.2\n10.0\n10.1\n\n\n\n\nStatistical Model:\n\nWrite the model equation that represents the relationship between treatment and response while accounting for blocks.\n\nAnalysis:\n\nConduct a one-way ANOVA using R to test for treatment differences. Include R code and output.\n\nInterpretation:\n\nState the null and alternative hypotheses.\nSummarize the ANOVA results, including whether to reject the null hypothesis.\n\nDiagnostics:\n\nPerform residual analysis to evaluate the assumptions of ANOVA."
  },
  {
    "objectID": "assignments/assignment1_basics_of_experimental_design.html#grading-rubric",
    "href": "assignments/assignment1_basics_of_experimental_design.html#grading-rubric",
    "title": "Assignment 1: Introduction to Experimental Design",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\n\n\n\n\n\nSection\nPoints\nCriteria\n\n\n\n\nExperiment Objectives\n10\n5 points for accurately listing two objectives, 5 points for providing correct examples.\n\n\nObservational Studies vs. Experiments\n15\n8 points for correctly explaining differences, 7 points for a clear and relevant example.\n\n\nDesign Your Own Experiment\n30\n10 points each for objectives, sources of variation, and assignment rule. Clear reasoning required.\n\n\nExperiments 1 and 7\n20\n10 points per experiment: 5 for objectives, 5 for sources of variation and treatment factors.\n\n\nExperiment 4\n25\n10 points for listing sources of variation, 10 for categorization and controllability, 5 for blocking.\n\n\nTotal\n100"
  },
  {
    "objectID": "contents.html",
    "href": "contents.html",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "This page organizes the key course materials, including lecture notes, assignments, supplemental readings, and important resources.\n\n\nThe course syllabus contains all essential details about the course structure, policies, assignments, and exam dates.\n\n📄 Download Full Course Syllabus (PDF)\nImportant: You must read the syllabus carefully, as it contains critical information not available elsewhere.\n\n\n\n\nLecture notes and slides will be posted on the Course GitHub Page in HTML format.\n\n📚 Access Lecture Notes\n\nNote: GitHub Pages may cache content. Refresh your browser if the notes appear outdated.\n\nTo refresh, use:\n\nCtrl + Shift + R (Windows/Linux)\n\nCmd + Shift + R (Mac)\n\n\n\n\n\n\n\nThe course schedule provides a week-by-week breakdown of topics, reading assignments, and due dates.\n\n📅 View Course Schedule\n\n\n\n\nAssignments will be posted on Canvas and must be submitted via Canvas. All assignments require the use of the Quarto template provided.\n\n🔗 Quarto Template\n\n\n\n\nSupplemental readings will be provided as needed. Public domain readings will be posted on the Contents page.\n\n🔗 Access Supplemental Readings on Canvas\n\nNote: Some readings may not be shared due to copyright restrictions.\n\n\n\nTo complete assignments and analyses, you will need access to R and related tools.\n\nR Installation: Download R\n\nRStudio IDE: Download RStudio\n\nVS Code IDE: Download VS Code\n\n\n\n\nQuarto Documentation: Learn Quarto\n\nR Tutorials: R Basics for Beginners"
  },
  {
    "objectID": "contents.html#course-syllabus",
    "href": "contents.html#course-syllabus",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "The course syllabus contains all essential details about the course structure, policies, assignments, and exam dates.\n\n📄 Download Full Course Syllabus (PDF)\nImportant: You must read the syllabus carefully, as it contains critical information not available elsewhere."
  },
  {
    "objectID": "contents.html#lecture-notes",
    "href": "contents.html#lecture-notes",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "Lecture notes and slides will be posted on the Course GitHub Page in HTML format.\n\n📚 Access Lecture Notes\n\nNote: GitHub Pages may cache content. Refresh your browser if the notes appear outdated.\n\nTo refresh, use:\n\nCtrl + Shift + R (Windows/Linux)\n\nCmd + Shift + R (Mac)"
  },
  {
    "objectID": "contents.html#course-schedule",
    "href": "contents.html#course-schedule",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "The course schedule provides a week-by-week breakdown of topics, reading assignments, and due dates.\n\n📅 View Course Schedule"
  },
  {
    "objectID": "contents.html#assignments",
    "href": "contents.html#assignments",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "Assignments will be posted on Canvas and must be submitted via Canvas. All assignments require the use of the Quarto template provided.\n\n🔗 Quarto Template"
  },
  {
    "objectID": "contents.html#supplemental-readings",
    "href": "contents.html#supplemental-readings",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "Supplemental readings will be provided as needed. Public domain readings will be posted on the Contents page.\n\n🔗 Access Supplemental Readings on Canvas\n\nNote: Some readings may not be shared due to copyright restrictions."
  },
  {
    "objectID": "contents.html#software-tools-and-resources",
    "href": "contents.html#software-tools-and-resources",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "To complete assignments and analyses, you will need access to R and related tools.\n\nR Installation: Download R\n\nRStudio IDE: Download RStudio\n\nVS Code IDE: Download VS Code\n\n\n\n\nQuarto Documentation: Learn Quarto\n\nR Tutorials: R Basics for Beginners"
  },
  {
    "objectID": "contents.html#r-data-files-for-design-and-analysis-of-experiments",
    "href": "contents.html#r-data-files-for-design-and-analysis-of-experiments",
    "title": "Course Content - STAT 454/545",
    "section": "R Data Files for Design and Analysis of Experiments",
    "text": "R Data Files for Design and Analysis of Experiments\nThese data files correspond to the book Design and Analysis of Experiments by Angela Dean, Dan Voss, and Daniel Draguljic (Springer-Verlag, 2017).\nClick on the links below to download the files."
  },
  {
    "objectID": "contents.html#a",
    "href": "contents.html#a",
    "title": "Course Content - STAT 454/545",
    "section": "A",
    "text": "A\n\nabrasive.wear.txt\nacid copper.txt\nair.freshener.txt\nair.rifle.txt\nair.velocity.txt\nair.velocity.contrasts.txt\nalcohol.txt\nammunition.txt\nanatase.txt\nantifungal.txt"
  },
  {
    "objectID": "contents.html#b",
    "href": "contents.html#b",
    "title": "Course Content - STAT 454/545",
    "section": "B",
    "text": "B\n\nballoon.txt\nbanana.txt\nbattery.txt\nbean.txt\nbeef.txt\nbicycle.txt\nbiscuit.txt\nbleach.txt\nbuttermilk.txt"
  },
  {
    "objectID": "contents.html#c",
    "href": "contents.html#c",
    "title": "Course Content - STAT 454/545",
    "section": "C",
    "text": "C\n\ncaffeine.txt\ncandle.txt\ncatalyst.txt\ncatalytic.reaction.txt\nchemical.txt\ncigarette.txt\nclean.wool.txt\ncoating.txt\ncoil.txt\ncolorfastness.txt"
  },
  {
    "objectID": "contents.html#d",
    "href": "contents.html#d",
    "title": "Course Content - STAT 454/545",
    "section": "D",
    "text": "D\n\ndairy.cow.txt\nDCIS.txt\ndecon.alpha.txt\ndecon.beta1.txt\ndecon.beta2.txt\ndessert.txt\ndetergent.txt\ndrill.advance.txt\ndrug.txt\ndye.txt"
  },
  {
    "objectID": "contents.html#e",
    "href": "contents.html#e",
    "title": "Course Content - STAT 454/545",
    "section": "E",
    "text": "E\n\neffervescent.txt\nexercise.bicycle.txt"
  },
  {
    "objectID": "contents.html#f",
    "href": "contents.html#f",
    "title": "Course Content - STAT 454/545",
    "section": "F",
    "text": "F\n\nfabric.stain.txt\nfield.txt\nfield2.txt\nfilm.viscosity.txt\nfishing.line.txt\nflour.txt\nflour.early.txt\nflour.production.txt\nfractionation.PCE.txt\nfractionation.yield.txt"
  },
  {
    "objectID": "contents.html#g",
    "href": "contents.html#g",
    "title": "Course Content - STAT 454/545",
    "section": "G",
    "text": "G\n\nglass.txt\ngolf.txt"
  },
  {
    "objectID": "contents.html#h",
    "href": "contents.html#h",
    "title": "Course Content - STAT 454/545",
    "section": "H",
    "text": "H\n\nheat.txt\nhelicopter.txt"
  },
  {
    "objectID": "contents.html#i",
    "href": "contents.html#i",
    "title": "Course Content - STAT 454/545",
    "section": "I",
    "text": "I\n\nice.cream.txt\ninclinometer.product.txt\nink.txt"
  },
  {
    "objectID": "contents.html#l",
    "href": "contents.html#l",
    "title": "Course Content - STAT 454/545",
    "section": "L",
    "text": "L\n\nLeatherette.txt"
  },
  {
    "objectID": "contents.html#m",
    "href": "contents.html#m",
    "title": "Course Content - STAT 454/545",
    "section": "M",
    "text": "M\n\nMCFS71.txt\nmeatball.txt\nmetal.txt\nmetal.powder.txt\nmungbean.txt"
  },
  {
    "objectID": "contents.html#n",
    "href": "contents.html#n",
    "title": "Course Content - STAT 454/545",
    "section": "N",
    "text": "N\n\nneuron.txt"
  },
  {
    "objectID": "contents.html#o",
    "href": "contents.html#o",
    "title": "Course Content - STAT 454/545",
    "section": "O",
    "text": "O\n\noats.txt\noxidation.txt"
  },
  {
    "objectID": "contents.html#p",
    "href": "contents.html#p",
    "title": "Course Content - STAT 454/545",
    "section": "P",
    "text": "P\n\npah.txt\npaper.txt\npaper.strength.txt\nplasma.txt\npolymer.txt\nprotein.txt"
  },
  {
    "objectID": "contents.html#r",
    "href": "contents.html#r",
    "title": "Course Content - STAT 454/545",
    "section": "R",
    "text": "R\n\nrail.weld.txt\nreaction.time.txt\nrice.txt"
  },
  {
    "objectID": "contents.html#s",
    "href": "contents.html#s",
    "title": "Course Content - STAT 454/545",
    "section": "S",
    "text": "S\n\nsensor.txt\nshrinkage.txt\nsludge.txt\nsoap.txt\nsoftdrink.txt\nsolder.txt\nspinning.txt\nstarch.txt\nsteam.txt"
  },
  {
    "objectID": "contents.html#t",
    "href": "contents.html#t",
    "title": "Course Content - STAT 454/545",
    "section": "T",
    "text": "T\n\ntemperature.txt\ntire.txt\ntomato.txt\ntrout.txt"
  },
  {
    "objectID": "contents.html#u",
    "href": "contents.html#u",
    "title": "Course Content - STAT 454/545",
    "section": "U",
    "text": "U\n\nuav.txt\nuav3.txt"
  },
  {
    "objectID": "contents.html#v",
    "href": "contents.html#v",
    "title": "Course Content - STAT 454/545",
    "section": "V",
    "text": "V\n\nvalve.txt\nvoltage.txt"
  },
  {
    "objectID": "contents.html#w",
    "href": "contents.html#w",
    "title": "Course Content - STAT 454/545",
    "section": "W",
    "text": "W\n\nwafer.txt\nwater.txt\nwelding.txt\nwheel.txt\nwindshield.txt"
  },
  {
    "objectID": "home_appendix.html",
    "href": "home_appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "This appendix provides supplementary resources and support materials to help you succeed in the course."
  },
  {
    "objectID": "home_appendix.html#r-help-and-documentation",
    "href": "home_appendix.html#r-help-and-documentation",
    "title": "Appendix",
    "section": "R Help and Documentation",
    "text": "R Help and Documentation\nThe following page provides detailed help and documentation for R programming:\n\nR Functions and Packages Used in the Course\n\nR help Page for the Course\n\nSubjects and Topics\nR Base Functions\nR Packages\n\n\nOnline R Help:\n\nCRAN R Documentation\nRStudio Cheat Sheets\n\nVideo Tutorials:\n\nR Programming for Beginners (YouTube)"
  },
  {
    "objectID": "home_appendix.html#navigating-the-website",
    "href": "home_appendix.html#navigating-the-website",
    "title": "Appendix",
    "section": "Navigating the Website",
    "text": "Navigating the Website\nIf you encounter any issues with outdated content on the website, try refreshing your browser:\n\nWindows/Linux: Press Ctrl + Shift + R.\nMac: Press Cmd + Shift + R.\n\nLet the instructor know if you have additional suggestions for materials to include in this appendix."
  },
  {
    "objectID": "syllabus/syllabus-final.html",
    "href": "syllabus/syllabus-final.html",
    "title": "Course Syllabus - STAT 454/545",
    "section": "",
    "text": "Instructor: Dr. Davood Tofighi\nLocation: Science Math Learning Center 120\nTime: Tuesday & Thursday, 2:00 pm – 3:15 pm\nOffice Hours: By appointment\nCourse Website:\n\nUNM Canvas\nhttps://Data-Wise.github.io/doe/."
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-information",
    "href": "syllabus/syllabus-final.html#course-information",
    "title": "Course Syllabus - STAT 454/545",
    "section": "",
    "text": "Instructor: Dr. Davood Tofighi\nLocation: Science Math Learning Center 120\nTime: Tuesday & Thursday, 2:00 pm – 3:15 pm\nOffice Hours: By appointment\nCourse Website:\n\nUNM Canvas\nhttps://Data-Wise.github.io/doe/."
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-description",
    "href": "syllabus/syllabus-final.html#course-description",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Course Description",
    "text": "Course Description\nThis course provides a rigorous introduction to experimental design and analysis of variance (ANOVA) methodologies, with an emphasis on their integration into linear modeling frameworks. Topics include fundamental principles of experimental design (randomization, replication, and blocking), handling unbalanced data, diagnostics for model assumptions, and advanced designs such as split-plot, nested, and mixed-effects models. Students will develop skills in designing experiments, selecting appropriate statistical approaches, analyzing data using R, and communicating findings effectively."
  },
  {
    "objectID": "syllabus/syllabus-final.html#learning-outcomes",
    "href": "syllabus/syllabus-final.html#learning-outcomes",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this course, students will be able to:\n\nDesign experiments using principles of randomization, replication, and blocking.\nImplement one-way, two-way, and higher-order factorial ANOVA techniques.\nAnalyze unbalanced data using modern statistical methods.\nConduct residual analysis, transformations, and diagnostics for model assumptions.\nPerform multiple comparisons and construct contrasts for treatment effect interpretation.\nIncorporate covariates into ANOVA frameworks through analysis of covariance (ANCOVA).\nApply advanced designs like row–column, nested, and split-plot experiments.\nUtilize statistical software (e.g., R) to perform experimental analyses and generate reproducible workflows."
  },
  {
    "objectID": "syllabus/syllabus-final.html#prerequisite",
    "href": "syllabus/syllabus-final.html#prerequisite",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Prerequisite",
    "text": "Prerequisite\nA prior course in statistics, such as STAT 440/550, is strongly recommended. Students without this background must meet with the instructor to discuss their preparation."
  },
  {
    "objectID": "syllabus/syllabus-final.html#textbook-and-software",
    "href": "syllabus/syllabus-final.html#textbook-and-software",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Textbook And Software",
    "text": "Textbook And Software\n\nTextbook: Dean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\n\nAvailable as a free eBook through the UNM Library. Use your UNM credentials to access it.\n\nSoftware:\n\nR (version 4.2 or higher), paired with RStudio, VS Code, or Positron, along with Quarto for reproducible workflows.\nKey R packages: lm, emmeans, multcomp, lme4, and others as introduced during the course."
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-resources",
    "href": "syllabus/syllabus-final.html#course-resources",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Course Resources",
    "text": "Course Resources\n1. Lecture Notes and Online Resources\n\nLecture notes and online tutorials will be made available in HTML/PDF format on the dedicated GitHub Pages website for this course. This platform ensures easy access to interactive and organized course materials.\nGitHub Pages Website Address: https://Data-Wise.github.io/doe/.\nThe GitHub Pages website will be regularly updated with new materials. However, please note that:\n\nSome supplemental readings may not be available on the website due to copyright restrictions.\nOnly supplemental readings in the public domain or with proper permissions will be provided on GitHub Pages.\n\n\nImportant Note about GitHub Pages and Browser Caching\n\nGitHub Pages occasionally caches content, which may result in outdated materials being displayed. To ensure you are viewing the latest updates:\nRefresh your browser: Press Ctrl + F5 (Windows) or Cmd + Shift + R (Mac) for a hard refresh.\nClear your browser cache: If refreshing doesn’t work, clear your browser’s cache in your browser’s settings and revisit the website.\nUse an Incognito Window: Opening the website in an incognito or private browsing window can bypass cached content.\n\n2. Canvas Course Platform\n\nAll other course materials, such as:\nHomework assignments\nQuarto templates\nSubmission portals\nSupplemental readings not in the public domain\nAnnouncements\nDiscussion boards\n\nwill be hosted on Canvas.\n\nStudents are responsible for checking Canvas regularly for important updates, announcements, and access to restricted course materials.\n\n3. Supplemental Readings\n\nSupplemental readings will be provided through Canvas whenever possible. However:\nReadings under copyright or without appropriate permissions will not be distributed directly.\nWhere applicable, references or external links will be shared for you to access materials through UNM’s library system or other authorized platforms.\n\nContact for Technical Issues\n\nIf you encounter persistent issues accessing materials on GitHub Pages or Canvas, please notify the instructor or TA immediately for assistance.\n\nBy leveraging GitHub Pages for publicly available materials and Canvas for copyright-restricted or logistical resources, the course provides a seamless blend of accessibility and compliance."
  },
  {
    "objectID": "syllabus/syllabus-final.html#communication-guidelines",
    "href": "syllabus/syllabus-final.html#communication-guidelines",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Communication Guidelines",
    "text": "Communication Guidelines\n\nAnnouncements: All important course announcements will be posted on Canvas. It is your responsibility to regularly check Canvas to stay updated. Missing important information due to not checking Canvas will not be considered an acceptable excuse.\nDiscussion/Questions: For course-related questions, post them on the Canvas discussion board. Do not email me or the TA directly for general questions about the course. Before posting, review the syllabus to ensure your question hasn’t already been addressed. Questions that are already answered in the syllabus will not receive a response.\nEmail: All email communication must be sent through Canvas. Direct emails to my university or personal email address will not receive a response. Please ensure that all personal or sensitive matters requiring email communication are sent via Canvas.\nPersonal Matters: For personal concerns, contact me only through Canvas email or visit during office hours.\nOffice Hours (by Appointment): Please feel free to schedule an appointment with me or the TA if you need additional help or clarification. Office hours are a great opportunity to discuss your questions and ensure your understanding of the material.\n\nReminder: Checking Canvas regularly is essential for staying informed and up-to-date with the course."
  },
  {
    "objectID": "syllabus/syllabus-final.html#grading-and-assessments",
    "href": "syllabus/syllabus-final.html#grading-and-assessments",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Grading And Assessments",
    "text": "Grading And Assessments\nThe final grade in this course will be determined by the following components and their respective weights:\n\n\n\nComponent\nWeight\n\n\n\n\nHomework Assignments\n50%\n\n\nMidterm Exam\n20%\n\n\nFinal Exam\n30%\n\n\nExtra Credit\nUp to 4%\n\n\n\n\nGrading Scale\nThe following cut-off values will be used to assign final letter grades:\n\n\n\nPercentage\nLetter Grade\n\n\n\n\n98–100\nA+\n\n\n93–97.99\nA\n\n\n90–92.99\nA-\n\n\n87–89.99\nB+\n\n\n83–86.99\nB\n\n\n80–82.99\nB-\n\n\n77–79.99\nC+\n\n\n73–76.99\nC\n\n\n70–72.99\nC-\n\n\n67–69.99\nD+\n\n\n63–66.99\nD\n\n\n60–62.99\nD-\n\n\nBelow 60\nF\n\n\n\n\n\nHomework (50%)\n\nHomework Submission Guidelines\n\nFormat and Template:\nAll homework assignments must be completed using the Quarto template provided on the course website. You are required to submit the rendered PDF version of the Quarto document. Failure to use the provided template or submit the correct format will result in point deductions.\nSubmission Platform:\nSubmit your completed assignments electronically through Canvas. Navigate to the “Assignments” folder, click on the appropriate homework assignment, and attach your rendered PDF file.\nPlagiarism Check:\nAll submitted assignments will be automatically checked for plagiarism by Canvas. Any instances of plagiarism or academic dishonesty will be handled in accordance with UNM’s Student Code of Conduct and the Support in Receiving Help and Doing What is Right section outlined in this syllabus. Please familiarize yourself with the academic integrity policies detailed in the syllabus and on the UNM website. Ignorance of the policy is not an excuse for violations.\nNo Late Submissions Accepted:\nAll homework assignments are due by the specified deadline on Canvas. Late submissions will not be accepted under any circumstances. This policy is strict and applies to all students without exceptions.\nExtra Credit Opportunity:\nIf you miss an assignment, you may compensate for it by earning extra credit as outlined in the syllabus. This provides an opportunity to recover points without requiring extensions or exceptions for missed deadlines.\nDo Not Email About Late Assignments:\nPlease do not email me or the TA regarding missed or late homework submissions. We cannot make exceptions or accept late assignments under any conditions. Instead, focus on utilizing the extra credit opportunities provided in the syllabus.\nContent Requirements:\n\nAll responses must be typed within the Quarto document.\nIf you include hand-written calculations or derivations, they must be scanned and appended to the Quarto document as an appendix before rendering the final PDF.\n\nIndependence:\nAll homework assignments must be completed independently. While collaboration in studying is encouraged, your final submission must reflect your own work.\nGrading and Feedback:\nAssignments will be graded and feedback will be provided through Canvas. Be sure to review comments and feedback promptly to address any misunderstandings before the next assignment.\n\nImportant Note: To avoid missing deadlines, plan your work carefully and submit your assignments early. If you are unfamiliar with Quarto or need assistance using the template, refer to the “Resources” section on Canvas for tutorials or contact me during office hours.\n\n\n\nExams (50%)\nMidterm Exam (20%)\n\nDate: Thursday, March 6, 2025 (during class time)\nLocation: Classroom (as specified in the syllabus)\nContent: Covers topics from Weeks 1–7.\n\nFinal Exam (30%)\n\nDate: Tuesday, May 13, 2025, from 10:00 a.m. to 12:00 p.m.\nLocation: Classroom (as specified in the syllabus)\nContent: Comprehensive, covering the entire course.\n\nExam Guidelines\n\nFormat: Exams are closed book but you may bring up to three double-sided cheat sheets (US Letter size) and a calculator. No other electronics or materials (e.g., books, lecture notes) are allowed.\nQuestion Types: Includes a combination of short-answer questions, computations, and application-based problems.\n\nNo Make-Up Exams Policy\n\nMake-up exams will only be granted in cases of proven emergency that meet the criteria set forth by the Department of Mathematics and Statistics and UNM policies (e.g., documented medical emergencies, official university-sponsored activities, or other unavoidable situations as defined by university guidelines).\nIf you anticipate a potential conflict or emergency, you must notify the instructor in advance of the exam date whenever possible. Failure to do so will result in forfeiture of the opportunity to take a make-up exam.\nFor non-emergency situations, personal conflicts, or reasons outside of those outlined in this document (e.g., vacation, workload, or personal oversight), make-up exams will not be permitted.\n\nProven Emergency Requirements:\n\nMedical emergencies: Provide a signed note from a licensed healthcare provider explaining the need for absence.\nUniversity-sponsored activities: Submit official documentation from the relevant department.\nOther emergencies: Submit verifiable proof (e.g., legal documentation, official notifications).\n\nIn all cases, documentation must be provided as soon as possible and must align with UNM’s student policies on academic integrity and attendance. For more information, consult the UNM Student Handbook: https://pathfinder.unm.edu.\nImportance of Timely Notification\nIf you foresee a conflict or are unable to attend an exam due to an emergency, notify the instructor as early as possible. While advance notice does not guarantee a make-up exam, failing to provide notice will disqualify you from consideration for a make-up opportunity.\nBy adhering to this policy, we ensure fairness and consistency for all students while maintaining compliance with university guidelines. If you have further questions or concerns, please contact the instructor before the exam dates.\n\n\nExtra Credit Opportunities\nStudents have the opportunity to earn up to 4% extra credit toward their final grade by completing the following tasks:\n1. End-of-Semester Course Evaluation (Up to 3%)\n\nStudents who complete the end-of-semester course evaluation will receive up to 3% extra credit.\nTo receive the extra credit, students must:\nSubmit the course evaluation through the university’s designated system (e.g., UNM’s course evaluation platform).\nPost a screenshot or copy of the receipt of submission (no personal information visible) to the “Course Evaluation” discussion board on Canvas.\nDeadline for posting the receipt will be announced during the final week of class.\n\n2. Background and Syllabus Quiz (1%)\n\nA “Background and Syllabus Quiz” will be available on the course website during the first week of class.\nThis quiz will assess familiarity with the syllabus and the course’s policies and expectations. Completing the quiz will earn 1% extra credit.\nNote: The availability of this quiz may vary by semester and will be announced at the start of the course.\n\nImportant Notes:\n\nThese extra credit opportunities are designed to encourage engagement with the course policies and participation in university processes.\nExtra credit is optional and cannot replace or make up for missed assignments, exams, or other coursework.\nThe maximum total extra credit available is 4%, which will be applied to your final grade at the end of the semester.\nStudents are encouraged to review the “Grading and Assessments” section of the syllabus to understand how extra credit impacts their overall grade.\n\nPlease take advantage of these opportunities to improve your final grade while reinforcing your understanding of the course and contributing to constructive course feedback."
  },
  {
    "objectID": "syllabus/syllabus-final.html#what-to-expect-and-how-to-study-and-be-successful-in-this-course",
    "href": "syllabus/syllabus-final.html#what-to-expect-and-how-to-study-and-be-successful-in-this-course",
    "title": "Course Syllabus - STAT 454/545",
    "section": "What to Expect and How to Study and Be Successful in This Course",
    "text": "What to Expect and How to Study and Be Successful in This Course\nThis course is designed to be both rigorous and rewarding, equipping you with essential skills in experimental design and analysis of variance (ANOVA). Success in this course requires preparation, consistent effort, and active engagement with the material. Below are some key recommendations and expectations to help you succeed:\n1. Expect to Invest Significant Time\n\nMastery of the material requires dedicated study time each week. Plan to spend at least 8-10 hours per week outside of class on:\nReading assigned materials.\nCompleting homework assignments.\nPracticing statistical techniques in R.\nReviewing notes and working through practice problems.\n\n2. Prioritize Class Attendance\n\nAttendance is critical to your success in this course.\nClasses are designed to build upon each other, with cumulative content.\nDiscussions, group activities, and real-time problem-solving in class will reinforce your understanding.\nHistorical data has shown that students who miss class sessions struggle to keep up with the material.\nIf you must miss a class, review the lecture notes and course materials promptly, and reach out to classmates for additional support.\n\n3. Come Prepared for Every Class\n\nRead the assigned materials before class:\nFamiliarize yourself with the key topics to be covered.\nJot down questions or areas of confusion to bring to class discussions.\nEngage with supplemental materials like videos, tutorials, or examples provided on Canvas.\nStay organized by keeping track of due dates, reading assignments, and planned lectures using the course schedule.\n\n4. Stay Active During Class\n\nTake detailed notes during lectures and participate in discussions.\nActively engage in group activities. Collaborating with classmates helps deepen your understanding and clarifies difficult concepts.\nAsk questions during class or post them on Canvas if you need further clarification.\n\n5. Utilize Office Hours and Discussion Boards\n\nOffice hours are an excellent opportunity for one-on-one assistance with:\nUnderstanding difficult concepts.\nReviewing homework problems.\nDiscussing your specific challenges with course material.\nUse the Canvas Discussion Boards for peer-to-peer support and quick clarifications from the instructor or TA.\n\n6. Practice Regularly\n\nMastery of statistical methods requires consistent practice.\nWork through examples provided in class and additional exercises from the textbook.\nAttempt problems in R to solidify your understanding of statistical tools and software.\n\n7. Review and Revise\n\nAfter each class, set aside time to:\nReview your notes.\nSummarize key concepts in your own words.\nAttempt practice problems to apply what you’ve learned.\n\n8. Focus on Understanding, Not Memorization\n\nThis course emphasizes application and critical thinking.\nStrive to understand the “why” behind statistical techniques and experimental designs.\nApply concepts to real-world scenarios or examples from your own field of study.\n\n9. Manage Your Time Wisely\n\nStart assignments early to avoid last-minute stress.\nBreak down large tasks into smaller steps and set achievable goals for each study session.\nUse tools like planners or digital calendars to track deadlines and manage your workload effectively.\n\n10. Maintain a Growth Mindset\n\nStatistics can be challenging, but persistence is key.\nEmbrace mistakes as part of the learning process.\nReach out for help early if you’re struggling with a concept or assignment.\n\n11. Make Use of the Resources Provided\n\nThe course provides several resources to support your learning:\nA Quarto template for homework submissions.\nSupplemental readings, videos, and tutorials.\nAccess to the course textbook through UNM’s library system.\nR and VS Code or RStudio (or equivalent IDEs) for conducting statistical analyses.\n\n12. Avoid Falling Behind\n\nGiven the cumulative nature of this course, falling behind on one topic will make subsequent topics more difficult to grasp.\nStay on top of assignments and make reviewing previous material part of your weekly routine.\n\n13. Academic Honesty and Collaboration\n\nWhile collaboration is encouraged during study sessions or discussions, ensure all submitted work reflects your own understanding.\nFamiliarize yourself with the Support In Receiving Help and Doing What is Right section of the syllabus to avoid issues related to academic dishonesty.\n\nBy following these recommendations, staying engaged, and managing your time wisely, you’ll position yourself for success in this course. Remember, learning statistics is a journey that requires consistent effort and a positive attitude. I am here to support you every step of the way, so don’t hesitate to reach out for assistance."
  },
  {
    "objectID": "syllabus/syllabus-final.html#accommodation",
    "href": "syllabus/syllabus-final.html#accommodation",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Accommodation",
    "text": "Accommodation\nIn accordance with University Policy 2310 and the Americans with Disabilities Act (ADA), academic accommodations will be provided for students who notify the instructor of their need. It is the student’s responsibility to inform the instructor promptly so accommodations can be arranged. Instructors are not legally permitted to inquire about disabilities or the need for accommodations. For assistance in arranging accommodations, contact the Accessibility Resource Center (ARC):\n\nPhone: 277-3506\nEmail: arcsrvs@unm.edu\nWebsite: Accessibility Resource Center\n\nStudents who may require assistance in emergency evacuations should also contact the instructor to discuss the most appropriate procedures to follow.\nUNM is dedicated to fostering an inclusive, accessible, and supportive learning environment for all participants. If you encounter physical, academic, or mental health barriers, or concerns related to physical health, mental health, or COVID-19, you are encouraged to communicate these concerns to the instructor via Canvas email, phone, or during office hours. Additional support is available through the Accessibility Resource Center. Let’s work together to ensure you have equitable access to this course."
  },
  {
    "objectID": "syllabus/syllabus-final.html#title-ix",
    "href": "syllabus/syllabus-final.html#title-ix",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Title IX",
    "text": "Title IX\nAs part of UNM’s commitment to a safe and equitable campus environment, faculty, Teaching Assistants (TAs), and Graduate Assistants (GAs) are considered “responsible employees” by the Department of Education. This means that any report of gender discrimination—including sexual harassment, misconduct, or violence—shared with a faculty member, TA, or GA must be reported to the Title IX Coordinator at the Office of Equal Opportunity (OEO).\n\nOffice of Equal Opportunity Website: https://oeo.unm.edu\nTitle IX policy details: https://policy.unm.edu/university-policies/2000/2740.html\n\nFor more information on reporting obligations, see the Title IX Guidelines (https://www2.ed.gov/about/offices/list/ocr/docs/qa-201404-title-ix.pdf). If you prefer to discuss sensitive concerns confidentially, please consider reaching out to the UNM LoboRESPECT Advocacy Center or other confidential resources on campus."
  },
  {
    "objectID": "syllabus/syllabus-final.html#support-in-receiving-help-and-doing-what-is-right",
    "href": "syllabus/syllabus-final.html#support-in-receiving-help-and-doing-what-is-right",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Support In Receiving Help and Doing What is Right",
    "text": "Support In Receiving Help and Doing What is Right\nUNM provides a range of services and policies to help students succeed academically and thrive within the university community. If you are unsure where to begin, consult the UNM Student Services website (https://students.unm.edu) or reach out to the instructor for guidance in identifying the right resource.\nKey Policies and Support Resources:\n\nStudent Grievances: Policies D175 and D176 in the Faculty Handbook detail procedures for addressing grievances.\nAcademic Dishonesty: Adhere to UNM’s policies outlined in the Faculty Handbook D100, which address plagiarism and academic misconduct.\nRespectful Campus Policy: Uphold UNM’s guidelines for fostering a respectful academic environment (Faculty Handbook C09).\n\nFind these resources in the Student Pathfinder (https://pathfinder.unm.edu) or the Faculty Handbook (https://handbook.unm.edu).\nPlagiarism and Academic Dishonesty: Violations of academic integrity have serious consequences, including failing an assignment or course, and potential suspension or expulsion. If you have any questions about avoiding plagiarism, please ask the instructor or consult UNM’s policies. It is always better to ask for clarification than to risk committing an academic violation.\nIf you need assistance understanding the guidelines or accessing these resources, please do not hesitate to reach out."
  },
  {
    "objectID": "syllabus/syllabus-final.html#land-acknowledgement",
    "href": "syllabus/syllabus-final.html#land-acknowledgement",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Land Acknowledgement",
    "text": "Land Acknowledgement\nFounded in 1889, the University of New Mexico sits on the traditional homelands of the Pueblo of Sandia. The original peoples of New Mexico Pueblo, Navajo, and Apache since time immemorial, have deep connections to the land and have made significant contributions to the broader community statewide. We honor the land itself and those who remain stewards of this land throughout the generations and also acknowledge our committed relationship to Indigenous peoples. We gratefully recognize our history."
  },
  {
    "objectID": "syllabus/syllabus-final.html#course-schedule-assignment-deadlines-and-exam-dates",
    "href": "syllabus/syllabus-final.html#course-schedule-assignment-deadlines-and-exam-dates",
    "title": "Course Syllabus - STAT 454/545",
    "section": "Course Schedule, Assignment Deadlines, and Exam Dates",
    "text": "Course Schedule, Assignment Deadlines, and Exam Dates\nThe course schedule, including assignment due dates and the midterm exam date, is subject to change based on the progress of the class, unforeseen circumstances, or instructor discretion. Students are responsible for regularly checking Canvas and announcements on the course GitHub page for the most up-to-date information regarding deadlines and schedule adjustments.\nHowever, the final exam date is not subject to change, as it is determined and scheduled by the university. Please plan accordingly based on the final exam schedule provided by UNM:\n\nFinal Exam Date: Tuesday, May 13, 2025, 10:00 a.m.–12:00 p.m.\nLocation: Classroom (as specified in the syllabus)\n\nStudents are strongly encouraged to stay proactive, monitor changes, and ensure they meet all revised deadlines and expectations as announced.\n\nTentative Course Schedule\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopics\nReadings & Homework Assignments\n\n\n\n\n1\nJan. 21 & 23, 2025\nIntroduction to Experimental Design\nCh. 1 (Sections 1.1–1.1.1), Ch. 2 (Sections 2.1–2.2).\n\n\n2\nJan. 28 & 30, 2025\nCompletely Randomized Designs (CRD)\nCh. 3 (Sections 3.1–3.5); Homework 1 (due Feb. 6).\n\n\n3\nFeb. 4 & 6, 2025\nMultiple Comparisons\nCh. 4 (Sections 4.1–4.4); Homework 2 (due Feb. 13).\n\n\n4\nFeb. 11 & 13, 2025\nChecking Model Assumptions\nCh. 5 (Sections 5.1–5.3); Homework 3 (due Feb. 20).\n\n\n5\nFeb. 18 & 20, 2025\nTwo-Way Factorial Designs\nCh. 6 (Sections 6.1–6.5); Homework 4 (due Feb. 27).\n\n\n6\nFeb. 25 & 27, 2025\nHigher-Order Factorial Designs\nCh. 7 (Sections 7.1–7.6); Homework 5 (due Mar. 6).\n\n\n7\nMar. 4 & 6, 2025\nMidterm Exam\nReview all chapters and notes.\n\n\n8\nMar. 11 & 13, 2025\nBlocking & RCBD\nCh. 10 (Sections 10.1–10.4); Homework 6 (due Mar. 25).\n\n\n9\nMar. 16–20, 2025\n==Spring Break==\nNo classes.\n\n\n10\nMar. 25 & 27, 2025\nComplete Block Designs\nCh. 10 (Sections 10.6–10.8); Homework 7 (due Apr. 3).\n\n\n11\nApr. 1 & 3, 2025\nAnalysis of Covariance (ANCOVA)\nCh. 9 (Sections 9.1–9.4); Homework 8 (due Apr. 10).\n\n\n12\nApr. 8 & 10, 2025\nRow–Column (Latin square) Designs\nCh. 12 (Sections 12.1–12.4); Homework 9 (due Apr. 17).\n\n\n13\nApr. 15 & 17, 2025\nRandom & Mixed Models\nCh. 17 (Sections 17.1–17.8); Homework 10 (due Apr. 24).\n\n\n14\nApr. 22 & 24, 2025\nNested Models\nCh. 18 (Sections 18.1–18.4); Homework 11 (due May 1).\n\n\n15\nApr. 29 & May 1, 2025\nSplit-Plot Designs\nCh. 19 (Sections 19.1–19.5); Homework 12 (due May 6).\n\n\n16\nMay 6 & 8, 2025\nCatching Up and Review\nComprehensive review and Q&A.\n\n\n17\nMay 13\nFinal Exam\nComprehensive (Weeks 1–15)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html",
    "href": "lectures/week-03_comparisons-contrasts.html",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "In many experiments, researchers compare multiple treatments or conditions to understand their effects on a response variable. For example, consider a horticulturalist testing various fertilizers on plant growth or a biologist examining different drugs on patient outcomes. After performing an ANOVA and finding evidence that not all treatment means are equal, the next step is to determine which treatments differ, and by how much.\nMultiple Comparison Procedures (MCPs) and contrasts address this challenge:\n\nMCPs control the overall probability of false positives (Type I errors) when making many comparisons.\nContrasts focus on specific, pre-defined linear combinations of treatment means, often reflecting scientific questions.\n\nThe combination of MCPs and contrasts ensures valid inferences, mitigating the risk of drawing erroneous conclusions from multiple tests while retaining interpretability and power.\n\n\n\nUnderstand the rationale for performing multiple comparisons after ANOVA.\nExplore common MCPs: Fisher’s LSD, Tukey’s HSD, Bonferroni, and Scheffé methods.\nLearn how to construct orthogonal and non-orthogonal contrasts and interpret their results.\nUtilize R to perform multiple comparisons and test contrasts on real or simulated data.\nDiscuss Type I error control, power considerations, and the trade-offs in choosing different procedures.\nDevelop deeper theoretical insights (in the Appendix) and practice through exercises of medium and challenging difficulty.\n\n\n\n\nTo implement MCPs and contrasts in R, we will use the following packages:\n\npacman::p_load(emmeans, pwr, multcomp, agricolae, lsmeans, car)\n\n\nemmeans: For computing contrasts and post-hoc tests.\npwr: For power analysis and sample size calculations.\nmultcomp: For multiple comparison procedures.\nagricolae: For conducting multiple comparisons and post-hoc tests.\nlsmeans: For computing least-squares means and contrasts.\ncar: For linear contrasts and hypothesis testing.\n\n\n\n\nImagine testing four battery types (A, B, C, D) to see which lasts longest. After ANOVA suggests at least one mean differs, you want to pinpoint where differences lie:\n\nAre A and B different?\nIs C worse than the average of A and B?\nDoes D stand apart?\n\nWithout controlling for multiple testing, repeated pairwise comparisons inflate the risk of false positives. MCPs handle this by adjusting significance criteria, while contrasts let you test targeted hypotheses, such as comparing the average performance of two treatments against another group’s mean.\n\n\n\nMail Sorting Analogy: Consider sorting through a large pile of mail (treatment comparisons). Without rules (MCPs), you might mistakenly pick “important” letters (differences) that are actually junk (Type I errors). MCPs act like filters, reducing the chance you misclassify too many letters as important.\nMagnifying Glass for Specific Questions (Contrasts): Instead of looking at all letters at once, a contrast is like using a magnifying glass to focus on a subset of letters (treatment means) to answer a specific question."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#objectives",
    "href": "lectures/week-03_comparisons-contrasts.html#objectives",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Understand the rationale for performing multiple comparisons after ANOVA.\nExplore common MCPs: Fisher’s LSD, Tukey’s HSD, Bonferroni, and Scheffé methods.\nLearn how to construct orthogonal and non-orthogonal contrasts and interpret their results.\nUtilize R to perform multiple comparisons and test contrasts on real or simulated data.\nDiscuss Type I error control, power considerations, and the trade-offs in choosing different procedures.\nDevelop deeper theoretical insights (in the Appendix) and practice through exercises of medium and challenging difficulty."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#r-packages",
    "href": "lectures/week-03_comparisons-contrasts.html#r-packages",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "To implement MCPs and contrasts in R, we will use the following packages:\n\npacman::p_load(emmeans, pwr, multcomp, agricolae, lsmeans, car)\n\n\nemmeans: For computing contrasts and post-hoc tests.\npwr: For power analysis and sample size calculations.\nmultcomp: For multiple comparison procedures.\nagricolae: For conducting multiple comparisons and post-hoc tests.\nlsmeans: For computing least-squares means and contrasts.\ncar: For linear contrasts and hypothesis testing."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#motivation-and-context",
    "href": "lectures/week-03_comparisons-contrasts.html#motivation-and-context",
    "title": "Multiple Comparisons and Contrasts",
    "section": "",
    "text": "Imagine testing four battery types (A, B, C, D) to see which lasts longest. After ANOVA suggests at least one mean differs, you want to pinpoint where differences lie:\n\nAre A and B different?\nIs C worse than the average of A and B?\nDoes D stand apart?\n\nWithout controlling for multiple testing, repeated pairwise comparisons inflate the risk of false positives. MCPs handle this by adjusting significance criteria, while contrasts let you test targeted hypotheses, such as comparing the average performance of two treatments against another group’s mean.\n\n\n\nMail Sorting Analogy: Consider sorting through a large pile of mail (treatment comparisons). Without rules (MCPs), you might mistakenly pick “important” letters (differences) that are actually junk (Type I errors). MCPs act like filters, reducing the chance you misclassify too many letters as important.\nMagnifying Glass for Specific Questions (Contrasts): Instead of looking at all letters at once, a contrast is like using a magnifying glass to focus on a subset of letters (treatment means) to answer a specific question."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#what-is-a-contrast",
    "href": "lectures/week-03_comparisons-contrasts.html#what-is-a-contrast",
    "title": "Multiple Comparisons and Contrasts",
    "section": "What is a Contrast?",
    "text": "What is a Contrast?\nA contrast is a linear combination of treatment means that sums to zero:\n\\[\nL = \\sum_{i=1}^{v} c_i \\mu_i, \\quad \\text{with } \\sum_{i=1}^{v} c_i = 0.\n\\]\nThis ensures we’re measuring relative differences. Examples:\n\nPairwise difference: \\(\\mu_1 - \\mu_2\\).\nComparison of groups: \\(\\frac{\\mu_1 + \\mu_2}{2} - \\frac{\\mu_3 + \\mu_4}{2}\\)."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#variance-of-a-contrast",
    "href": "lectures/week-03_comparisons-contrasts.html#variance-of-a-contrast",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Variance of a Contrast",
    "text": "Variance of a Contrast\nIf each treatment \\(i\\) has \\(n_i\\) observations and errors are \\(N(0,\\sigma^2)\\), then:\n\\[\n\\text{Var}(L) = \\sigma^2 \\sum_{i=1}^v \\frac{c_i^2}{n_i}.\n\\]\nThis variance helps form confidence intervals and tests for the contrast.\n\nExample\nIn a battery life study with treatments A, B, C, D, suppose we suspect that (A and B) as a group differ from (C and D). We might form a contrast:\n\\[\nL = \\frac{\\mu_A + \\mu_B}{2} - \\frac{\\mu_C + \\mu_D}{2}.\n\\]\nThis contrast directly tests a scientific hypothesis: Do the top two battery types outperform the bottom two on average?"
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#fishers-least-significant-difference-lsd",
    "href": "lectures/week-03_comparisons-contrasts.html#fishers-least-significant-difference-lsd",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Fisher’s Least Significant Difference (LSD)",
    "text": "Fisher’s Least Significant Difference (LSD)\n\nConduct an ANOVA test first. If significant, proceed.\nCompare pairwise means using the standard t-test but do not adjust alpha for multiple tests.\nPros: Simple, more powerful.\nCons: Higher chance of Type I errors when many comparisons are made."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#tukeys-honest-significant-difference-hsd",
    "href": "lectures/week-03_comparisons-contrasts.html#tukeys-honest-significant-difference-hsd",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Tukey’s Honest Significant Difference (HSD)",
    "text": "Tukey’s Honest Significant Difference (HSD)\n\nDesigned for pairwise comparisons.\nControls the family-wise error rate (FWER).\nFor equal sample sizes:\n\n\\[\n\\text{HSD} = q_{\\alpha,v,n-v} \\sqrt{\\frac{\\text{MSE}}{n}},\n\\]\nwhere \\(q\\) is from the studentized range distribution."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#bonferroni-method",
    "href": "lectures/week-03_comparisons-contrasts.html#bonferroni-method",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Bonferroni Method",
    "text": "Bonferroni Method\n\nDivide \\(\\alpha\\) by the number of comparisons \\(m\\).\nVery conservative for large \\(m\\).\nEnsures strong control of FWER."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#scheffés-method",
    "href": "lectures/week-03_comparisons-contrasts.html#scheffés-method",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Scheffé’s Method",
    "text": "Scheffé’s Method\n\nApplies to any linear contrast, not just pairwise differences.\nMore general but often more conservative (wide confidence intervals).\nScheffé’s method is useful when the final set of contrasts is not fully decided a priori."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#example-1-pairwise-comparisons-with-tukeys-hsd",
    "href": "lectures/week-03_comparisons-contrasts.html#example-1-pairwise-comparisons-with-tukeys-hsd",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Example 1: Pairwise Comparisons with Tukey’s HSD",
    "text": "Example 1: Pairwise Comparisons with Tukey’s HSD\nData Setup (Simulated):\n\nset.seed(123)\ndata &lt;- data.frame(\n    Treatment = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 5),\n    Response = c(rnorm(5, 10, 2), rnorm(5, 15, 2), rnorm(5, 20, 2), rnorm(5, 25, 2))\n)\n\nfit &lt;- aov(Response ~ Treatment, data = data)\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment    3  631.3  210.43    47.7 3.33e-08 ***\nResiduals   16   70.6    4.41                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Apply Tukey’s HSD if ANOVA is significant\nTukeyHSD(fit)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Response ~ Treatment, data = data)\n\n$Treatment\n         diff        lwr       upr     p adj\nB-A  4.524222  0.7237295  8.324714 0.0170952\nC-A 10.228663  6.4281709 14.029155 0.0000050\nD-A 14.831544 11.0310518 18.632036 0.0000000\nC-B  5.704441  1.9039494  9.504933 0.0028150\nD-B 10.307322  6.5068303 14.107814 0.0000045\nD-C  4.602881  0.8023889  8.403373 0.0151748\n\n\n\nInterpretation\n\nLook at the output: intervals that do not include zero indicate significant differences.\nTukey is effective for all pairs while maintaining the overall confidence level."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#example-2-testing-a-specific-contrast",
    "href": "lectures/week-03_comparisons-contrasts.html#example-2-testing-a-specific-contrast",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Example 2: Testing a Specific Contrast",
    "text": "Example 2: Testing a Specific Contrast\nDefining a Contrast:\n\n# Suppose we want to test (A,B) vs (C,D)\ncontrast_vec &lt;- c(0.5, 0.5, -0.5, -0.5)\n\nemm &lt;- emmeans(fit, ~Treatment)\ncontrast(emm, method = list(\"AB_vs_CD\" = contrast_vec))\n\n contrast estimate    SE df t.ratio p.value\n AB_vs_CD    -10.3 0.939 16 -10.932  &lt;.0001\n\n\n\n\nFor more detail on the emmeans package, see the tutorial page emmeans in the appendix.\n\nInterpretation\nThe output gives an estimate, standard error, t-value, and p-value. If p &lt; alpha, we conclude there is evidence that (A,B) differ from (C,D)."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#medium-difficulty",
    "href": "lectures/week-03_comparisons-contrasts.html#medium-difficulty",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nPairwise Comparisons: Given four treatments, run a simulation in R, fit an ANOVA, and use Tukey’s HSD. Interpret at least one pairwise comparison.\nConstructing Contrasts: For three treatments (X, Y, Z), form a contrast comparing X with the average of Y and Z. Compute its variance using a known MSE and sample sizes."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#challenging-problems",
    "href": "lectures/week-03_comparisons-contrasts.html#challenging-problems",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nScheffé’s Method for Arbitrary Contrasts: Generate data from five treatments and test a complex contrast (e.g., \\((\\mu_1+\\mu_2)-(\\mu_4+\\mu_5)\\)). Implement Scheffé’s method and interpret the wide confidence interval.\nPower Analysis for MCPs: Using the pwr package in R, simulate a scenario with four treatments and known variance. Determine sample size needed to achieve 80% power under Tukey’s HSD for detecting a specified difference."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#references",
    "href": "lectures/week-03_comparisons-contrasts.html#references",
    "title": "Multiple Comparisons and Contrasts",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C. (2020). Design and Analysis of Experiments. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-03_comparisons-contrasts.html#proofs-and-derivations",
    "href": "lectures/week-03_comparisons-contrasts.html#proofs-and-derivations",
    "title": "Multiple Comparisons and Contrasts",
    "section": "Proofs and Derivations",
    "text": "Proofs and Derivations\n\nVariance of a Contrast\nFor a contrast \\(L = \\sum c_i \\mu_i\\), assuming independent samples of size \\(n_i\\) and variance \\(\\sigma^2\\):\n\\[\n\\hat{L} = \\sum c_i \\bar{Y}_i, \\quad \\text{Var}(\\hat{L}) = \\sigma^2 \\sum \\frac{c_i^2}{n_i}.\n\\]\nProof involves linearity of expectation and variance properties under independence and homoscedasticity (Dean et al., 2017).\n\n\nDerivation of Tukey’s HSD\nTukey’s HSD critical value is derived from the studentized range distribution \\(q\\):\n\nLet \\(\\text{max diff} = \\max_i(\\bar{Y}_i) - \\min_i(\\bar{Y}_i)\\).\nUnder \\(H_0\\), \\(\\frac{\\text{max diff}}{\\sqrt{\\text{MSE}/n}}\\) follows a studentized range distribution.\nInvert this to find the critical range \\(q_{\\alpha,v,n-v}\\).\n\n(Montgomery, 2020)\n\n\nScheffé’s Method\nScheffé’s intervals are based on the fact that any linear contrast of treatment means can be bounded using a \\((v-1)F\\)-distribution multiplier:\n\\[\nL \\in \\hat{L} \\pm \\sqrt{(v-1)F_{v-1, n-v,\\alpha}}\\cdot \\text{SE}(\\hat{L}).\n\\]\nThe proof uses the union-intersection principle and geometry of the parameter space (Christensen, 2018)."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html",
    "href": "lectures/week-09_complete-blocks.html",
    "title": "Complete Blocks Design",
    "section": "",
    "text": "For designs where treatments are replicated within blocks, the model extends to include interaction effects:\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit}\n\\]\nWhere:\n\n\\(Y_{hit}\\): Response for treatment \\(i\\) in block \\(h\\), replicate \\(t\\).\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\((\\theta \\tau)_{hi}\\): Interaction effect between block \\(h\\) and treatment \\(i\\).\n\\(\\epsilon_{hit}\\): Random error.\n\n\n\n\nDetermining the appropriate sample size is crucial for ensuring adequate power in detecting treatment effects.\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Variance estimate.\n\\(\\phi\\): Desired power.\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\nDeeper Insight:\nSample size calculations consider the balance between desired statistical power and the practical constraints of the experiment. Increasing the number of blocks or replicates can enhance the power but may require more resources.\nReference:\nMontgomery (2019) discusses power analysis and sample size determination in Design and Analysis of Experiments.\n\n\n\n\nIn scenarios where interaction effects are suspected, it’s essential to include them in the model to avoid biased estimates.\n\n\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit}\n\\]\nImplications:\n\nSignificant Interaction: Indicates that the effect of treatments varies across blocks, necessitating separate treatment effect analyses within each block.\nNon-Significant Interaction: Supports the additive model, validating the use of standard RCBD analysis.\n\nReference:\nKempthorne (1977) elaborates on the importance of accounting for interactions in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#general-complete-block-designs",
    "href": "lectures/week-09_complete-blocks.html#general-complete-block-designs",
    "title": "Complete Blocks Design",
    "section": "",
    "text": "For designs where treatments are replicated within blocks, the model extends to include interaction effects:\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit}\n\\]\nWhere:\n\n\\(Y_{hit}\\): Response for treatment \\(i\\) in block \\(h\\), replicate \\(t\\).\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\((\\theta \\tau)_{hi}\\): Interaction effect between block \\(h\\) and treatment \\(i\\).\n\\(\\epsilon_{hit}\\): Random error.\n\n\n\n\nDetermining the appropriate sample size is crucial for ensuring adequate power in detecting treatment effects.\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Variance estimate.\n\\(\\phi\\): Desired power.\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\nDeeper Insight:\nSample size calculations consider the balance between desired statistical power and the practical constraints of the experiment. Increasing the number of blocks or replicates can enhance the power but may require more resources.\nReference:\nMontgomery (2019) discusses power analysis and sample size determination in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#handling-block-treatment-interactions",
    "href": "lectures/week-09_complete-blocks.html#handling-block-treatment-interactions",
    "title": "Complete Blocks Design",
    "section": "",
    "text": "In scenarios where interaction effects are suspected, it’s essential to include them in the model to avoid biased estimates.\n\n\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit}\n\\]\nImplications:\n\nSignificant Interaction: Indicates that the effect of treatments varies across blocks, necessitating separate treatment effect analyses within each block.\nNon-Significant Interaction: Supports the additive model, validating the use of standard RCBD analysis.\n\nReference:\nKempthorne (1977) elaborates on the importance of accounting for interactions in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#example-resting-metabolic-rate",
    "href": "lectures/week-09_complete-blocks.html#example-resting-metabolic-rate",
    "title": "Complete Blocks Design",
    "section": "Example: Resting Metabolic Rate",
    "text": "Example: Resting Metabolic Rate\n\nScenario\nA study aims to compare three different exercise protocols (Treatments A, B, C) on subjects’ resting metabolic rates. The subjects are grouped into four blocks based on age groups to control for age-related variability.\n\n\nStep-by-Step R Code\n\n1. Load and Prepare Data\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:4, each = 3))\ntreatments &lt;- factor(rep(c(\"A\", \"B\", \"C\"), times = 4))\n\n# Simulate response variable (e.g., resting metabolic rate in kcal/day)\nresponse &lt;- c(\n    rnorm(3, mean = 1500, sd = 50), # Block 1\n    rnorm(3, mean = 1550, sd = 50), # Block 2\n    rnorm(3, mean = 1480, sd = 50), # Block 3\n    rnorm(3, mean = 1520, sd = 50) # Block 4\n)\n\n# Create data frame\ndata &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data)\n\n\n\n2. Fit the RCBD Model\n\n# Fit the RCBD model using ANOVA\n\nmodel &lt;- aov(response ~ blocks + treatments, data = data)\n# Summary of the model\nsummary(model)\n\nInterpretation:\n\nBlocks: Account for age-related variability.\nTreatments: Assess the impact of exercise protocols on metabolic rate after controlling for age.\n\n\n\n3. Diagnostic Plots\n\n# Diagnostic plots for the ANOVA model\n\npar(mfrow = c(2, 2))\nplot(model)\npar(mfrow = c(1, 1))\n\nInterpretation:\nEnsure that residuals meet ANOVA assumptions. Look for random scatter in Residuals vs Fitted, linearity in Scale-Location, and normality in Q-Q plots.\n\n\n4. Post-Hoc Analysis\n\n# Load emmeans package for estimated marginal means\nlibrary(emmeans)\n# Estimated marginal means for treatments\nemm &lt;- emmeans(model, ~treatments)\n# Pairwise comparisons\npairwise_comparisons &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise_comparisons)\n\nInterpretation:\nDetermine which exercise protocols significantly differ in their effect on resting metabolic rates after adjusting for age."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#conclusion",
    "href": "lectures/week-09_complete-blocks.html#conclusion",
    "title": "Complete Blocks Design",
    "section": "Conclusion",
    "text": "Conclusion\nComplete Block Designs (CBDs), particularly Randomized Complete Block Designs (RCBDs), are indispensable in experimental research for controlling variability due to nuisance factors. By structuring experiments to include blocks where each treatment is represented, CBDs enhance the precision and reliability of treatment effect estimates. The integration of ANOVA facilitates robust analysis, allowing researchers to discern meaningful differences among treatments while accounting for block-induced variability."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#key-takeaways",
    "href": "lectures/week-09_complete-blocks.html#key-takeaways",
    "title": "Complete Blocks Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBlocking: Reduces variability from known sources, increasing the efficiency of treatment comparisons.\nANOVA in RCBDs: Decomposes variability to isolate treatment effects, providing a clear framework for hypothesis testing.\nGeneral Complete Block Designs: Extend the utility of CBDs to more complex scenarios involving treatment replications within blocks.\nPractical Implementation: R offers comprehensive tools for designing, analyzing, and interpreting CBDs, empowering researchers with robust statistical capabilities.\nAdvanced Topics: Understanding interactions and sample size considerations enhances the depth and applicability of CBDs in diverse research contexts.\n\nMastery of Complete Block Designs equips graduate students with the methodological rigor necessary for conducting sophisticated experiments, ensuring that findings are both accurate and generalizable."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#proof-of-rcbd-model-assumptions",
    "href": "lectures/week-09_complete-blocks.html#proof-of-rcbd-model-assumptions",
    "title": "Complete Blocks Design",
    "section": "Proof of RCBD Model Assumptions",
    "text": "Proof of RCBD Model Assumptions\n\n1. Variance Decomposition\nThe RCBD model is expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Error term.\n\n\n\nAssumptions\n\nNormality: \\(\\epsilon_{hi}\\) are normally distributed.\nIndependence: Observations are independent.\nHomoscedasticity: \\(\\sigma^2\\) is constant across all treatments and blocks.\nNo Interaction: Effects of blocks and treatments are additive.\n\n\n\nVariance Decomposition\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]\n\n\nDerivation\n\nTotal Sum of Squares (SS\\(_\\text{Total}\\)):\n\n\\[\nSS_\\text{Total} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}\\) is the grand mean.\n\nBlock Sum of Squares (SS\\(_\\text{Block}\\)):\n\n\\[\nSS_\\text{Block} = t \\sum_{h=1}^b (\\bar{Y}_h - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_h\\) is the mean of block \\(h\\).\n\nTreatment Sum of Squares (SS\\(_\\text{Treatment}\\)):\n\n\\[\nSS_\\text{Treatment} = b \\sum_{i=1}^t (\\bar{Y}_i - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_i\\) is the mean of treatment \\(i\\).\n\nError Sum of Squares (SS\\(_\\text{Error}\\)):\n\n\\[\nSS_\\text{Error} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y}_h - \\bar{Y}_i + \\bar{Y})^2\n\\]\nRepresents variability not explained by blocks or treatments.\nConclusion:\nThe variance decomposition allows for the assessment of how much variability in the response is attributable to blocks, treatments, and random error.\nReference:\nMontgomery (2019) provides a detailed derivation of ANOVA components in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#effects-of-ignoring-block-treatment-interaction",
    "href": "lectures/week-09_complete-blocks.html#effects-of-ignoring-block-treatment-interaction",
    "title": "Complete Blocks Design",
    "section": "Effects of Ignoring Block-Treatment Interaction",
    "text": "Effects of Ignoring Block-Treatment Interaction\nIn the RCBD model, it is assumed that there is no interaction between blocks and treatments. Ignoring a significant block-treatment interaction can lead to biased estimates of treatment effects.\n\nMathematical Illustration\nSuppose the true model includes an interaction term:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nIf the interaction term \\((\\theta \\tau)_{hi}\\) is significant and is omitted from the model, the estimates of \\(\\tau_i\\) will be biased because the model incorrectly attributes part of the interaction effect to treatment effects.\n\n\nProof Outline\n\nFull Model: Includes interaction.\nReduced Model: Omits interaction.\nBias Derivation: Show that \\(\\hat{\\tau}_i\\) in the reduced model includes components of the interaction effect.\n\nConclusion:\nIt is crucial to test for block-treatment interactions. If significant, a more complex model that includes interaction terms should be employed.\nReference:\nKempthorne (1977) discusses the implications of model misspecification in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#sample-size-calculations-for-rcbds",
    "href": "lectures/week-09_complete-blocks.html#sample-size-calculations-for-rcbds",
    "title": "Complete Blocks Design",
    "section": "Sample Size Calculations for RCBDs",
    "text": "Sample Size Calculations for RCBDs\nDetermining the appropriate sample size is essential for achieving sufficient power to detect treatment effects.\n\nFormula\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Estimated variance of the response.\n\\(\\phi\\): Desired power (e.g., 0.80).\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\n\n\nDerivation\n\nPower Analysis:\n\nBased on the desired power (\\(\\phi\\)) and significance level (\\(\\alpha\\)), determine the non-centrality parameter needed to detect effect size \\(\\Delta\\).\n\nRearrange for \\(n\\):\n\nSolving the power equation for \\(n\\) yields the sample size formula.\nExample Calculation:\nSuppose you have 4 treatments (\\(v = 4\\)), 5 blocks (\\(b = 5\\)), an estimated variance (\\(\\sigma^2 = 20\\)), desire a power of 0.80 (\\(\\phi = 0.84\\)), and aim to detect a minimum effect size of 5 units (\\(\\Delta = 5\\)).\n\\[\nn \\geq \\frac{2 \\times 4 \\times 20 \\times 0.84^2}{5 \\times 5^2} = \\frac{2 \\times 4 \\times 20 \\times 0.7056}{125} \\approx \\frac{113.792}{125} \\approx 0.91\n\\]\nSince \\(n\\) must be an integer, at least 1 replicate per treatment is needed. However, practical considerations often necessitate larger sample sizes.\nReference:\nMontgomery (2019) elaborates on sample size determination in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#medium-difficulty",
    "href": "lectures/week-09_complete-blocks.html#medium-difficulty",
    "title": "Complete Blocks Design",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction:\n\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\nGenerate data for treatments A, B, C with a covariate X.\nFit an ANCOVA model adjusting for X.\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\nX &lt;- rnorm(90, mean = 50, sd = 10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) + 0.5 * (X - mean(X)) + rnorm(90, mean = 0, sd = 3)\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\nQuestions:\n\nWhat are the main effects of Treatment and the covariate X?\nAre there significant differences between treatments after adjusting for X?\nDo the diagnostic plots suggest any assumption violations?\n\n\nBalloon Data Analysis:\n\n\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\nR Code Example:\n\nlibrary(car)\nlibrary(emmeans)\n\n# Load and prepare data\nballoon.data &lt;- read.table(\"data/dean2017/balloon.txt\", header = TRUE)\n\nballoon.data |&gt; dplyr::glimpse()\n\nballoon.data &lt;- within(balloon.data, {\n    x &lt;- Order - mean(Order)\n    fColor &lt;- factor(Color)\n})\n\n# Fit ANCOVA model\n\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n# Test homogeneity of slopes\n\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n# Estimated marginal means\n\nemm_balloon &lt;- emmeans(model, ~fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\nQuestions:\n\nDoes the interaction term indicate equality of slopes across treatments?\nHow do the adjusted means compare across balloon colors?\nWhat conclusions can be drawn about the effect of balloon color on inflation time after adjusting for run order?"
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#challenging-problems",
    "href": "lectures/week-09_complete-blocks.html#challenging-problems",
    "title": "Complete Blocks Design",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor ANCOVA Analysis:\n\n\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\nR Code Example:\n\nlibrary(car)\nset.seed(456)\n# Simulate unbalanced data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n# Simulate responses with treatment and covariate effects\n\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean = 0, sd = 3)\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\n\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n# Type II Sum of Squares\n\ntypeII &lt;- car::Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\n\ntypeIII &lt;- car::Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#discussion-points",
    "href": "lectures/week-09_complete-blocks.html#discussion-points",
    "title": "Complete Blocks Design",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\nImpact of Imbalance:\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\nSignificance of Effects:\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\nImplications for Experimental Conclusions:\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\nReference:\nChristensen (2018) provides guidance on handling unbalanced designs and selecting appropriate sums of squares types in Analysis of Variance, Design, and Regression.\n\nThree-Factor Interaction Interpretation:\n\n\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#r-code-example",
    "href": "lectures/week-09_complete-blocks.html#r-code-example",
    "title": "Complete Blocks Design",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(emmeans)\n\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\n\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\n\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\n\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\n\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + 0.5 * (X - mean(X)) + ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2) + rnorm(72, mean = 0, sd = 3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\n\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nsummary(fit_three)\n\n# Plot three-way interaction\n\nggplot(data_three, aes(x = FactorA, y = Y, color = FactorB, group = FactorB)) +\ngeom_point() +\ngeom_line() +\nfacet_wrap(~FactorC) +\nlabs(title = \"Three-Way Interaction Plot\", x = \"Factor A\", y = \"Response\") +\ntheme_minimal()\n\n# Estimated marginal means\n\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#interpretation",
    "href": "lectures/week-09_complete-blocks.html#interpretation",
    "title": "Complete Blocks Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction:\n\nThe interaction plot shows how the interaction between Factor A and Factor B varies across levels of Factor C. Non-parallel lines across different facets (Factor C levels) indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation. The effect of one factor depends on the combination of other factors.\n\nPractical Implications:\n\nUnderstanding three-way interactions allows for more nuanced insights into how multiple factors jointly influence the response variable, leading to better-informed decisions and strategies.\nReference:\nDean, Voss & Draguljić (2017) elaborate on interpreting higher-order interactions in Design and Analysis of Experiments.\n\nMathematical Proof of Interaction Contrasts:\n\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#proof-outline-1",
    "href": "lectures/week-09_complete-blocks.html#proof-outline-1",
    "title": "Complete Blocks Design",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij} + (\\alpha\\gamma)^*_{ik} + (\\beta\\gamma)^*_{jk} + (\\alpha\\beta\\gamma)^*_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)^*_{ik}\\) and \\((\\beta\\gamma)^*_{jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProof of Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta)^*_{ij} = (\\alpha\\gamma)^*_{ik} = (\\beta\\gamma)*{jk} = (\\alpha\\beta\\gamma)^*_{ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nindicating that the response is purely additive with respect to the main effects.\n\nZero Interaction Contrasts:\n\nIf all interaction contrasts are zero, the model exhibits additivity. Thus, interaction contrasts quantify how much the actual model deviates from an additive one.\n\nConclusion:\n\nInteraction contrasts provide a mathematical measure of the presence and magnitude of interactions. Non-zero contrasts indicate that the combined effect of factors is not merely the sum of their individual effects, highlighting the importance of considering interactions in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017) offer detailed mathematical treatments of interaction contrasts in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#proof-of-rcbd-model-assumptions-1",
    "href": "lectures/week-09_complete-blocks.html#proof-of-rcbd-model-assumptions-1",
    "title": "Complete Blocks Design",
    "section": "Proof of RCBD Model Assumptions",
    "text": "Proof of RCBD Model Assumptions\n\n1. Variance Decomposition\nThe RCBD model is expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Error term.\n\nAssumptions:\n\nNormality: \\(\\epsilon_{hi}\\) are normally distributed.\nIndependence: Observations are independent.\nHomoscedasticity: \\(\\sigma^2\\) is constant across all treatments and blocks.\nNo Interaction: Effects of blocks and treatments are additive.\n\nVariance Decomposition:\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]\nDerivation:\n\nTotal Sum of Squares (SS\\(_\\text{Total}\\)):\n\n\\[\nSS_\\text{Total} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}\\) is the grand mean.\n\nBlock Sum of Squares (SS\\(_\\text{Block}\\)):\n\n\\[\nSS_\\text{Block} = t \\sum_{h=1}^b (\\bar{Y}_h - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_h\\) is the mean of block \\(h\\).\n\nTreatment Sum of Squares (SS\\(_\\text{Treatment}\\)):\n\n\\[\nSS_\\text{Treatment} = b \\sum_{i=1}^t (\\bar{Y}_i - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_i\\) is the mean of treatment \\(i\\).\n\nError Sum of Squares (SS\\(_\\text{Error}\\)):\n\n\\[\nSS_\\text{Error} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y}_h - \\bar{Y}_i + \\bar{Y})^2\n\\]\nRepresents variability not explained by blocks or treatments.\nConclusion:\nThe variance decomposition allows for the assessment of how much variability in the response is attributable to blocks, treatments, and random error.\nReference:\nMontgomery (2019) provides a detailed derivation of ANOVA components in Design and Analysis of Experiments.\n\n\n2. Effects of Ignoring Block-Treatment Interaction\nIn the RCBD model, it is assumed that there is no interaction between blocks and treatments. Ignoring a significant block-treatment interaction can lead to biased estimates of treatment effects.\nMathematical Illustration:\nSuppose the true model includes an interaction term:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nIf the interaction term \\((\\theta \\tau)_{hi}\\) is significant and is omitted from the model, the estimates of \\(\\tau_i\\) will be biased because the model incorrectly attributes part of the interaction effect to treatment effects.\nProof Outline:\n\nFull Model: Includes interaction.\nReduced Model: Omits interaction.\nBias Derivation: Show that \\(\\hat{\\tau}_i\\) in the reduced model includes components of the interaction effect.\n\nConclusion:\nIt is crucial to test for block-treatment interactions. If significant, a more complex model that includes interaction terms should be employed.\nReference:\nKempthorne (1977) discusses the implications of model misspecification in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#sample-size-calculations-for-rcbds-1",
    "href": "lectures/week-09_complete-blocks.html#sample-size-calculations-for-rcbds-1",
    "title": "Complete Blocks Design",
    "section": "Sample Size Calculations for RCBDs",
    "text": "Sample Size Calculations for RCBDs\nDetermining the appropriate sample size is essential for ensuring adequate power to detect treatment effects.\n\nFormula\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Variance estimate.\n\\(\\phi\\): Desired power (e.g., 0.80).\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\n\n\nDerivation\n\nPower Analysis:\n\nBased on the desired power (\\(\\phi\\)) and significance level (\\(\\alpha\\)), determine the non-centrality parameter needed to detect effect size \\(\\Delta\\).\n\nRearrange for \\(n\\):\n\nSolving the power equation for \\(n\\) yields the sample size formula.\nExample Calculation:\nSuppose you have 4 treatments (\\(v = 4\\)), 5 blocks (\\(b = 5\\)), an estimated variance (\\(\\sigma^2 = 20\\)), desire a power of 0.80 (\\(\\phi = 0.84\\)), and aim to detect a minimum effect size of 5 units (\\(\\Delta = 5\\)).\n\\[\nn \\geq \\frac{2 \\times 4 \\times 20 \\times 0.84^2}{5 \\times 5^2} = \\frac{2 \\times 4 \\times 20 \\times 0.7056}{125} \\approx \\frac{113.792}{125} \\approx 0.91\n\\]\nSince \\(n\\) must be an integer, at least 1 replicate per treatment is needed. However, practical considerations often necessitate larger sample sizes.\nReference:\nMontgomery (2019) elaborates on sample size determination in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#handling-block-treatment-interactions-1",
    "href": "lectures/week-09_complete-blocks.html#handling-block-treatment-interactions-1",
    "title": "Complete Blocks Design",
    "section": "Handling Block-Treatment Interactions",
    "text": "Handling Block-Treatment Interactions\nIn scenarios where interaction effects between blocks and treatments are suspected, it is essential to include them in the model to avoid biased estimates and incorrect inferences.\n\nModel with Interaction\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nImplications:\n\nSignificant Interaction: Suggests that the effect of treatments varies across blocks. Separate treatment effect analyses within each block may be necessary.\nNon-Significant Interaction: Supports the additive model, validating the use of standard RCBD analysis.\n\nReference:\nKempthorne (1977) elaborates on the importance of accounting for interactions in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#sample-size-calculation-example-in-r",
    "href": "lectures/week-09_complete-blocks.html#sample-size-calculation-example-in-r",
    "title": "Complete Blocks Design",
    "section": "Sample Size Calculation Example in R",
    "text": "Sample Size Calculation Example in R\nTo determine the necessary number of replicates per treatment in an RCBD, you can use R for power and sample size calculations.\n\n# Parameters\nv &lt;- 4 # Number of treatments\nb &lt;- 5 # Number of blocks\nsigma2 &lt;- 20 # Variance estimate\nphi &lt;- 0.84 # Desired power (approx 0.80)\nDelta &lt;- 5 # Minimum detectable effect size\n\n# Calculate sample size\nn &lt;- ceiling((2 * v * sigma2 * phi^2) / (b * Delta^2))\nprint(n)\n\nInterpretation:\nThe ceiling function ensures that the sample size is rounded up to the next integer, ensuring adequate power."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#medium-difficulty-1",
    "href": "lectures/week-09_complete-blocks.html#medium-difficulty-1",
    "title": "Complete Blocks Design",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction:\n\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\nGenerate data for treatments A, B, C with a covariate X.\nFit an ANCOVA model adjusting for X.\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\nR Code Example:\n\nset.seed(123)\n# Define factors and covariate\n\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50, sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4) + 0.5 * (X - mean(X)) + rnorm(90, mean=0, sd=3)\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n# Diagnostic plots\n\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\n\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\nQuestions:\n\nWhat are the main effects of Treatment and the covariate X?\nAre there significant differences between treatments after adjusting for X?\nDo the diagnostic plots suggest any assumption violations?\n\n\nBalloon Data Analysis:\n\n\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\n\nR Code Example\n\nlibrary(car)\nlibrary(emmeans)\n\n# Load and prepare data\nballoon.data &lt;- read.table(\"data/balloon.txt\", header = TRUE)\nballoon.data &lt;- within({\n  balloon.data\n, x &lt;- Order - mean(Order),\nfColor &lt;- factor(Color)\n})\n\n# Fit ANCOVA model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n# Test homogeneity of slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n# Estimated marginal means\nemm_balloon &lt;- emmeans(model ~ fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\nQuestions:\n\nDoes the interaction term indicate equality of slopes across treatments?\nHow do the adjusted means compare across balloon colors?\nWhat conclusions can be drawn about the effect of balloon color on inflation time after adjusting for run order?"
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#challenging-problems-1",
    "href": "lectures/week-09_complete-blocks.html#challenging-problems-1",
    "title": "Complete Blocks Design",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor ANCOVA Analysis:\n\n\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\n\nR Code Example\n\nlibrary(car)\nset.seed(456)\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean = 0, sd = 3)\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#discussion-points-1",
    "href": "lectures/week-09_complete-blocks.html#discussion-points-1",
    "title": "Complete Blocks Design",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\nImpact of Imbalance:\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\nSignificance of Effects:\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\nImplications for Experimental Conclusions:\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\nReference:\nChristensen (2018) provides guidance on handling unbalanced designs and selecting appropriate sums of squares types in Analysis of Variance, Design, and Regression.\n\nThree-Factor Interaction Interpretation:\n\n\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\n\nR Code Example\n\nlibrary(emmeans)\nlibrary(ggplot2)\nset.seed(789)\n\n# Simulate data\n\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\n\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + 0.5 * (X - mean(X)) + ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)) + rnorm(72, mean = 0, sd = 3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\n\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\n\nsummary(fit_three)\n\n# Plot three-way interaction\n\nggplot(data_three, aes(x = FactorA, y = Y, color = FactorB, group = FactorB)) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(~FactorC) +\n    labs(title = \"Three-Way Interaction Plot\", x = \"Factor A\", y = \"Response\") +\n    theme_minimal()\n\n# Estimated marginal means\n\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#interpretation-1",
    "href": "lectures/week-09_complete-blocks.html#interpretation-1",
    "title": "Complete Blocks Design",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction:\n\nThe interaction plot illustrates how the interaction between Factor A and Factor B varies across different levels of Factor C. Non-parallel lines across facets indicate a significant three-way interaction.\n\nImpact on Main and Two-Way Interactions:\n\nSignificant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation. The effect of one factor depends on the combination of other factors.\n\nPractical Implications:\n\nUnderstanding three-way interactions allows for more nuanced insights into how multiple factors jointly influence the response variable, leading to better-informed decisions and strategies.\nReference:\nDean, Voss & Draguljić (2017) elaborate on interpreting higher-order interactions in Design and Analysis of Experiments.\n\nMathematical Proof of Interaction Contrasts:\n\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#proof-outline-2",
    "href": "lectures/week-09_complete-blocks.html#proof-outline-2",
    "title": "Complete Blocks Design",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij} + (\\alpha\\gamma)^*{ik} + (\\beta\\gamma)^*_{jk} + (\\alpha\\beta\\gamma)^*_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)^*_{ik}\\) and \\((\\beta\\gamma)^*_{jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProof of Additivity:\n\n\nAdditive Model:\n\nIf all interaction terms are zero (\\((\\alpha\\beta)^*_{ij} = (\\alpha\\gamma)^*_{ik} = (\\beta\\gamma)^*_{jk} = (\\alpha\\beta\\gamma)^*_{ijk} = 0\\)), the model simplifies to:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nindicating that the response is purely additive with respect to the main effects.\n\nZero Interaction Contrasts:\n\nIf all interaction contrasts are zero, the model exhibits additivity. Thus, interaction contrasts quantify how much the actual model deviates from an additive one.\n\nConclusion:\n\nInteraction contrasts provide a mathematical measure of the presence and magnitude of interactions. Non-zero contrasts indicate that the combined effect of factors is not merely the sum of their individual effects, highlighting the importance of considering interactions in experimental analysis.\nReference:\nDean, Voss & Draguljić (2017) offer detailed mathematical treatments of interaction contrasts in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#adjusted-treatment-means",
    "href": "lectures/week-09_complete-blocks.html#adjusted-treatment-means",
    "title": "Complete Blocks Design",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe adjusted mean for treatment \\(i\\) accounts for the influence of the covariate, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#derivation-3",
    "href": "lectures/week-09_complete-blocks.html#derivation-3",
    "title": "Complete Blocks Design",
    "section": "Derivation",
    "text": "Derivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference:\nMontgomery (2019) and Scheffé (1959) provide detailed derivations of adjusted means in ANCOVA.\n\nF-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\nWhere \\(\\beta\\) is the common slope for all treatments.\n\nModel With Interaction (Allowing Slopes to Vary):\n\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Statistic\n\\[\nF = \\frac{\\text{SSR}^*_{\\text{Interaction}} / \\text{DF}^*_{\\text{Interaction}}}{\\text{SSE} / \\text{DF}_{\\text{Error}}}\n\\]\nWhere:\n\n\\(\\text{SSR}_{\\text{Interaction}}\\): Sum of squares for the interaction term.\n\\(\\text{DF}_{\\text{Interaction}}\\): Degrees of freedom for the interaction.\n\\(\\text{SSE}\\): Sum of squares for error.\n\\(\\text{DF}_{\\text{Error}}\\): Degrees of freedom for error.\n\nDecision Rule:\nReject \\(H_0\\) if \\(F\\) exceeds the critical value from the F-distribution with degrees of freedom \\((\\text{DF}*{\\text{Interaction}}, \\text{DF}*{\\text{Error}})\\).\nConclusion:\nA significant \\(F\\)-statistic indicates that the slopes are not homogeneous, violating a key ANCOVA assumption.\nReference:\nScheffé (1959) elaborates on hypothesis testing for interaction terms in ANCOVA."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-09_complete-blocks.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Complete Blocks Design",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\n\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast:\n\nUsing the emmeans package to define and estimate contrasts.\n\nCalculate the Confidence Interval:\n\nThe emmeans package automatically provides confidence intervals when performing contrasts.\n\n\nExample\n\nlibrary(emmeans)\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~Treatment)\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\n\n\n\nInterpretation\nThe output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference:\nChristensen (2018) discusses confidence intervals for contrasts in Analysis of Variance, Design, and Regression."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#conclusion-1",
    "href": "lectures/week-09_complete-blocks.html#conclusion-1",
    "title": "Complete Blocks Design",
    "section": "Conclusion",
    "text": "Conclusion\nComplete Block Designs (CBDs), particularly Randomized Complete Block Designs (RCBDs), are invaluable in experimental research for controlling variability due to nuisance factors. By structuring experiments to include blocks where each treatment is represented, CBDs enhance the precision and reliability of treatment effect estimates. The integration of ANOVA facilitates robust analysis, allowing researchers to discern meaningful differences among treatments while accounting for block-induced variability."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#key-takeaways-1",
    "href": "lectures/week-09_complete-blocks.html#key-takeaways-1",
    "title": "Complete Blocks Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBlocking: Reduces variability from known sources, increasing the efficiency of treatment comparisons.\nANOVA in RCBDs: Decomposes variability to isolate treatment effects, providing a clear framework for hypothesis testing.\nGeneral Complete Block Designs: Extend the utility of CBDs to more complex scenarios involving treatment replications within blocks.\nPractical Implementation: R offers comprehensive tools for designing, analyzing, and interpreting CBDs, empowering researchers with robust statistical capabilities.\nAdvanced Topics: Understanding interactions and sample size considerations enhances the depth and applicability of CBDs in diverse research contexts."
  },
  {
    "objectID": "lectures/week-09_complete-blocks.html#references",
    "href": "lectures/week-09_complete-blocks.html#references",
    "title": "Complete Blocks Design",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company.\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html",
    "href": "lectures/week-05_factorial-anova_v0.html",
    "title": "Factorial ANOVA",
    "section": "",
    "text": "Many real-world experiments involve multiple factors that may jointly influence a response. Factorial designs, especially two-factor designs, allow us to study each factor’s main effect and their interaction. Understanding interactions is crucial: sometimes one factor’s effect depends on the level of another, and ignoring these relationships can lead to misguided conclusions.\nIntuition: Think of baking a cake (response): Factor A = Oven Temperature (low, medium, high), Factor B = Baking Time (short, medium, long). The sweetness or fluffiness of the cake (response) depends on both temperature and time. If at a high temperature, an extra-long baking time burns the cake, whereas at a low temperature, you might need longer time to get it fully baked. This interplay is the interaction effect—neither factor alone explains the outcome fully without considering the other.\n\n\n\nUnderstand how two-factor factorial designs are structured and why they’re important.\nDistinguish between main effects and interaction effects in a two-way ANOVA.\nConduct a two-way ANOVA, interpret results, and create interaction plots in R.\nHandle complexities such as unbalanced designs and understand different types of sums of squares.\nExplore both conceptual understanding (through analogies) and rigorous formulation (through mathematics and proofs in the Appendix)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#objectives",
    "href": "lectures/week-05_factorial-anova_v0.html#objectives",
    "title": "Factorial ANOVA",
    "section": "",
    "text": "Understand how two-factor factorial designs are structured and why they’re important.\nDistinguish between main effects and interaction effects in a two-way ANOVA.\nConduct a two-way ANOVA, interpret results, and create interaction plots in R.\nHandle complexities such as unbalanced designs and understand different types of sums of squares.\nExplore both conceptual understanding (through analogies) and rigorous formulation (through mathematics and proofs in the Appendix)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#structure-and-terminology",
    "href": "lectures/week-05_factorial-anova_v0.html#structure-and-terminology",
    "title": "Factorial ANOVA",
    "section": "Structure and Terminology",
    "text": "Structure and Terminology\nA two-factor design has two treatment factors, \\(A\\) and \\(B\\), with \\(a\\) and \\(b\\) levels each. Every combination of levels is tested, resulting in \\(ab\\) treatment combinations (sometimes called “cells”):\n\nFactor \\(A\\): \\(a\\) levels \\(A_1, A_2, \\dots, A_a\\)\nFactor \\(B\\): \\(b\\) levels \\(B_1, B_2, \\dots, B_b\\)\n\nExample: In a crop yield experiment: - Factor A = Fertilizer type (3 levels) - Factor B = Irrigation method (2 levels) Total treatment combos = 3 × 2 = 6.\nEach cell can have one or more observations (replicates)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#main-effects-vs.-interaction",
    "href": "lectures/week-05_factorial-anova_v0.html#main-effects-vs.-interaction",
    "title": "Factorial ANOVA",
    "section": "Main Effects vs. Interaction",
    "text": "Main Effects vs. Interaction\n\nMain Effect (of A): The average change in response when we move across levels of A, ignoring B.\nMain Effect (of B): The average change in response when we move across levels of B, ignoring A.\n\nInteraction: Occurs if the effect of one factor on the response changes depending on the level of the other factor. If lines connecting treatment means in an interaction plot are not parallel, we have evidence of interaction.\nAnalogy: Think of wearing sunglasses (Factor A) and a hat (Factor B) on a sunny day. The effect of wearing sunglasses on how well you see (response) might differ depending on whether you’re also wearing a hat that shades your eyes. If sunglasses alone help a bit and the hat alone helps a bit, but together they help a lot more than expected by adding their individual effects, that’s an interaction."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#the-full-model-with-interaction",
    "href": "lectures/week-05_factorial-anova_v0.html#the-full-model-with-interaction",
    "title": "Factorial ANOVA",
    "section": "The Full Model with Interaction",
    "text": "The Full Model with Interaction\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\]\nwhere:\n\n\\(\\mu\\): Overall mean\n\\(\\alpha_i\\): Effect of the \\(i\\)-th level of Factor A (with \\(\\sum \\alpha_i=0\\))\n\\(\\beta_j\\): Effect of the \\(j\\)-th level of Factor B (with \\(\\sum \\beta_j=0\\))\n\\((\\alpha\\beta)_{ij}\\): Interaction effect (with constraints \\(\\sum_i (\\alpha\\beta)_{ij}=0\\) and \\(\\sum_j (\\alpha\\beta)_{ij}=0\\))\n\\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\): Random error\n\nIf replicates per treatment combination are \\(n\\), total observations \\(N = abn\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#reduced-models",
    "href": "lectures/week-05_factorial-anova_v0.html#reduced-models",
    "title": "Factorial ANOVA",
    "section": "Reduced Models",
    "text": "Reduced Models\n\nNo Interaction Model: If \\((\\alpha\\beta)_{ij}=0\\) for all \\(i,j\\), the model reduces to:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk}.\n\\]\n\nAdditivity: Without interaction, the effects of A and B simply add up."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#hypotheses",
    "href": "lectures/week-05_factorial-anova_v0.html#hypotheses",
    "title": "Factorial ANOVA",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nInteraction Test:\n\n\\(H_0:\\) No interaction (\\((\\alpha\\beta)_{ij}=0\\))\n\\(H_a:\\) Interaction present\n\nMain Effect of A:\n\n\\(H_0:\\alpha_1=\\alpha_2=\\cdots=\\alpha_a=0\\)\n\\(H_a:\\) At least one \\(\\alpha_i \\neq 0\\)\n\nMain Effect of B:\n\n\\(H_0:\\beta_1=\\beta_2=\\cdots=\\beta_b=0\\)\n\\(H_a:\\) At least one \\(\\beta_j \\neq 0\\)\n\n\nTesting Order: Generally, check interaction first. If significant, main effects need careful interpretation."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#anova-table",
    "href": "lectures/week-05_factorial-anova_v0.html#anova-table",
    "title": "Factorial ANOVA",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF-test\n\n\n\n\nA\n\\(a-1\\)\n\\(SS_A\\)\n\\(MS_A\\)\n\\(F_A = MS_A/MS_E\\)\n\n\nB\n\\(b-1\\)\n\\(SS_B\\)\n\\(MS_B\\)\n\\(F_B = MS_B/MS_E\\)\n\n\nA×B\n\\((a-1)(b-1)\\)\n\\(SS_{AB}\\)\n\\(MS_{AB}\\)\n\\(F_{AB} = MS_{AB}/MS_E\\)\n\n\nError\n\\(N - ab\\)\n\\(SS_E\\)\n\\(MS_E\\)\n-\n\n\nTotal\n\\(N-1\\)\n\\(SS_T\\)\n-\n-\n\n\n\nNote: If data are unbalanced, different sum of squares types (Type I, II, III) might be considered. See Christensen (2018) for details."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#medium-difficulty",
    "href": "lectures/week-05_factorial-anova_v0.html#medium-difficulty",
    "title": "Factorial ANOVA",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nBasic Analysis: Simulate a balanced two-factor design (e.g., \\(a=3\\), \\(b=2\\), \\(n=5\\)). Perform a two-way ANOVA, check for interaction, and create an interaction plot in R. Interpret the results.\nInterpretation Without Interaction: Given data where no interaction is found, interpret main effects. Use confidence intervals for factor level means and explain the practical meaning."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#challenging-problems",
    "href": "lectures/week-05_factorial-anova_v0.html#challenging-problems",
    "title": "Factorial ANOVA",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor Design: Work with a dataset where one cell has fewer observations. Fit the model and compare Type I, II, and III sums of squares using car::Anova(model, type=…). Discuss how the choice of sum of squares affects conclusions about main and interaction effects.\nComplex Interaction Interpretation: Use a real or simulated dataset with a significant interaction. Implement emmeans in R to estimate marginal means and pairwise comparisons. Write a short report explaining how the interaction modifies the main effects. Include plots of estimated means and confidence intervals.\nMathematical Proof of Interaction Contrasts (Appendix): Derive formulas for interaction contrasts and show how they measure the deviation from additivity. Prove that if all interaction contrasts are zero, the model is additive."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#references",
    "href": "lectures/week-05_factorial-anova_v0.html#references",
    "title": "Factorial ANOVA",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#interaction-contrasts",
    "href": "lectures/week-05_factorial-anova_v0.html#interaction-contrasts",
    "title": "Factorial ANOVA",
    "section": "Interaction Contrasts",
    "text": "Interaction Contrasts\nAn interaction can be represented by contrasts that compare how factor A’s differences vary across levels of factor B. For example, consider a simple 2×2 case (\\(a=2,b=2\\)):\n\\[\n(\\alpha\\beta)_{11} - (\\alpha\\beta)_{12} - (\\alpha\\beta)_{21} + (\\alpha\\beta)_{22}\n\\]\nmeasures the extent to which effects deviate from additivity."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#additivity-condition",
    "href": "lectures/week-05_factorial-anova_v0.html#additivity-condition",
    "title": "Factorial ANOVA",
    "section": "Additivity Condition",
    "text": "Additivity Condition\nFor no interaction:\n\\[\n(\\alpha\\beta)_{ij} = 0 \\implies \\tau_{ij} = \\alpha_i + \\beta_j.\n\\]\nThis forms a linear decomposition of cell means into separate contributions of factors A and B."
  },
  {
    "objectID": "lectures/week-05_factorial-anova_v0.html#proofs-for-sums-of-squares",
    "href": "lectures/week-05_factorial-anova_v0.html#proofs-for-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "Proofs for Sums of Squares",
    "text": "Proofs for Sums of Squares\nFollowing Dean et al. (2017), sums of squares for main and interaction effects can be constructed from orthogonal contrasts. Derive the form of \\(SS_A, SS_B, SS_{AB}\\) from cell means \\(\\bar{Y}_{ij}\\) and show their orthogonality in balanced designs."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html",
    "href": "lectures/week-08_rcbd-blocking_v0.html",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "",
    "text": "In the realm of experimental design, Complete Block Designs (CBDs) play a pivotal role in controlling the effects of nuisance factors, thereby enhancing the precision of treatment comparisons. By organizing experimental units into blocks that share similar characteristics, CBDs effectively reduce variability stemming from these extraneous factors. This structured approach not only refines the efficiency of experiments but also ensures more accurate and reliable analyses.\n\n\nImagine you are testing different fertilizers on crops spread across a large farm. The farm has sections with varying soil types, sunlight exposure, and moisture levels. Without controlling for these variations, it would be challenging to discern the true effect of each fertilizer. By grouping plots with similar soil types into blocks, you can more precisely compare the performance of fertilizers within each homogeneous group, minimizing the confounding influence of soil variability.\nThis lecture delves into the structure, applications, and analysis of Complete Block Designs, with a focus on Randomized Complete Block Designs (RCBDs) and their generalizations. We will explore theoretical underpinnings, practical implementations using R, and advanced topics to provide a comprehensive understanding tailored for graduate statistics students."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#introduction",
    "href": "lectures/week-08_rcbd-blocking_v0.html#introduction",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "",
    "text": "In the realm of experimental design, Complete Block Designs (CBDs) play a pivotal role in controlling the effects of nuisance factors, thereby enhancing the precision of treatment comparisons. By organizing experimental units into blocks that share similar characteristics, CBDs effectively reduce variability stemming from these extraneous factors. This structured approach not only refines the efficiency of experiments but also ensures more accurate and reliable analyses.\n\n\nImagine you are testing different fertilizers on crops spread across a large farm. The farm has sections with varying soil types, sunlight exposure, and moisture levels. Without controlling for these variations, it would be challenging to discern the true effect of each fertilizer. By grouping plots with similar soil types into blocks, you can more precisely compare the performance of fertilizers within each homogeneous group, minimizing the confounding influence of soil variability.\nThis lecture delves into the structure, applications, and analysis of Complete Block Designs, with a focus on Randomized Complete Block Designs (RCBDs) and their generalizations. We will explore theoretical underpinnings, practical implementations using R, and advanced topics to provide a comprehensive understanding tailored for graduate statistics students."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#objectives",
    "href": "lectures/week-08_rcbd-blocking_v0.html#objectives",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Objectives",
    "text": "Objectives\n\nDefine and distinguish between complete and incomplete block designs.\nExplore the structure and application of Randomized Complete Block Designs (RCBDs).\nPerform Analysis of Variance (ANOVA) for RCBDs and assess the efficiency of blocking.\nUnderstand general complete block designs and their applications.\nImplement complete block designs in R and interpret the results.\nReinforce concepts with exercises covering numerical examples, conceptual understanding, and mathematical proofs.\nExplore advanced mathematical derivations and proofs in the Appendix."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#what-are-complete-block-designs",
    "href": "lectures/week-08_rcbd-blocking_v0.html#what-are-complete-block-designs",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "What Are Complete Block Designs?",
    "text": "What Are Complete Block Designs?\nComplete Block Designs (CBDs) are experimental designs where each block contains all the treatments being tested. This ensures that every treatment is applied in each block, allowing for direct comparisons under similar conditions.\n\nBlock: A group of experimental units that are similar in some aspect and expected to yield comparable responses.\nComplete Block: Each block includes every treatment exactly once.\n\n\nExample\nIn a clinical trial comparing drug treatments across different hospitals, each hospital represents a block. By administering all treatments in each hospital, you account for hospital-specific factors, such as patient demographics and care protocols, ensuring that treatment effects are evaluated consistently."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#randomized-complete-block-designs-rcbds",
    "href": "lectures/week-08_rcbd-blocking_v0.html#randomized-complete-block-designs-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Randomized Complete Block Designs (RCBDs)",
    "text": "Randomized Complete Block Designs (RCBDs)\nRCBDs are a specific type of CBD where treatments are randomly assigned within each block. This randomness ensures that the assignment of treatments is free from bias and that any differences observed are attributable to the treatments rather than systematic variations.\nCharacteristics of RCBDs:\n\nThe number of experimental units in each block equals the number of treatments.\nEach treatment appears exactly once per block.\nTreatments are assigned randomly within each block.\n\nAnalogy: Think of RCBDs as seating arrangements in a classroom where each row (block) must contain one student from each house (treatments). By randomly assigning students to seats within rows, you ensure that no particular house has an advantage in any row."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#general-complete-block-designs",
    "href": "lectures/week-08_rcbd-blocking_v0.html#general-complete-block-designs",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "General Complete Block Designs",
    "text": "General Complete Block Designs\nBeyond RCBDs, General Complete Block Designs allow for multiple observations or replicates of treatments within each block. This is particularly useful when interaction effects between treatments and blocks are anticipated.\nFeatures:\n\nA treatment is replicated \\(s &gt; 1\\) times within a block.\nFacilitates the detection of interactions between treatments and blocking factors.\n\nExample: In a clinical trial, if a treatment needs to be administered multiple times within each hospital ward (block) to account for ward-specific factors, a general complete block design would be appropriate."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#model-and-assumptions",
    "href": "lectures/week-08_rcbd-blocking_v0.html#model-and-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Model and Assumptions",
    "text": "Model and Assumptions\nThe statistical model for an RCBD can be expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response variable for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Random error term."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#key-assumptions",
    "href": "lectures/week-08_rcbd-blocking_v0.html#key-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nNormality: The residuals (\\(\\epsilon_{hi}\\)) are normally distributed.\nHomogeneity of Variance (Homoscedasticity): The variance of residuals is constant across all treatments and blocks.\nIndependence: Observations are independent of each other.\nNo Interaction: There is no interaction between blocks and treatments, i.e., the effect of a treatment is consistent across all blocks.\n\nDeeper Insight: Assumption of no interaction is critical. If this assumption is violated, the RCBD model may not adequately capture the variability, leading to biased estimates of treatment effects. In such cases, more complex designs that account for interactions, like general complete block designs, are necessary.\nReference: Kempthorne (1977) elaborates on the assumptions and their implications in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#anova-for-rcbds",
    "href": "lectures/week-08_rcbd-blocking_v0.html#anova-for-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "ANOVA for RCBDs",
    "text": "ANOVA for RCBDs\nTo analyze RCBDs, we employ Analysis of Variance (ANOVA), which decomposes the total variability in the data into components attributable to blocks, treatments, and random error.\n\nVariance Decomposition\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]\n\n\nANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegrees of Freedom (DF)\nSum of Squares (SS)\nMean Square (MS)\nF-ratio\n\n\n\n\nBlocks\n\\(b - 1\\)\n\\(SS_\\text{Block}\\)\n\\(MS_\\text{Block} = SS_\\text{Block} / (b-1)\\)\n\n\n\nTreatments\n\\(t - 1\\)\n\\(SS_\\text{Treatment}\\)\n\\(MS_\\text{Treatment} = SS_\\text{Treatment} / (t-1)\\)\n\\(\\frac{MS_\\text{Treatment}}{MS_\\text{Error}}\\)\n\n\nError\n\\((b-1)(t-1)\\)\n\\(SS_\\text{Error}\\)\n\\(MS_\\text{Error} = SS_\\text{Error} / ((b-1)(t-1))\\)\n\n\n\nTotal\n\\(bt - 1\\)\n\\(SS_\\text{Total}\\)\n\n\n\n\n\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)): All treatment means are equal (\\(\\tau_i = 0\\) for all \\(i\\)).\nAlternative Hypothesis (\\(H_A\\)): At least one treatment mean is different.\n\nDecision Rule: Reject \\(H_0\\) if the F-ratio exceeds the critical value from the F-distribution with degrees of freedom \\((t-1, (b-1)(t-1))\\) at the chosen significance level (\\(\\alpha\\)).\n\nIntuitive Explanation: The F-ratio compares the variability due to treatments to the variability due to random error. A higher F-ratio indicates that treatments explain a significant portion of the variability in responses, suggesting that treatment effects are meaningful."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#example-bread-baking-experiment",
    "href": "lectures/week-08_rcbd-blocking_v0.html#example-bread-baking-experiment",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Example: Bread-Baking Experiment",
    "text": "Example: Bread-Baking Experiment\n\nScenario\nAn experimenter aims to compare four bread recipes (treatments) across three oven shelves (blocks). Each shelf is expected to have similar conditions, making it an ideal blocking factor.\n\n\nStep-by-Step R Code\n\n1. Load Data and Prepare Factors\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:3, each = 4))\ntreatments &lt;- factor(rep(1:4, times = 3))\n\n# Simulate response variable (e.g., loaf weight in grams)\nresponse &lt;- c(\n    rnorm(4, mean = 500, sd = 10), # Block 1\n    rnorm(4, mean = 505, sd = 10), # Block 2\n    rnorm(4, mean = 498, sd = 10) # Block 3\n)\n\n# Create data frame\ndata &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data)\n\nOutput Interpretation: The dataset comprises three blocks, each containing all four treatments. The response variable represents the loaf weight, simulated with slight block-specific means to reflect potential block effects.\n\n\n2. Fit the RCBD Model\n\n# Fit the RCBD model using ANOVA\nmodel &lt;- aov(response ~ blocks + treatments, data = data)\n\n# Summary of the model\nsummary(model)\n\nInterpretation:\n\nBlocks: Capture the variability due to different oven shelves.\nTreatments: Assess the effect of different bread recipes after accounting for block effects.\nError: Represents unexplained variability.\n\n\n\n3. Assess Blocking Efficiency\nCompare the Mean Square for Blocks (\\(MS_\\text{Block}\\)) with Mean Square for Error (\\(MS_\\text{Error}\\)). A higher \\(MS_\\text{Block}\\) indicates substantial block effects, enhancing the precision of treatment comparisons.\n\n\n4. Diagnostic Plots\n\n# Diagnostic plots for the ANOVA model\npar(mfrow = c(2, 2))\nplot(model)\npar(mfrow = c(1, 1))\n\nInterpretation:\n\nResiduals vs Fitted: Check for homoscedasticity and non-linearity.\nNormal Q-Q: Assess normality of residuals.\nScale-Location: Evaluate homogeneity of variance.\nResiduals vs Leverage: Identify influential observations.\n\n\n\n5. Multiple Comparisons (Post-Hoc Analysis)\n\n# Load emmeans package for estimated marginal means\nlibrary(emmeans)\n\n# Estimated marginal means for treatments\nemm &lt;- emmeans(model, ~treatments)\n\n# Pairwise comparisons\npairwise_comparisons &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise_comparisons)\n\nInterpretation: Identify which bread recipes significantly differ in loaf weight after adjusting for oven shelf effects."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#general-complete-block-designs-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#general-complete-block-designs-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "General Complete Block Designs",
    "text": "General Complete Block Designs\n\nModel\nFor designs where treatments are replicated within blocks, the model extends to include interaction effects:\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit}\n\\]\nWhere:\n\n\\(Y_{hit}\\): Response for treatment \\(i\\) in block \\(h\\), replicate \\(t\\).\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\((\\theta \\tau)_{hi}\\): Interaction effect between block \\(h\\) and treatment \\(i\\).\n\\(\\epsilon_{hit}\\): Random error.\n\n\n\nSample Size Calculations\nDetermining the appropriate sample size is crucial for ensuring adequate power in detecting treatment effects.\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Variance estimate.\n\\(\\phi\\): Desired power.\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\nDeeper Insight: Sample size calculations consider the balance between desired statistical power and the practical constraints of the experiment. Increasing the number of blocks or replicates can enhance the power but may require more resources."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#handling-block-treatment-interactions",
    "href": "lectures/week-08_rcbd-blocking_v0.html#handling-block-treatment-interactions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Handling Block-Treatment Interactions",
    "text": "Handling Block-Treatment Interactions\nIn scenarios where interaction effects are suspected, it’s essential to include them in the model to avoid biased estimates.\n\nModel with Interaction\n\\[\nY_{hit} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hit}\n\\]\nImplications:\n\nSignificant Interaction: Indicates that the effect of treatments varies across blocks, necessitating separate treatment effect analyses within each block.\nNon-Significant Interaction: Supports the additive model, validating the use of standard RCBD analysis."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#example-resting-metabolic-rate",
    "href": "lectures/week-08_rcbd-blocking_v0.html#example-resting-metabolic-rate",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Example: Resting Metabolic Rate",
    "text": "Example: Resting Metabolic Rate\n\nScenario\nA study aims to compare three different exercise protocols (Treatments A, B, C) on subjects’ resting metabolic rates. The subjects are grouped into four blocks based on age groups to control for age-related variability.\n\n\nStep-by-Step R Code\n\n1. Load and Prepare Data\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Define blocks and treatments\nblocks &lt;- factor(rep(1:4, each = 3))\ntreatments &lt;- factor(rep(c(\"A\", \"B\", \"C\"), times = 4))\n\n# Simulate response variable (e.g., resting metabolic rate in kcal/day)\nresponse &lt;- c(\n    rnorm(3, mean = 1500, sd = 50), # Block 1\n    rnorm(3, mean = 1550, sd = 50), # Block 2\n    rnorm(3, mean = 1480, sd = 50), # Block 3\n    rnorm(3, mean = 1520, sd = 50) # Block 4\n)\n\n# Create data frame\ndata &lt;- data.frame(blocks, treatments, response)\n\n# View the data\nprint(data)\n\n\n\n2. Fit the RCBD Model\n\n# Fit the RCBD model using ANOVA\nmodel &lt;- aov(response ~ blocks + treatments, data = data)\n\n# Summary of the model\nsummary(model)\n\nInterpretation:\n\nBlocks: Account for age-related variability.\nTreatments: Assess the impact of exercise protocols on metabolic rate after controlling for age.\n\n\n\n3. Diagnostic Plots\n\n# Diagnostic plots for the ANOVA model\npar(mfrow = c(2, 2))\nplot(model)\npar(mfrow = c(1, 1))\n\nInterpretation: Ensure that residuals meet ANOVA assumptions. Look for random scatter in Residuals vs Fitted, linearity in Scale-Location, and normality in Q-Q plots.\n\n\n4. Post-Hoc Analysis\n\n# Load emmeans package for estimated marginal means\nlibrary(emmeans)\n\n# Estimated marginal means for treatments\nemm &lt;- emmeans(model, ~treatments)\n\n# Pairwise comparisons\npairwise_comparisons &lt;- contrast(emm, method = \"pairwise\")\nsummary(pairwise_comparisons)\n\nInterpretation: Determine which exercise protocols significantly differ in their effect on resting metabolic rates after adjusting for age."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#conclusion",
    "href": "lectures/week-08_rcbd-blocking_v0.html#conclusion",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Conclusion",
    "text": "Conclusion\nComplete Block Designs (CBDs), particularly Randomized Complete Block Designs (RCBDs), are indispensable in experimental research for controlling variability due to nuisance factors. By structuring experiments to include blocks where each treatment is represented, CBDs enhance the precision and reliability of treatment effect estimates. The integration of ANOVA facilitates robust analysis, allowing researchers to discern meaningful differences among treatments while accounting for block-induced variability."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#key-takeaways",
    "href": "lectures/week-08_rcbd-blocking_v0.html#key-takeaways",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBlocking: Reduces variability from known sources, increasing the efficiency of treatment comparisons.\nANOVA in RCBDs: Decomposes variability to isolate treatment effects, providing a clear framework for hypothesis testing.\nGeneral Complete Block Designs: Extend the utility of CBDs to more complex scenarios involving treatment replications within blocks.\nPractical Implementation: R offers comprehensive tools for designing, analyzing, and interpreting CBDs, empowering researchers with robust statistical capabilities.\nAdvanced Topics: Understanding interactions and sample size considerations enhances the depth and applicability of CBDs in diverse research contexts.\n\nMastery of Complete Block Designs equips graduate students with the methodological rigor necessary for conducting sophisticated experiments, ensuring that findings are both accurate and generalizable."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#proof-of-rcbd-model-assumptions",
    "href": "lectures/week-08_rcbd-blocking_v0.html#proof-of-rcbd-model-assumptions",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Proof of RCBD Model Assumptions",
    "text": "Proof of RCBD Model Assumptions\n\n1. Variance Decomposition\nThe RCBD model is expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Error term.\n\n\n\nAssumptions\n\nNormality: \\(\\epsilon_{hi}\\) are normally distributed.\nIndependence: Observations are independent.\nHomoscedasticity: \\(\\sigma^2\\) is constant across all treatments and blocks.\nNo Interaction: Effects of blocks and treatments are additive.\n\n\n\nVariance Decomposition\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]\n\n\nDerivation\n\nTotal Sum of Squares (SS\\(_\\text{Total}\\)):\n\n\\[\nSS_\\text{Total} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}\\) is the grand mean.\n\nBlock Sum of Squares (SS\\(_\\text{Block}\\)):\n\n\\[\nSS_\\text{Block} = t \\sum_{h=1}^b (\\bar{Y}_h - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_h\\) is the mean of block \\(h\\).\n\nTreatment Sum of Squares (SS\\(_\\text{Treatment}\\)):\n\n\\[\nSS_\\text{Treatment} = b \\sum_{i=1}^t (\\bar{Y}_i - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_i\\) is the mean of treatment \\(i\\).\n\nError Sum of Squares (SS\\(_\\text{Error}\\)):\n\n\\[\nSS_\\text{Error} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y}_h - \\bar{Y}_i + \\bar{Y})^2\n\\]\nRepresents variability not explained by blocks or treatments.\nConclusion: The variance decomposition allows for the assessment of how much variability in the response is attributable to blocks, treatments, and random error."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#effects-of-ignoring-block-treatment-interaction",
    "href": "lectures/week-08_rcbd-blocking_v0.html#effects-of-ignoring-block-treatment-interaction",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Effects of Ignoring Block-Treatment Interaction",
    "text": "Effects of Ignoring Block-Treatment Interaction\nIn the RCBD model, it is assumed that there is no interaction between blocks and treatments. Ignoring a significant block-treatment interaction can lead to biased estimates of treatment effects.\n\nMathematical Illustration\nSuppose the true model includes an interaction term:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nIf the interaction term \\((\\theta \\tau)_{hi}\\) is significant and is omitted from the model, the estimates of \\(\\tau_i\\) will be biased because the model incorrectly attributes part of the interaction effect to treatment effects.\n\n\nProof Outline\n\nFull Model: Includes interaction.\nReduced Model: Omits interaction.\nBias Derivation: Show that \\(\\hat{\\tau}_i\\) in the reduced model includes components of the interaction effect.\n\nConclusion: It is crucial to test for block-treatment interactions. If significant, a more complex model that includes interaction terms should be employed."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#sample-size-calculations-for-rcbds",
    "href": "lectures/week-08_rcbd-blocking_v0.html#sample-size-calculations-for-rcbds",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Sample Size Calculations for RCBDs",
    "text": "Sample Size Calculations for RCBDs\nDetermining the appropriate sample size is essential for achieving sufficient power to detect treatment effects.\n\nFormula\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Estimated variance of the response.\n\\(\\phi\\): Desired power (e.g., 0.80).\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\n\n\nDerivation\n\nPower Analysis: Based on the desired power (\\(\\phi\\)) and significance level (\\(\\alpha\\)), determine the non-centrality parameter needed to detect effect size \\(\\Delta\\).\nRearrange for \\(n\\): Solving the power equation for \\(n\\) yields the sample size formula.\n\nExample Calculation: Suppose you have 4 treatments (\\(v = 4\\)), 5 blocks (\\(b = 5\\)), an estimated variance (\\(\\sigma^2 = 20\\)), desire a power of 0.80 (\\(\\phi = 0.84\\)), and aim to detect a minimum effect size of 5 units (\\(\\Delta = 5\\)).\n\\[\nn \\geq \\frac{2 \\times 4 \\times 20 \\times 0.84^2}{5 \\times 5^2} = \\frac{2 \\times 4 \\times 20 \\times 0.7056}{125} \\approx \\frac{113.792}{125} \\approx 0.91\n\\]\nSince \\(n\\) must be an integer, at least 1 replicate per treatment is needed. However, practical considerations often necessitate larger sample sizes."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#medium-difficulty",
    "href": "lectures/week-08_rcbd-blocking_v0.html#medium-difficulty",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction:\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate X.\nFit an ANCOVA model adjusting for X.\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\n\n\nR Code Example:\n\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\nX &lt;- rnorm(90, mean = 50, sd = 10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- 5 + ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4)) +\n    0.5 * (X - mean(X)) + rnorm(90, mean = 0, sd = 3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim, ~Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\nQuestions:\n\nWhat are the main effects of Treatment and the covariate X?\nAre there significant differences between treatments after adjusting for X?\nDo the diagnostic plots suggest any assumption violations?\n\n\nBalloon Data Analysis:\n\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\n\n\nR Code Example:\n\nlibrary(car)\nlibrary(emmeans)\n\n# Load and prepare data\nballoon.data &lt;- read.table(\"data/dean2017/balloon.txt\", header = TRUE)\n\nballoon.data |&gt; dplyr::glimpse()\n\nballoon.data &lt;- within(balloon.data, {\n    x &lt;- Order - mean(Order)\n    fColor &lt;- factor(Color)\n})\n\n# Fit ANCOVA model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n# Test homogeneity of slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n# Estimated marginal means\nemm_balloon &lt;- emmeans(model, ~fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\nQuestions:\n\nDoes the interaction term indicate equality of slopes across treatments?\nHow do the adjusted means compare across balloon colors?\nWhat conclusions can be drawn about the effect of balloon color on inflation time after adjusting for run order?"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#challenging-problems",
    "href": "lectures/week-08_rcbd-blocking_v0.html#challenging-problems",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor ANCOVA Analysis:\n\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\n\n\nR Code Example:\n\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean = 0, sd = 3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- car::Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- car::Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#discussion-points",
    "href": "lectures/week-08_rcbd-blocking_v0.html#discussion-points",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Discussion Points",
    "text": "Discussion Points\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\n\nThree-Factor Interaction Interpretation:\n\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#r-code-example",
    "href": "lectures/week-08_rcbd-blocking_v0.html#r-code-example",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "R Code Example",
    "text": "R Code Example\n\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) + 0.5 * (X - mean(X)) + ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0, ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2) + rnorm(72, mean = 0, sd = 3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x = FactorA, y = Y, color = FactorB, group = FactorB)) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(~FactorC) +\n    labs(title = \"Three-Way Interaction Plot\", x = \"Factor A\", y = \"Response\") +\n    theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#interpretation",
    "href": "lectures/week-08_rcbd-blocking_v0.html#interpretation",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction: The interaction plot shows how the interaction between Factor A and Factor B varies across levels of Factor C. Non-parallel lines across different facets (Factor C levels) indicate a significant three-way interaction.\nImpact on Main and Two-Way Interactions: Significant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation. The effect of one factor depends on the combination of other factors.\nPractical Implications: Understanding three-way interactions allows for more nuanced insights into how multiple factors jointly influence the response variable, leading to better-informed decisions and strategies.\n\nReference: Dean, Voss & Draguljić (2017) elaborate on interpreting higher-order interactions in Design and Analysis of Experiments.\n\nMathematical Proof of Interaction Contrasts:\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\n\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#proof-outline-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#proof-outline-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Proof Outline",
    "text": "Proof Outline\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij} + (\\alpha\\gamma)^*_{ik} + (\\beta\\gamma)^*_{jk} + (\\alpha\\beta\\gamma)^*_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\n\\((\\alpha\\beta)^*_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)^*_{ik}\\) and \\((\\beta\\gamma)^*_{jk}\\).\n\nThree-Way Interaction:\n\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\n\nProof of Additivity:\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)^*_{ij} = (\\alpha\\gamma)^*_{ik} = (\\beta\\gamma)^*{jk} = (\\alpha\\beta\\gamma)^*_{ijk} = 0\\)), the model simplifies to:\n\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nindicating that the response is purely additive with respect to the main effects.\n\nZero Interaction Contrasts:\nIf all interaction contrasts are zero, the model exhibits additivity. Thus, interaction contrasts quantify how much the actual model deviates from an additive one.\n\n\nConclusion:\n\nInteraction contrasts provide a mathematical measure of the presence and magnitude of interactions. Non-zero contrasts indicate that the combined effect of factors is not merely the sum of their individual effects, highlighting the importance of considering interactions in experimental analysis.\nReference: Dean, Voss & Draguljić (2017) offer detailed mathematical treatments of interaction contrasts in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#proof-of-rcbd-model-assumptions-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#proof-of-rcbd-model-assumptions-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Proof of RCBD Model Assumptions",
    "text": "Proof of RCBD Model Assumptions\n\n1. Variance Decomposition\nThe RCBD model is expressed as:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\epsilon_{hi}\n\\]\nWhere:\n\n\\(Y_{hi}\\): Response for treatment \\(i\\) in block \\(h\\).\n\\(\\mu\\): Overall mean.\n\\(\\theta_h\\): Effect of block \\(h\\).\n\\(\\tau_i\\): Effect of treatment \\(i\\).\n\\(\\epsilon_{hi} \\sim N(0, \\sigma^2)\\): Error term.\n\nAssumptions: 1. Normality: \\(\\epsilon_{hi}\\) are normally distributed. 2. Independence: Observations are independent. 3. Homoscedasticity: \\(\\sigma^2\\) is constant across all treatments and blocks. 4. No Interaction: Effects of blocks and treatments are additive.\nVariance Decomposition:\n\\[\n\\text{Total SS} = \\text{Block SS} + \\text{Treatment SS} + \\text{Error SS}\n\\]\nDerivation: 1. Total Sum of Squares (SS\\(_\\text{Total}\\)):\n\\[\nSS_\\text{Total} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}\\) is the grand mean.\n\nBlock Sum of Squares (SS\\(_\\text{Block}\\)):\n\n\\[\nSS_\\text{Block} = t \\sum_{h=1}^b (\\bar{Y}_h - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_h\\) is the mean of block \\(h\\).\n\nTreatment Sum of Squares (SS\\(_\\text{Treatment}\\)):\n\n\\[\nSS_\\text{Treatment} = b \\sum_{i=1}^t (\\bar{Y}_i - \\bar{Y})^2\n\\]\nWhere \\(\\bar{Y}_i\\) is the mean of treatment \\(i\\).\n\nError Sum of Squares (SS\\(_\\text{Error}\\)):\n\n\\[\nSS_\\text{Error} = \\sum_{h=1}^b \\sum_{i=1}^t (Y_{hi} - \\bar{Y}_h - \\bar{Y}_i + \\bar{Y})^2\n\\]\nRepresents variability not explained by blocks or treatments.\nConclusion: The variance decomposition allows for the assessment of how much variability in the response is attributable to blocks, treatments, and random error.\nReference: Montgomery (2019) provides a detailed derivation of ANOVA components in Design and Analysis of Experiments.\n\n\n2. Effects of Ignoring Block-Treatment Interaction\nIn the RCBD model, it is assumed that there is no interaction between blocks and treatments. Ignoring a significant block-treatment interaction can lead to biased estimates of treatment effects.\nMathematical Illustration:\nSuppose the true model includes an interaction term:\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nIf the interaction term \\((\\theta \\tau)_{hi}\\) is significant and is omitted from the model, the estimates of \\(\\tau_i\\) will be biased because the model incorrectly attributes part of the interaction effect to treatment effects.\nProof Outline: 1. Full Model: Includes interaction. 2. Reduced Model: Omits interaction. 3. Bias Derivation: Show that \\(\\hat{\\tau}_i\\) in the reduced model includes components of the interaction effect.\nConclusion: It is crucial to test for block-treatment interactions. If significant, a more complex model that includes interaction terms should be employed.\nReference: Kempthorne (1977) discusses the implications of model misspecification in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#sample-size-calculations-for-rcbds-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#sample-size-calculations-for-rcbds-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Sample Size Calculations for RCBDs",
    "text": "Sample Size Calculations for RCBDs\nDetermining the appropriate sample size is essential for ensuring adequate power to detect treatment effects.\n\nFormula\n\\[\nn \\geq \\frac{2 v \\sigma^2 \\phi^2}{b \\Delta^2}\n\\]\nWhere:\n\n\\(n\\): Number of replicates per treatment.\n\\(v\\): Number of treatments.\n\\(\\sigma^2\\): Variance estimate.\n\\(\\phi\\): Desired power (e.g., 0.80).\n\\(b\\): Number of blocks.\n\\(\\Delta\\): Minimum detectable effect size.\n\n\n\nDerivation\n\nPower Analysis: Based on the desired power (\\(\\phi\\)) and significance level (\\(\\alpha\\)), determine the non-centrality parameter needed to detect effect size \\(\\Delta\\).\nRearrange for \\(n\\): Solving the power equation for \\(n\\) yields the sample size formula.\n\nExample Calculation:\nSuppose you have 4 treatments (\\(v = 4\\)), 5 blocks (\\(b = 5\\)), an estimated variance (\\(\\sigma^2 = 20\\)), desire a power of 0.80 (\\(\\phi = 0.84\\)), and aim to detect a minimum effect size of 5 units (\\(\\Delta = 5\\)).\n\\[\nn \\geq \\frac{2 \\times 4 \\times 20 \\times 0.84^2}{5 \\times 5^2} = \\frac{2 \\times 4 \\times 20 \\times 0.7056}{125} \\approx \\frac{113.792}{125} \\approx 0.91\n\\]\nSince \\(n\\) must be an integer, at least 1 replicate per treatment is needed. However, practical considerations often necessitate larger sample sizes.\nReference: Montgomery (2019) elaborates on sample size determination in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#handling-block-treatment-interactions-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#handling-block-treatment-interactions-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Handling Block-Treatment Interactions",
    "text": "Handling Block-Treatment Interactions\nIn scenarios where interaction effects between blocks and treatments are suspected, it is essential to include them in the model to avoid biased estimates and incorrect inferences.\n\nModel with Interaction\n\\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\nImplications:\n\nSignificant Interaction: Suggests that the effect of treatments varies across blocks. Separate treatment effect analyses within each block may be necessary.\nNon-Significant Interaction: Supports the additive model, validating the use of standard RCBD analysis.\n\nReference: Kempthorne (1977) elaborates on the importance of accounting for interactions in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#sample-size-calculation-example-in-r",
    "href": "lectures/week-08_rcbd-blocking_v0.html#sample-size-calculation-example-in-r",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Sample Size Calculation Example in R",
    "text": "Sample Size Calculation Example in R\nTo determine the necessary number of replicates per treatment in an RCBD, you can use R for power and sample size calculations.\n\n# Parameters\nv &lt;- 4 # Number of treatments\nb &lt;- 5 # Number of blocks\nsigma2 &lt;- 20 # Variance estimate\nphi &lt;- 0.84 # Desired power (approx 0.80)\nDelta &lt;- 5 # Minimum detectable effect size\n\n# Calculate sample size\nn &lt;- ceiling((2 * v * sigma2 * phi^2) / (b * Delta^2))\nprint(n)\n\nInterpretation: The ceiling function ensures that the sample size is rounded up to the next integer, ensuring adequate power."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#medium-difficulty-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#medium-difficulty-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Medium Difficulty",
    "text": "Medium Difficulty\n\nTwo-Way ANCOVA with Interaction:\n\nDataset: Simulate a dataset with three treatments and one covariate.\nTasks:\n\nGenerate data for treatments A, B, C with a covariate X.\nFit an ANCOVA model adjusting for X.\nTest the significance of treatment effects and the covariate.\nGenerate diagnostic plots and interpret them.\n\n\n\nR Code Example:\n\nset.seed(123)\n\n# Define factors and covariate\nTreatment &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each=30))\nX &lt;- rnorm(90, mean=50,   sd=10)\n\n# Simulate responses with treatment effects and covariate effect\nY &lt;- ifelse(Treatment == \"A\", 0, ifelse(Treatment == \"B\", 2, 4) + 0.5 * (X - mean(X)) + rnorm(90, mean=0, sd=3)\n\ndata_sim &lt;- data.frame(Treatment, X, Y)\n\n# Fit ANCOVA model\nmodel_sim &lt;- lm(Y ~ Treatment + X, data = data_sim)\nsummary(model_sim)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_sim)\n\n# Estimated marginal means\nlibrary(emmeans)\nemm_sim &lt;- emmeans(model_sim ~ Treatment)\npairwise_sim &lt;- contrast(emm_sim, method = \"pairwise\")\nsummary(pairwise_sim)\n\nQuestions: - What are the main effects of Treatment and the covariate X? - Are there significant differences between treatments after adjusting for X? - Do the diagnostic plots suggest any assumption violations?\n\nBalloon Data Analysis:\n\nScenario: Reanalyze the balloon data, testing for equality of slopes.\nTasks:\n\nFit an ANCOVA model.\nTest the homogeneity of regression slopes.\nInterpret the adjusted treatment means.\n\n\n\n\nR Code Example\n\nlibrary(car)\nlibrary(emmeans)\n\n# Load and prepare data\nballoon.data &lt;- read.table(\"data/balloon.txt\", header = TRUE)\nballoon.data &lt;- within(balloon.data\n, x &lt;- Order - mean(Order)\nfColor &lt;- factor(Color)\n})\n\n# Fit ANCOVA model\nmodel &lt;- lm(Time ~ fColor + x, data = balloon.data)\nsummary(model)\n\n# Test homogeneity of slopes\nmodel_interaction &lt;- lm(Time ~ fColor * x, data = balloon.data)\nanova(model, model_interaction)\n\n# Estimated marginal means\nemm_balloon &lt;- emmeans(model ~ fColor)\npairwise_balloon &lt;- contrast(emm_balloon, method = \"pairwise\")\nsummary(pairwise_balloon)\n\nQuestions:\n\nDoes the interaction term indicate equality of slopes across treatments?\nHow do the adjusted means compare across balloon colors?\nWhat conclusions can be drawn about the effect of balloon color on inflation time after adjusting for run order?"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#challenging-problems-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#challenging-problems-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Challenging Problems",
    "text": "Challenging Problems\n\nUnbalanced Two-Factor ANCOVA Analysis:\n\nDataset: Utilize a dataset with unequal numbers of replicates across treatment combinations.\nTasks:\n\nSimulate an unbalanced dataset with two factors and one covariate.\nFit an ANCOVA model using both Type II and Type III sums of squares.\nCompare the results and discuss how imbalance affects the interpretation of main and interaction effects.\n\n\n\n\nR Code Example\n\nlibrary(car)\n\nset.seed(456)\n\n# Simulate unbalanced data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), times = c(4, 6, 5)))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 7, times = 2))\nX &lt;- rnorm(15, mean = 50, sd = 10)\n\n# Simulate responses with treatment and covariate effects\nY &lt;- 5 + ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) + ifelse(FactorB == \"B1\", 0, 3) + 0.5 * (X - mean(X)) + rnorm(15, mean = 0, sd = 3)\n\ndata_unbalanced &lt;- data.frame(FactorA, FactorB, X, Y)\n\n# Fit ANCOVA model\nfit_unbalanced &lt;- lm(Y ~ FactorA * FactorB + X, data = data_unbalanced)\n\n# Type II Sum of Squares\ntypeII &lt;- Anova(fit_unbalanced, type = \"II\")\nprint(typeII)\n\n# Type III Sum of Squares\ntypeIII &lt;- Anova(fit_unbalanced, type = \"III\")\nprint(typeIII)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#discussion-points-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#discussion-points-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Discussion Points:",
    "text": "Discussion Points:\n\nType II vs. Type III SS:\n\nType II: Tests main effects after accounting for other main effects but ignores interactions.\nType III: Tests main effects after accounting for all other factors, including interactions.\n\nImpact of Imbalance:\n\nImbalanced designs can lead to different interpretations under Type II and Type III SS.\nType I SS is not appropriate here as it is sequential and depends on the order of factors.\n\nSignificance of Effects:\n\nIdentify which factors and interactions are significant under each type.\nDiscuss how imbalance might inflate or deflate the significance levels of certain effects.\n\nImplications for Experimental Conclusions:\n\nChoosing the appropriate type of sums of squares is crucial for accurate inference.\nMisinterpretation can lead to incorrect conclusions about factor effects and interactions.\n\n\nReference: Christensen (2018) provides guidance on handling unbalanced designs and selecting appropriate sums of squares types in Analysis of Variance, Design, and Regression.\n\nThree-Factor Interaction Interpretation:\n\nDataset: Use a simulated dataset with three factors exhibiting a significant three-way interaction.\nTasks:\n\nSimulate data with three factors and one covariate.\nFit a three-way ANCOVA model.\nVisualize the three-way interaction.\nUse emmeans to estimate marginal means and perform pairwise comparisons.\nExplain how the three-way interaction affects the interpretation of main and two-way interactions.\n\n\n\n\nR Code Example:\n\nlibrary(emmeans)\nlibrary(ggplot2)\n\nset.seed(789)\n\n# Simulate data\nFactorA &lt;- factor(rep(c(\"A1\", \"A2\", \"A3\"), each = 24))\nFactorB &lt;- factor(rep(c(\"B1\", \"B2\"), each = 12, times = 3))\nFactorC &lt;- factor(rep(c(\"C1\", \"C2\", \"C3\"), times = 8))\nX &lt;- rnorm(72, mean = 50, sd = 10)\n\n# Simulate responses with three-way interaction\nY &lt;- 5 +\n    ifelse(FactorA == \"A1\", 0, ifelse(FactorA == \"A2\", 2, 4)) +\n    ifelse(FactorB == \"B1\", 0, 3) +\n    ifelse(FactorC == \"C1\", 0, ifelse(FactorC == \"C2\", 1.5, 3)) +\n    0.5 * (X - mean(X)) +\n    ifelse(FactorA == \"A1\" & FactorB == \"B1\", 0,\n        ifelse(FactorA == \"A2\" & FactorB == \"B2\", 1, 2)\n    ) +\n    rnorm(72, mean = 0, sd = 3)\n\ndata_three &lt;- data.frame(FactorA, FactorB, FactorC, X, Y)\n\n# Fit three-way ANCOVA model\nfit_three &lt;- lm(Y ~ FactorA * FactorB * FactorC + X, data = data_three)\nsummary(fit_three)\n\n# Plot three-way interaction\nggplot(data_three, aes(x = FactorA, y = Y, color = FactorB, group = FactorB)) +\n    geom_point() +\n    geom_line() +\n    facet_wrap(~FactorC) +\n    labs(title = \"Three-Way Interaction Plot\", x = \"Factor A\", y = \"Response\") +\n    theme_minimal()\n\n# Estimated marginal means\nemm_three &lt;- emmeans(fit_three, ~ FactorA * FactorB * FactorC)\ncontrast_results &lt;- contrast(emm_three, method = \"pairwise\", adjust = \"none\")\nsummary(contrast_results)"
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#interpretation-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#interpretation-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Interpretation",
    "text": "Interpretation\n\nThree-Way Interaction: The interaction plot illustrates how the interaction between Factor A and Factor B varies across different levels of Factor C. Non-parallel lines across facets indicate a significant three-way interaction.\nImpact on Main and Two-Way Interactions: Significant higher-order interactions imply that main effects and two-way interactions cannot be interpreted in isolation. The effect of one factor depends on the combination of other factors.\nPractical Implications: Understanding three-way interactions allows for more nuanced insights into how multiple factors jointly influence the response variable, leading to better-informed decisions and strategies.\n\nReference: Dean, Voss & Draguljić (2017) elaborate on interpreting higher-order interactions in Design and Analysis of Experiments.\n\nMathematical Proof of Interaction Contrasts:\n\nTask: Derive formulas for interaction contrasts in a three-factor design and prove that they measure deviations from additivity.\nRequirements:\n\nShow how to construct interaction contrasts.\nProve that if all interaction contrasts are zero, the model is additive.\n\n\nHints:\n\nUtilize the definitions of main effects and interaction terms.\nEmploy properties of linear algebra to demonstrate the relationship between interaction contrasts and additivity.\nReference Dean et al. (2017) and Christensen (2018) for theoretical foundations."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#proof-outline-2",
    "href": "lectures/week-08_rcbd-blocking_v0.html#proof-outline-2",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Proof Outline:",
    "text": "Proof Outline:\n\nDefine the Three-Factor Model:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)^*_{ij} + (\\alpha\\gamma)^*{ik} + (\\beta\\gamma)^*_{jk} + (\\alpha\\beta\\gamma)^*_{ijk} + \\epsilon_{ijk}\n\\]\nwhere \\(\\sum \\alpha_i = \\sum \\beta_j = \\sum \\gamma_k = 0\\).\n\nConstruct Interaction Contrasts:\n\n\nTwo-Way Interactions:\n\\((\\alpha\\beta)_{ij}\\) measures the interaction between Factors A and B.\nSimilarly for \\((\\alpha\\gamma)^*_{ik}\\) and \\((\\beta\\gamma)^*_{jk}\\).\nThree-Way Interaction:\n\\((\\alpha\\beta\\gamma)_{ijk}\\) measures the deviation from additivity among all three factors.\n\n\nProof of Additivity:\n\n\nAdditive Model: If all interaction terms are zero (\\((\\alpha\\beta)^*_{ij} = (\\alpha\\gamma)^*_{ik} = (\\beta\\gamma)^*_{jk} = (\\alpha\\beta\\gamma)^*_{ijk} = 0\\)), the model simplifies to:\n\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}\n\\]\nindicating that the response is purely additive with respect to the main effects.\n\nZero Interaction Contrasts:\nIf all interaction contrasts are zero, the model exhibits additivity. Thus, interaction contrasts quantify how much the actual model deviates from an additive one.\n\n\nConclusion:\n\nInteraction contrasts provide a mathematical measure of the presence and magnitude of interactions. Non-zero contrasts indicate that the combined effect of factors is not merely the sum of their individual effects, highlighting the importance of considering interactions in experimental analysis.\nReference: Dean, Voss & Draguljić (2017) offer detailed mathematical treatments of interaction contrasts in Design and Analysis of Experiments."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#adjusted-treatment-means",
    "href": "lectures/week-08_rcbd-blocking_v0.html#adjusted-treatment-means",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Adjusted Treatment Means",
    "text": "Adjusted Treatment Means\nThe adjusted mean for treatment \\(i\\) accounts for the influence of the covariate, providing a more accurate estimate of the treatment effect.\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nWhere:\n\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect for treatment \\(i\\).\n\\(\\bar{Y}_i\\): Mean response for treatment \\(i\\).\n\\(\\beta\\): Regression coefficient for the covariate.\n\\(\\bar{X}_i\\): Mean covariate value for treatment \\(i\\).\n\\(\\bar{X}\\): Overall mean of the covariate."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#derivation-3",
    "href": "lectures/week-08_rcbd-blocking_v0.html#derivation-3",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Derivation",
    "text": "Derivation\n\nModel Equation:\n\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta (X_{ij} - \\bar{X}) + \\epsilon_{ij}\n\\]\n\nTaking the Mean for Treatment \\(i\\):\n\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X}) + \\bar{\\epsilon}_i\n\\]\nSince \\(\\bar{\\epsilon}_i = 0\\) (assuming errors have mean zero),\n\\[\n\\bar{Y}_i = \\mu + \\tau_i + \\beta (\\bar{X}_i - \\bar{X})\n\\]\n\nSolving for Adjusted Mean:\n\n\\[\n\\mu + \\tau_i = \\bar{Y}_i - \\beta (\\bar{X}_i - \\bar{X})\n\\]\nThis equation represents the adjusted treatment mean, accounting for differences in the covariate.\nReference: Montgomery (2019) and Scheffé (1959) provide detailed derivations of adjusted means in ANCOVA.\n\nF-Test for Homogeneity of Slopes\nTesting the assumption of homogeneity of regression slopes ensures that the relationship between the covariate and the response is consistent across all treatment groups.\n\n\nModel Comparison\n\nModel Without Interaction (Assuming Homogeneous Slopes): \\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + \\epsilon_{hi}\n\\]\n\nWhere \\(\\beta\\) is the common slope for all treatments.\n\nModel With Interaction (Allowing Slopes to Vary): \\[\nY_{hi} = \\mu + \\theta_h + \\tau_i + \\beta X_{hi} + (\\theta \\tau)^*_{hi} + \\epsilon^*_{hi}\n\\]\n\nWhere \\((\\theta \\tau)_{hi}\\) allows the slope of the covariate to vary with treatments.\n\n\nHypothesis Testing\n\nNull Hypothesis (\\(H_0\\)): Homogeneity of slopes (\\(\\beta\\) is the same across treatments).\nAlternative Hypothesis (\\(H_A\\)): Slopes vary across treatments (\\(\\beta_i \\neq \\beta_j\\) for some \\(i, j\\)).\n\n\n\nTest Statistic\n\\[\nF = \\frac{\\text{SSR}^*_{\\text{Interaction}} / \\text{DF}^*_{\\text{Interaction}}}{\\text{SSE} / \\text{DF}_{\\text{Error}}}\n\\]\nWhere:\n\n\\(\\text{SSR}_{\\text{Interaction}}\\): Sum of squares for the interaction term.\n\\(\\text{DF}_{\\text{Interaction}}\\): Degrees of freedom for the interaction.\n\\(\\text{SSE}\\): Sum of squares for error.\n\\(\\text{DF}_{\\text{Error}}\\): Degrees of freedom for error.\n\nDecision Rule: Reject \\(H_0\\) if \\(F\\) exceeds the critical value from the F-distribution with degrees of freedom \\((\\text{DF}*{\\text{Interaction}}, \\text{DF}*{\\text{Error}})\\).\nConclusion: A significant \\(F\\)-statistic indicates that the slopes are not homogeneous, violating a key ANCOVA assumption.\nReference: Scheffé (1959) elaborates on hypothesis testing for interaction terms in ANCOVA."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#confidence-intervals-for-higher-order-contrasts",
    "href": "lectures/week-08_rcbd-blocking_v0.html#confidence-intervals-for-higher-order-contrasts",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Confidence Intervals for Higher-Order Contrasts",
    "text": "Confidence Intervals for Higher-Order Contrasts\nFor higher-order interactions involving multiple factors, confidence intervals for contrasts provide insights into the magnitude and significance of these interactions.\n\nFormula\n\\[\n\\text{CI} = \\hat{L} \\pm t_{\\alpha/2, df} \\sqrt{\\text{Var}(\\hat{L})}\n\\]\nWhere:\n\n\\(\\hat{L}\\): Estimated contrast.\n\\(t_{\\alpha/2, df}\\): Critical value from the t-distribution.\n\\(\\text{Var}(\\hat{L})\\): Variance of the contrast estimate.\n\n\n\nSteps in R\n\nEstimate the Contrast: Using the emmeans package to define and estimate contrasts.\nCalculate the Confidence Interval: The emmeans package automatically provides confidence intervals when performing contrasts.\n\n\n\nExample:\n\nlibrary(emmeans)\n\n# Fit the ANCOVA model\nmodel &lt;- lm(Y ~ Treatment + X, data = data_sim)\n\n# Define estimated marginal means\nemm &lt;- emmeans(model, ~Treatment)\n\n# Define a contrast (e.g., Treatment B vs. Treatment A)\ncontrast_AB &lt;- contrast(emm, method = \"pairwise\")\nsummary(contrast_AB, infer = TRUE)\n\n\n\nInterpretation:\nThe output provides the estimated difference, standard error, t-value, and confidence interval for the contrast. If the confidence interval does not include zero, the contrast is statistically significant.\nReference: Christensen (2018) discusses confidence intervals for contrasts in Analysis of Variance, Design, and Regression."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#conclusion-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#conclusion-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Conclusion",
    "text": "Conclusion\nComplete Block Designs (CBDs), particularly Randomized Complete Block Designs (RCBDs), are invaluable in experimental research for controlling variability due to nuisance factors. By structuring experiments to include blocks where each treatment is represented, CBDs enhance the precision and reliability of treatment effect estimates. The integration of ANOVA facilitates robust analysis, allowing researchers to discern meaningful differences among treatments while accounting for block-induced variability."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#key-takeaways-1",
    "href": "lectures/week-08_rcbd-blocking_v0.html#key-takeaways-1",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBlocking: Reduces variability from known sources, increasing the efficiency of treatment comparisons.\nANOVA in RCBDs: Decomposes variability to isolate treatment effects, providing a clear framework for hypothesis testing.\nGeneral Complete Block Designs: Extend the utility of CBDs to more complex scenarios involving treatment replications within blocks.\nPractical Implementation: R offers comprehensive tools for designing, analyzing, and interpreting CBDs, empowering researchers with robust statistical capabilities.\nAdvanced Topics: Understanding interactions and sample size considerations enhances the depth and applicability of CBDs in diverse research contexts."
  },
  {
    "objectID": "lectures/week-08_rcbd-blocking_v0.html#references",
    "href": "lectures/week-08_rcbd-blocking_v0.html#references",
    "title": "Randomized Complete Block Design (RCBD) with Blocking",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer.\nMontgomery, D. C., & Runger, G. C. (2010). Applied Statistics and Probability for Engineers. Wiley.\nChristensen, R. (2018). Analysis of Variance, Design, and Regression. Chapman and Hall/CRC.\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company.\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html",
    "href": "lectures/week-05_factorial-anova.html",
    "title": "Factorial ANOVA",
    "section": "",
    "text": "Understand the structure and terminology of two-factor factorial designs.\nDistinguish between main effects and interaction effects in a two-way ANOVA.\nConduct a two-way ANOVA, interpret results, and create interaction plots in R.\nHandle complexities such as unbalanced designs and understand different types of sums of squares."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#objectives",
    "href": "lectures/week-05_factorial-anova.html#objectives",
    "title": "Factorial ANOVA",
    "section": "",
    "text": "Understand the structure and terminology of two-factor factorial designs.\nDistinguish between main effects and interaction effects in a two-way ANOVA.\nConduct a two-way ANOVA, interpret results, and create interaction plots in R.\nHandle complexities such as unbalanced designs and understand different types of sums of squares."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#r-packages",
    "href": "lectures/week-05_factorial-anova.html#r-packages",
    "title": "Factorial ANOVA",
    "section": "R Packages",
    "text": "R Packages\nTo run the R code examples in these notes, we recommend installing the following packages:\n\n# Required R Packages\npacman::p_load(dplyr, ggplot2, lsmeans, emmeans, here, readr)  \n\n\ndplyr: Data manipulation and summarization.\nggplot2: Data visualization.\nlsmeans and emmeans: Post hoc comparisons and multiple comparisons.\npacman: Package management.\nhere: File path management.\nreadr: Reading data files."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#checking-the-assumptions-on-the-model",
    "href": "lectures/week-05_factorial-anova.html#checking-the-assumptions-on-the-model",
    "title": "Factorial ANOVA",
    "section": "Checking the Assumptions on the Model",
    "text": "Checking the Assumptions on the Model\nThe assumptions for both the two-way complete model and the two-way main-effects model are that the error random variables have equal variances, are mutually independent, and are normally distributed. The strategy for checking these assumptions follows the procedures outlined in Chapter 5 (see Dean et al. 2017).\n\nStandardized Residuals\nThe standardized residuals are computed as\n\\[\nz_{ijt} = \\frac{y_{ijt} - \\hat{y}_{ijt}}{\\sqrt{SS_{Error}/(n - 1)}},\n\\]\nwhere\n\n\\(\\hat{y}_{ijt} = \\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j + (\\alpha\\beta)_{ij}\\) (two-way complete model), or\n\\(\\hat{y}_{ijt} = \\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j\\) (two-way main-effects model),\n\ndepending on which model is used. A “hat” denotes a least squares estimate.\nTypical residual plots to check:\n\nObservation Order vs. Residuals to assess independence.\nLevels of Each Factor (and possibly \\(\\hat{y}_{ijt}\\)) vs. Residuals to detect outliers and variance inhomogeneity.\nNormal Q-Q Plot (Normal Scores) to assess normality.\n\nWhen using the main-effects model, interaction plots (e.g., Figures 6.1, 6.2 in reference texts) help visualize any potential interaction. Another method is to plot standardized residuals against one factor’s levels while labeling or grouping by the second factor’s levels."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#contrasts-for-main-effects-and-interactions",
    "href": "lectures/week-05_factorial-anova.html#contrasts-for-main-effects-and-interactions",
    "title": "Factorial ANOVA",
    "section": "Contrasts for Main Effects and Interactions",
    "text": "Contrasts for Main Effects and Interactions\nRecalling the cell-means model\n\\[\ny_{ijt} = \\mu + \\tau_{ij} + \\varepsilon_{ijt},\n\\]\nequivalent to a one-way ANOVA model with \\(ab\\) treatments, we have that all contrasts in \\(\\tau_{ij}\\) are estimable. In two-way layouts, common contrasts include:\n\nTreatment Contrasts \\(\\sum_{i,j} d_{ij}\\,\\tau_{ij}\\).\nInteraction Contrasts measure non-parallelism in interaction plots. For instance,\n\n\\[\n(\\tau_{sh} - \\tau_{(s+1)h}) - (\\tau_{sq} - \\tau_{(s+1)q}).\n\\]\nUsing the two-way complete model \\(\\tau_{ij} = \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}\\), an interaction contrast zeroes out if \\((\\alpha\\beta)_{ij}=0\\) for all \\(i,j\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#writing-contrasts-as-coefficient-lists",
    "href": "lectures/week-05_factorial-anova.html#writing-contrasts-as-coefficient-lists",
    "title": "Factorial ANOVA",
    "section": "Writing Contrasts as Coefficient Lists",
    "text": "Writing Contrasts as Coefficient Lists\nWhen working in the two-way complete model, we can express contrasts in two ways:\n\nAs a list of coefficients of \\(\\alpha_i^*\\), \\(\\beta_j^*\\), and \\((\\alpha\\beta)_{ij}\\).\nAs a list of coefficients of the \\(\\tau_{ij}\\).\n\nExample 6.3.1 (Battery Experiment) A two-factor experiment with two levels each: “duty” (1 = alkaline, 2 = heavy duty) and “brand” (1 = name brand, 2 = store brand). Treatment combinations are \\((1,1)\\), \\((1,2)\\), \\((2,1)\\), and \\((2,2)\\), each with \\(r=4\\) observations. An interaction contrast might be\n\\[\n\\tau_{11} - \\tau_{12} - \\tau_{21} + \\tau_{22}\n\\;=\\; (\\alpha\\beta)_{11} - (\\alpha\\beta)_{12} - (\\alpha\\beta)_{21} + (\\alpha\\beta)_{22}.\n\\]"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#least-squares-estimators",
    "href": "lectures/week-05_factorial-anova.html#least-squares-estimators",
    "title": "Factorial ANOVA",
    "section": "Least Squares Estimators",
    "text": "Least Squares Estimators\nFrom standard ANOVA theory,\n\n\\(\\hat{\\mu} + \\hat{\\tau}_{ij} = \\hat{\\mu} + \\hat{\\alpha}_i + \\hat{\\beta}_j + \\widehat{(\\alpha\\beta)}_{ij} = \\bar{Y}_{ij\\cdot}\\),\n\\(\\text{Var}(\\bar{Y}_{ij\\cdot}) = \\sigma^2 / r_{ij}\\).\n\nAn interaction contrast \\(\\sum_{i,j} d_{ij}\\,\\tau_{ij}\\) has LSE \\(\\sum_{i,j} d_{ij}\\,\\bar{Y}_{ij\\cdot}\\) with variance \\(\\sigma^2 \\sum d_{ij}^2/r_{ij}\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#estimation-of-sigma2",
    "href": "lectures/week-05_factorial-anova.html#estimation-of-sigma2",
    "title": "Factorial ANOVA",
    "section": "Estimation of \\(\\sigma^2\\)",
    "text": "Estimation of \\(\\sigma^2\\)\nBecause the two-way complete model is equivalent to a one-way ANOVA with \\(ab\\) cells, an unbiased estimate of \\(\\sigma^2\\) is obtained via the error sum of squares:\n\\[\nSS_{Error} = \\sum_{i,j,t} (y_{ijt} - \\bar{Y}_{ij\\cdot})^2,\n\\]\nwith \\(v = ab\\) levels and \\(n = \\sum_{i,j} r_{ij}\\) total observations. The corresponding mean square error is\n\\[\nMS_{Error} = \\frac{SS_{Error}}{n - v}.\n\\]\nExample 6.4.2 (Reaction Time Experiment) A pilot experiment with \\(a=2\\) levels of “Cue Stimulus” and \\(b=3\\) levels of “Cue Time,” each having \\(r=3\\) replications, yields data in Table 6.2. The sum of squares for error is computed as\n\\[\nSS_{Error} = \\sum_{i,j,t} y_{ijt}^2 \\;-\\; \\sum_{i,j} \\frac{r_{ij}\\,\\bar{Y}_{ij\\cdot}^2}{\\cdot} \\;=\\; 0.00347,\n\\]\nand hence \\(MS_{Error} = SS_{Error}/(n-ab)\\).\n\nMultiple Comparisons with Unequal Variances\nIf variance heterogeneity persists and cannot be corrected by transformations, one can use Satterthwaite’s approximation (Christensen 2018)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-the-complete-model",
    "href": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-the-complete-model",
    "title": "Factorial ANOVA",
    "section": "Analysis of Variance for the Complete Model",
    "text": "Analysis of Variance for the Complete Model\nWe commonly examine three hypotheses:\n\nInteraction \\(H_{0}^{AB}\\): \\((\\alpha\\beta)_{ij} = 0\\) for all \\(i,j\\).\nMain Effect of A \\(H_{0}^{A}\\): \\(\\alpha_1^* = \\alpha_2^* = \\cdots = \\alpha_a^*\\).\nMain Effect of B \\(H_{0}^{B}\\): \\(\\beta_1^* = \\beta_2^* = \\cdots = \\beta_b^*\\).\n\nIf \\(H_{0}^{AB}\\) is not rejected, we then focus on main effects. Otherwise, the cell-means model is typically used to compare all \\(ab\\) treatment combinations.\nExample 6.4.5 (Reaction Time Experiment, continued) An ANOVA table yields:\n\n\\(MS_{AB}/MS_{Error} = 1.46 &lt; F_{2,12,0.01} = 6.93\\) (Fail to reject interaction)\n\\(MS_{A}/MS_{Error} = 81.38 &gt; 9.33\\) (Reject \\(H_{0}^{A}\\))\n\\(MS_{B}/MS_{Error} = 2.00 &lt; 6.93\\) (Fail to reject \\(H_{0}^{B}\\))"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#least-squares-estimators-equal-r",
    "href": "lectures/week-05_factorial-anova.html#least-squares-estimators-equal-r",
    "title": "Factorial ANOVA",
    "section": "Least Squares Estimators (Equal \\(r\\))",
    "text": "Least Squares Estimators (Equal \\(r\\))\nIf \\(r\\) is constant across all \\(ab\\) cells, the LSE of \\(E[y_{ijt}] = \\mu + \\alpha_i + \\beta_j\\) is\n\\[\n\\hat{\\mu} + \\hat{\\alpha}_i + \\hat{\\beta}_j = \\bar{Y}_{i\\cdot\\cdot} + \\bar{Y}_{\\cdot j \\cdot} - \\bar{Y}_{\\cdot\\cdot\\cdot}.\n\\]\nPairwise comparisons (e.g., \\(\\alpha_p - \\alpha_s\\)) become \\(\\bar{Y}_{p\\cdot\\cdot} - \\bar{Y}_{s\\cdot\\cdot}\\).\nExample 6.5.1 (Nail Varnish Experiment) Two solvents (\\(A\\)) and three brands of varnish (\\(B\\)), each with \\(r=5\\). From the data, the difference in solvents is estimated as\n\\[\n\\hat{\\alpha}_1 - \\hat{\\alpha}_2 = \\bar{Y}_{1\\cdot\\cdot} - \\bar{Y}_{2\\cdot\\cdot} \\approx -3.6040.\n\\]"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#estimation-of-sigma2-main-effects-model",
    "href": "lectures/week-05_factorial-anova.html#estimation-of-sigma2-main-effects-model",
    "title": "Factorial ANOVA",
    "section": "Estimation of \\(\\sigma^2\\) (Main-Effects Model)",
    "text": "Estimation of \\(\\sigma^2\\) (Main-Effects Model)\nWith \\(n = abr\\), the sum of squares of errors is\n\\[\nSS_{Error} = \\sum_{i=1}^a \\sum_{j=1}^b \\sum_{t=1}^{r}\n\\left( y_{ijt} - \\bar{Y}_{i\\cdot\\cdot} - \\bar{Y}_{\\cdot j \\cdot} + \\bar{Y}_{\\cdot\\cdot\\cdot}\\right)^2.\n\\]\nThen\n\\[\nMS_{Error} = \\frac{SS_{Error}}{n - a - b + 1}.\n\\]\nExample 6.5.2 (Nail Varnish Experiment, continued) With \\(a=2\\), \\(b=3\\), \\(r=5\\), the sum of squares \\(SS_{Error}=216.7762\\) and \\(MS_{Error} = 216.7762/(30-2-3+1) \\approx 8.3375\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#multiple-comparisons-in-the-main-effects-model",
    "href": "lectures/week-05_factorial-anova.html#multiple-comparisons-in-the-main-effects-model",
    "title": "Factorial ANOVA",
    "section": "Multiple Comparisons in the Main-Effects Model",
    "text": "Multiple Comparisons in the Main-Effects Model\nFor equal sample sizes, Tukey, Bonferroni, and Scheffé methods still apply to main-effect contrasts. For example, a set of 100\\((1-\\alpha)\\%\\) simultaneous CIs for factor-A contrasts has the form:\n\\[\n\\sum_i c_i \\alpha_i \\;\\in\\; \\sum_i c_i \\bar{Y}_{i\\cdot\\cdot}\n\\;\\pm\\; w \\; \\sqrt{MS_{Error} \\,\\sum_i \\frac{c_i^2}{br}},\n\\]\nwhere \\(w\\) depends on the chosen procedure (Dean et al. 2017).\nExample 6.5.3 (Nail Varnish Experiment, continued) Simultaneous confidence intervals for differences among three varnishes can be constructed via Tukey’s HSD with \\(q_{3,26,0.01} = 4.54\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-equal-sample-sizes",
    "href": "lectures/week-05_factorial-anova.html#analysis-of-variance-for-equal-sample-sizes",
    "title": "Factorial ANOVA",
    "section": "Analysis of Variance for Equal Sample Sizes",
    "text": "Analysis of Variance for Equal Sample Sizes\nWhen \\(r\\) is constant:\n\n\\(H_{0}^B: \\{\\beta_1 = \\beta_2 = \\dots = \\beta_b\\}\\) is tested via\n\n\\[\n\\text{reject if}\\;\\; \\frac{MS_{B}}{MS_{Error}} &gt; F_{b-1,\\,n-a-b+1,\\,\\alpha}.\n\\]\n\n\\(H_{0}^A: \\{\\alpha_1 = \\alpha_2 = \\dots = \\alpha_a\\}\\) analogously.\n\nExample 6.5.4 (Nail Varnish Experiment) An ANOVA table yields \\(F\\)-ratios for factor \\(A\\) (solvent) and factor \\(B\\) (varnish). Solvent is significant while varnish is not."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#calculating-sample-sizes",
    "href": "lectures/week-05_factorial-anova.html#calculating-sample-sizes",
    "title": "Factorial ANOVA",
    "section": "Calculating Sample Sizes",
    "text": "Calculating Sample Sizes\nBoth the power-based approach (Sections 3.6) and confidence-interval-width approach (Sections 4.5) can be extended to two-way designs (Montgomery 2020). For main-effect differences of size \\(A\\) or \\(B\\), formulas such as\n\\[\nr = \\frac{2a\\,\\sigma^2\\,\\phi^2}{b\\,A^2}, \\quad\nr = \\frac{2b\\,\\sigma^2\\,\\phi^2}{a\\,B^2}\n\\]\nmay be used to assure adequate power, where \\(\\phi\\) is a function of the noncentral \\(F\\)-distribution (see also Christensen 2018)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#one-observation-per-cell",
    "href": "lectures/week-05_factorial-anova.html#one-observation-per-cell",
    "title": "Factorial ANOVA",
    "section": "One Observation Per Cell",
    "text": "One Observation Per Cell\nWhen \\(r=1\\) per cell, if an interaction is possible, we cannot estimate \\(\\sigma^2\\) directly (because \\(\\text{df for error} = ab(r-1) = 0\\)). The study design must:\n\nAssume interaction is negligible (use main-effects model), or\nHave \\(\\sigma^2\\) known in advance, or\nModel only a reduced set of contrasts (sparsity of effects)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#analysis-based-on-orthogonal-contrasts",
    "href": "lectures/week-05_factorial-anova.html#analysis-based-on-orthogonal-contrasts",
    "title": "Factorial ANOVA",
    "section": "Analysis Based on Orthogonal Contrasts",
    "text": "Analysis Based on Orthogonal Contrasts\nTwo contrasts \\(c_i\\tau_i\\) and \\(k_i\\tau_i\\) are orthogonal if\n\\[\n\\sum_i \\frac{c_i\\,k_i}{r_i} = 0.\n\\]\nA complete set of \\(v-1\\) orthogonal contrasts partitions the treatment sum of squares:\n\\[\nSS_{Total} = \\sum_{q=1}^{v-1} SS_{c_q}.\n\\]\nExample 6.7.1 (Battery Experiment, continued) Duty, brand, and interaction contrasts form a complete set of \\(3\\) orthogonal contrasts for \\(4\\) total treatments, and their sums of squares add to the total."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#tukeys-test-for-additivity",
    "href": "lectures/week-05_factorial-anova.html#tukeys-test-for-additivity",
    "title": "Factorial ANOVA",
    "section": "Tukey’s Test for Additivity",
    "text": "Tukey’s Test for Additivity\nWhen the interaction is thought to be of the form \\((\\alpha\\beta)_{ij} = \\gamma\\,\\alpha_i\\,\\beta_j\\), Tukey’s test uses only 1 df. The decision rule is\n\\[\n\\text{reject } H_0^\\gamma \\text{ if }\n\\frac{ss_{AB}^*}{SS_{Error}/e} &gt; F_{1,e,\\alpha},\n\\]\nwhere \\(ss_{AB}^*\\) is a computed sum of squares specifically for that parametric form of interaction."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#a-real-experimentair-velocity-experiment",
    "href": "lectures/week-05_factorial-anova.html#a-real-experimentair-velocity-experiment",
    "title": "Factorial ANOVA",
    "section": "A Real Experiment—Air Velocity Experiment",
    "text": "A Real Experiment—Air Velocity Experiment\nTable 6.11 shows data on air velocity for three rib heights (\\(A\\)) and six Reynolds numbers (\\(B\\)) with \\(r=1\\) per cell. Investigators suspected some interaction but believed certain higher-order trends would be negligible. They pooled those negligible interaction contrasts for an error estimate with 3 df, then tested the remaining contrasts individually. See Table 6.12 for the resulting ANOVA."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#model-checking-and-residual-diagnostics",
    "href": "lectures/week-05_factorial-anova.html#model-checking-and-residual-diagnostics",
    "title": "Factorial ANOVA",
    "section": "Model Checking and Residual Diagnostics",
    "text": "Model Checking and Residual Diagnostics\nBefore finalizing an ANOVA conclusion, we must check:\n\nIndependence: Plot residuals vs. observation order.\nConstant Variance: Plot residuals (or standardized residuals) vs. fitted values, factor levels, or treatment combos.\nNormality: Use a Normal Q-Q plot of residuals.\n\nThe standardized residual often used is:\n\\[\nz_{ijt} = \\frac{y_{ijt} - \\hat{y}_{ijt}}{\\sqrt{\\tfrac{SS_{Error}}{n - 1}}}.\n\\]\nDepending on whether we assume a complete or main-effects model, \\(\\hat{y}_{ijt}\\) includes:\n\n\\(\\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j + \\widehat{(\\alpha\\beta)}_{ij}\\) (complete model), or\n\\(\\hat{\\tau}_{ij} = \\hat{\\alpha}_i + \\hat{\\beta}_j\\) (main-effects model)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#fitting-the-two-way-complete-model",
    "href": "lectures/week-05_factorial-anova.html#fitting-the-two-way-complete-model",
    "title": "Factorial ANOVA",
    "section": "Fitting the Two-Way Complete Model",
    "text": "Fitting the Two-Way Complete Model\nA typical R formula for the two-way complete model is:\nmodelAB = aov(y ~ fA + fB + fA:fB, data = react_df)\nor equivalently:\nmodelAB = aov(y ~ fA * fB, data = react_df)\nHere, fA and fB are factor variables in R. The operator * adds both main effects and the interaction term. We often check:\nanova(modelAB)        # \"Type I\" sums of squares\ndrop1(modelAB, ~., test = \"F\")  # \"Type III\" sums of squares"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#type-i-vs.-type-iii-sums-of-squares",
    "href": "lectures/week-05_factorial-anova.html#type-i-vs.-type-iii-sums-of-squares",
    "title": "Factorial ANOVA",
    "section": "Type I vs. Type III Sums of Squares",
    "text": "Type I vs. Type III Sums of Squares\n\nType I (Sequential) sums of squares: Variation explained by adding each term in sequence.\nType III sums of squares: Variation explained by each term after adjusting for all other terms in the model.\n\nRequires options(contrasts = c(\"contr.sum\", \"contr.poly\")) for balanced constraints."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#fitting-the-cell-means-model",
    "href": "lectures/week-05_factorial-anova.html#fitting-the-cell-means-model",
    "title": "Factorial ANOVA",
    "section": "Fitting the Cell-Means Model",
    "text": "Fitting the Cell-Means Model\nAn alternative is the cell-means model, which estimates each τij_{ij} directly:\nmodelTC = aov(y ~ fTC, data = react_df)\nwhere fTC is a factor that indicates each treatment combination (i,j)(i,j). This approach is ideal for more direct comparisons of the individual cell means."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#example-reaction-time-experiment",
    "href": "lectures/week-05_factorial-anova.html#example-reaction-time-experiment",
    "title": "Factorial ANOVA",
    "section": "Example: Reaction Time Experiment",
    "text": "Example: Reaction Time Experiment\nIn the pasted notes, an experiment with Factor A = “Cue Stimulus” (a=2a=2 levels), Factor B = “Cue Time” (b=3b=3 levels), and unequal replicates is demonstrated. Output from anova(modelAB), drop1(modelAB, ~., test=\"F\"), and anova(modelTC):\n\nInteraction not significant,\nFactor A is significant,\nFactor B is not significant."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#main-effect-contrasts",
    "href": "lectures/week-05_factorial-anova.html#main-effect-contrasts",
    "title": "Factorial ANOVA",
    "section": "Main-Effect Contrasts",
    "text": "Main-Effect Contrasts\nlsmB = lsmeans(modelAB, ~ fB)\nsummary(contrast(lsmB, list(B12 = c(1, -1, 0))), infer = c(TRUE, TRUE))\nThis obtains the contrast β1−β2_1 - _2 for Factor B. We can similarly construct more complex contrasts or even interaction contrasts:\nlsmAB = lsmeans(modelAB, ~ fB:fA)\nsummary(contrast(lsmAB, list(AB = c(1, 0, -1, -1, 0, 1))), infer=c(TRUE,TRUE))"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#multiple-comparison-procedures",
    "href": "lectures/week-05_factorial-anova.html#multiple-comparison-procedures",
    "title": "Factorial ANOVA",
    "section": "Multiple Comparison Procedures",
    "text": "Multiple Comparison Procedures\nSeveral methods adjust for familywise error:\n\nTukey’s HSD\nBonferroni\nScheffé\nDunnett (comparing all treatments to a control)\n\n\nTukey Example\nsummary(\n  contrast(lsmB, method=\"pairwise\", adjust=\"tukey\"),\n  infer = c(TRUE, TRUE),\n  level = 0.99\n)\nGenerates pairwise comparisons among the levels of B with Tukey’s adjustment at 99% confidence."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#dunnett-example",
    "href": "lectures/week-05_factorial-anova.html#dunnett-example",
    "title": "Factorial ANOVA",
    "section": "Dunnett Example",
    "text": "Dunnett Example\nsummary(\n  contrast(lsmB, method=\"trt.vs.ctrl\", adj=\"mvt\", ref=1),\n  infer = c(TRUE, TRUE),\n  level = 0.99\n)\nCompares each level of B to the control (the first factor level by default)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#handling-unequal-sample-sizes",
    "href": "lectures/week-05_factorial-anova.html#handling-unequal-sample-sizes",
    "title": "Factorial ANOVA",
    "section": "Handling Unequal Sample Sizes",
    "text": "Handling Unequal Sample Sizes\nWhen the sample sizes rijr_{ij} differ, Type I and Type III sums of squares can produce different inferences. Type III is standard for “unbalanced” data when we interpret each effect adjusted for the others. The same code is used:\n\naov(y ~ fA + fB + fA:fB, data=…)\ndrop1(model, ~., test=\"F\")\n\nbut we must ensure contrasts are set to impose consistent constraints."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#one-observation-per-cell-small-experiments",
    "href": "lectures/week-05_factorial-anova.html#one-observation-per-cell-small-experiments",
    "title": "Factorial ANOVA",
    "section": "One Observation per Cell (Small Experiments)",
    "text": "One Observation per Cell (Small Experiments)\nWhen r=1r=1 per cell, there is no direct error estimate under the full two-way complete model—the error df is zero. One can:\n\nAssume interaction is negligible (use main-effects model only),\nKnow \\(\\sigma^2\\) a priori,\nUse orthogonal contrasts to identify “negligible” components of the interaction, thereby pooling them to form an error estimate."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#orthogonal-contrasts",
    "href": "lectures/week-05_factorial-anova.html#orthogonal-contrasts",
    "title": "Factorial ANOVA",
    "section": "Orthogonal Contrasts",
    "text": "Orthogonal Contrasts\nTwo contrasts are orthogonal if their corresponding sums of squares are uncorrelated. Equivalently, for equal rij=rr_{ij}=r, the sum of products of coefficients must be zero."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#model-building-and-type-i-ss",
    "href": "lectures/week-05_factorial-anova.html#model-building-and-type-i-ss",
    "title": "Factorial ANOVA",
    "section": "Model Building and Type I SS",
    "text": "Model Building and Type I SS\nFor building or refining a model, Type I SS can be used to see how adding each effect in sequence explains residual variation."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#r-code-examples-for-plotting",
    "href": "lectures/week-05_factorial-anova.html#r-code-examples-for-plotting",
    "title": "Factorial ANOVA",
    "section": "R Code Examples for Plotting",
    "text": "R Code Examples for Plotting\n\nResidual plots:\n\nres &lt;- residuals(modelAB)\nfits &lt;- fitted(modelAB)\nplot(res ~ fits)\nqqnorm(res); qqline(res)\n\nInteraction plots:\n\ninteraction.plot(x.factor = react.data$fA, \n                 trace.factor = react.data$fB,\n                 response = react.data$y, \n                 type=\"b\", \n                 xlab=\"A\", \n                 trace.label=\"B\", \n                 ylab=\"Mean of y\")"
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#conclusion",
    "href": "lectures/week-05_factorial-anova.html#conclusion",
    "title": "Factorial ANOVA",
    "section": "Conclusion",
    "text": "Conclusion\nIn these lecture notes, we have explored:\n\nTwo-way ANOVA models (complete, main-effects) and their key assumptions.\nModel checking via residual diagnostics to confirm normality, equal variances, and independence.\nContrasts and multiple comparisons in R using lsmeans (or emmeans), including Type I vs. Type III sums of squares for unbalanced designs.\nHandling special cases, such as one observation per cell, by leveraging orthogonal contrasts or known variance assumptions.\n\nThese methods allow us to make rigorous inferences about factor main effects and interactions in a two-factor experimental setup. In practice, thorough model checking (especially when cell counts are unequal) ensures valid scientific conclusions and properly calibrated multiple comparisons."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#assignments",
    "href": "lectures/week-05_factorial-anova.html#assignments",
    "title": "Factorial ANOVA",
    "section": "Assignments",
    "text": "Assignments\n\nResidual Diagnostics\n\nSimulate a two-way ANOVA dataset in R, fit the complete model, and produce diagnostic plots for normality, equal variances, and independence.\n\nInteraction Plot Interpretation\n\nProvide a small dataset with potential interaction. Plot the interaction and propose how you would test the no-interaction hypothesis.\n\nOrthogonal Contrasts\n\nFor a \\(2\\times 3\\) experiment, list a complete set of orthogonal contrasts for the interaction. Verify their orthogonality.\n\nPower Calculation\n\nAssume a \\(2\\times 2\\) design with equal sample size \\(r\\) per cell and desired power 0.80 to detect a main-effect difference of 2 (with \\(\\sigma^2\\) estimated at 3). Use the power-based formula to find \\(r\\)."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#exams",
    "href": "lectures/week-05_factorial-anova.html#exams",
    "title": "Factorial ANOVA",
    "section": "Exams",
    "text": "Exams\nConsider an exam scenario with both conceptual and computational questions:\n\nConceptual\n\nDefine the assumptions of the two-way complete model. How do you test them?\nExplain why we typically test interaction first before examining main effects.\n\nComputational\n\nGiven raw data for a \\(3 \\times 4\\) design, compute and fill in an ANOVA table. Perform an \\(F\\)-test for interaction at \\(\\alpha=0.05\\).\n\nShort Essay\n\nCompare and contrast the two-way complete model and the two-way main-effects model. Include real-world examples."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#visual-aids",
    "href": "lectures/week-05_factorial-anova.html#visual-aids",
    "title": "Factorial ANOVA",
    "section": "Visual Aids",
    "text": "Visual Aids\n\nFigure: Interaction Plots for typical data sets showing:\n\nParallel lines (no interaction).\nClearly crossing lines (significant interaction).\n\nFigure: Residual Plots illustrating typical patterns (equal vs. unequal variances, outliers).\nTable Templates for summarizing sample means, sum of squares, and mean squares."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#appendix-advanced-proofs-and-derivations",
    "href": "lectures/week-05_factorial-anova.html#appendix-advanced-proofs-and-derivations",
    "title": "Factorial ANOVA",
    "section": "Appendix: Advanced Proofs and Derivations",
    "text": "Appendix: Advanced Proofs and Derivations\n\nProof of Orthogonality Condition\n\nShow that two contrasts are uncorrelated \\(\\iff \\sum (d_{ij} , h_{ij} / r_{ij}) = 0\\).\n\nDerivation of LSE under Main-Effects Model (Unequal \\(r\\))\n\nSolve the normal equations for \\(\\hat{\\alpha}_i, \\hat{\\beta}_j\\) when \\(r_{ij}\\) differ."
  },
  {
    "objectID": "lectures/week-05_factorial-anova.html#references",
    "href": "lectures/week-05_factorial-anova.html#references",
    "title": "Factorial ANOVA",
    "section": "References",
    "text": "References\n\nDean, A., Voss, D., & Draguljić, D. (2017). Design and Analysis of Experiments. Springer International Publishing. https://doi.org/10.1007/978-3-319-52250-0\nChristensen, R. (2018). Analysis of Variance, Design, and Regression: Linear Modeling for Unbalanced Data (Second Edition). Chapman and Hall/CRC, imprint of Taylor & Francis.\nMontgomery, D. C. (2020). Design and Analysis of Experiments (10th ed.). Wiley."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html",
    "href": "lectures/week-01_intro-design_part1.html",
    "title": "Fundamentals of Experimental Design",
    "section": "",
    "text": "In experimental design, the goal is to identify how various factors or treatments influence a response variable. Unlike observational studies, well-planned experiments leverage randomization, blocking, and replication to control and isolate sources of variation, thereby allowing robust causal conclusions. By thoughtfully designing experiments, researchers can more efficiently use resources, reduce systematic biases, and interpret data using classical statistical frameworks.\nThe following lecture notes will cover the fundamental principles of experimental design in the context of mathematical statistics. We will discuss the logic and practice of key techniques—randomization, blocking, and replication—and explore their role in ensuring valid and efficient inference. We will also highlight the interplay between design and analysis, referencing classical texts in design of experiments and illustrating concepts through examples and R code."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#introduction",
    "href": "lectures/week-01_intro-design_part1.html#introduction",
    "title": "Fundamentals of Experimental Design",
    "section": "",
    "text": "In experimental design, the goal is to identify how various factors or treatments influence a response variable. Unlike observational studies, well-planned experiments leverage randomization, blocking, and replication to control and isolate sources of variation, thereby allowing robust causal conclusions. By thoughtfully designing experiments, researchers can more efficiently use resources, reduce systematic biases, and interpret data using classical statistical frameworks.\nThe following lecture notes will cover the fundamental principles of experimental design in the context of mathematical statistics. We will discuss the logic and practice of key techniques—randomization, blocking, and replication—and explore their role in ensuring valid and efficient inference. We will also highlight the interplay between design and analysis, referencing classical texts in design of experiments and illustrating concepts through examples and R code."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#objectives",
    "href": "lectures/week-01_intro-design_part1.html#objectives",
    "title": "Fundamentals of Experimental Design",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the theoretical foundations and objectives of experimental design in statistics.\nDistinguish between observational and experimental studies, emphasizing the advantages experiments provide.\nLearn the fundamental techniques of replication, blocking, and randomization.\nDiscuss how factorial experiments efficiently explore multiple factors simultaneously.\nUnderstand how design decisions affect data analysis and the validity of inferential procedures.\nReinforce concepts with R-based numerical examples and demonstrate their application to real or simulated data.\nExplore advanced mathematical derivations and proofs in the Appendix.\n\n\nReadings\n\nDean, Voss, and Draguljić (2017, Ch. 1 and 2)"
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#experimentation-vs.-observation",
    "href": "lectures/week-01_intro-design_part1.html#experimentation-vs.-observation",
    "title": "Fundamentals of Experimental Design",
    "section": "Experimentation vs. Observation",
    "text": "Experimentation vs. Observation\n\nObservational studies record data without intervention, often leaving confounding factors unaccounted for. Causal interpretations are limited.\nControlled experiments, on the other hand, manipulate factor levels and apply treatments to subjects (experimental units). By controlling the allocation of treatments, experiments more reliably identify causes of variation and support causal conclusions."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#goals-of-experimentation",
    "href": "lectures/week-01_intro-design_part1.html#goals-of-experimentation",
    "title": "Fundamentals of Experimental Design",
    "section": "Goals of Experimentation",
    "text": "Goals of Experimentation\n\nDetermine causes of variation in response.\nFind optimal conditions and treatment combinations.\nCompare responses across different treatments.\nDevelop predictive models that accurately reflect the underlying process.\n\nEffective experimental design ensures that maximum information is gained while efficiently using resources. These objectives directly influence how we choose the number of observations, treatment combinations, and the structure of the experimental layout."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#importance-of-experimental-design",
    "href": "lectures/week-01_intro-design_part1.html#importance-of-experimental-design",
    "title": "Fundamentals of Experimental Design",
    "section": "Importance of Experimental Design",
    "text": "Importance of Experimental Design\n\nCause-and-Effect: Only experiments can directly assess causal relationships, unlike observational studies prone to confounding.\nResource Allocation: Deciding how many observations to collect and how to arrange them ensures the best return on time, effort, and material.\nEfficient Analysis: A well-designed experiment simplifies subsequent data analysis and makes model assumptions (e.g., distributional forms, independence) more credible.\n\n(See Montgomery, 2019, Design and Analysis of Experiments for a comprehensive treatment.)"
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#replication",
    "href": "lectures/week-01_intro-design_part1.html#replication",
    "title": "Fundamentals of Experimental Design",
    "section": "Replication",
    "text": "Replication\n\nDefinition: Replication means repeating the entire set of treatments independently on different experimental units.\nPurpose: Replication estimates the experimental error, allowing generalization of conclusions to a broader population of similar units.\nContrast with Repeated Measurements: Repeated measurements on the same unit (e.g., measuring the same subject multiple times) do not constitute independent replications. True replication involves distinct subjects or units.\n\nWithout adequate replication, it is difficult to separate true treatment effects from random noise."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#blocking",
    "href": "lectures/week-01_intro-design_part1.html#blocking",
    "title": "Fundamentals of Experimental Design",
    "section": "Blocking",
    "text": "Blocking\n\nConcept: Blocking is used to control extraneous variation by grouping experimental units into homogeneous subsets called blocks. Treatments are then compared within these blocks.\nExample: If environmental conditions vary across different times of day, you may form blocks of time periods and randomly assign treatments within each block. This reduces noise and increases the precision of comparisons among treatments.\nOutcome: Blocking improves the precision of inference by removing a known source of variability, making it easier to detect treatment differences."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#randomization",
    "href": "lectures/week-01_intro-design_part1.html#randomization",
    "title": "Fundamentals of Experimental Design",
    "section": "Randomization",
    "text": "Randomization\n\nRationale: Random assignment of treatments to experimental units is fundamental. It ensures that no systematic bias influences treatment allocation.\nBenefits:\n\n  - Prevents experimenter preferences or unconscious patterns from influencing results.\n  - Justifies the use of standard statistical distributions (F, t) for inference.\n  - Confirms that observed effects can be attributed to treatments and not assignment bias.\nReference: Kempthorne (1977) explains how randomization ensures the validity of distributional assumptions underpinning common statistical tests.\n\nMethods of Randomization\n\nRandom Number Generators (RNG): Computer programs or calculators produce pseudo-random digits, used to assign treatments.\nRandom Number Tables: Historically used, a random table combined with random starting points can ensure unbiased allocations.\nEnsuring True Randomness: Use objective random devices (dice, RNGs) and avoid human-chosen “random” sequences, as humans are prone to inadvertent patterns."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#example-random-assignment-and-anova",
    "href": "lectures/week-01_intro-design_part1.html#example-random-assignment-and-anova",
    "title": "Fundamentals of Experimental Design",
    "section": "Example: Random Assignment and ANOVA",
    "text": "Example: Random Assignment and ANOVA\nSuppose we have three treatments (A, B, C) and want to randomly assign them to 9 experimental units.\n\n# Set seed for reproducibility\n\nset.seed(123)\n# Treatments\n\ntreatments &lt;- c(\"A\", \"B\", \"C\")\n# We have 9 units, randomly assign treatments\n\nassignments &lt;- sample(rep(treatments, each=3))\nassignments\n\n[1] \"A\" \"B\" \"C\" \"A\" \"C\" \"B\" \"C\" \"A\" \"B\"\n\n\nThis code generates a random permutation of A, B, and C assigned to the units.\nNow assume we collected data (simulated responses):\n\n# Simulate responses with a true difference: A &lt;-  B &lt;-  C\ntrue_means &lt;- c(A=10, B=12, C=15)\nresponses &lt;- rnorm(9, mean=rep(true_means, each=3), sd=2)\ndata &lt;- data.frame(unit = 1:9,treatment = assignments, response = responses)\n\n# Fit an ANOVA model\nfit &lt;- aov(response ~ treatment, data=data)\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ntreatment    2  32.01   16.00   1.287  0.343\nResiduals    6  74.61   12.44               \n\n\nThe ANOVA table will show how between-treatment variability compares to within-treatment variability. With a large enough difference and adequate replication, we expect a significant treatment effect.\nConclusion\nExperimental design is a cornerstone of scientific inquiry. By employing replication, blocking, and randomization, statisticians and researchers can draw clearer causal conclusions, improve the efficiency of experimentation, and ensure the validity of statistical tests.\nKey Takeaways:\n\nRandomization prevents systematic bias and validates inferential methods.\nReplication provides robust estimates of variability and supports generalizability.\nBlocking controls known sources of variation, improving precision.\nFactorial designs offer a more comprehensive view of multiple factors simultaneously.\n\nFuture sessions may delve deeper into specific complex designs (e.g., Latin squares, split-plots) and advanced analysis methods, ensuring students appreciate the rich interplay between design and analysis."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#references",
    "href": "lectures/week-01_intro-design_part1.html#references",
    "title": "Fundamentals of Experimental Design",
    "section": "References",
    "text": "References\n\nMontgomery, D. C. (2019). Design and Analysis of Experiments. Wiley.\nKempthorne, O. (1977). Design and Analysis of Experiments. Robert E. Krieger Publishing Company.\nScheffé, H. (1959). The Analysis of Variance. Wiley."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#a-proof-of-randomizations-role-in-distributional-assumptions",
    "href": "lectures/week-01_intro-design_part1.html#a-proof-of-randomizations-role-in-distributional-assumptions",
    "title": "Fundamentals of Experimental Design",
    "section": "A Proof of Randomization’s Role in Distributional Assumptions",
    "text": "A Proof of Randomization’s Role in Distributional Assumptions\nUnder a completely randomized design, every unit is equally likely to receive any treatment. Let \\(Y_{ij}\\) represent the response from the \\(j\\)-th replicate of treatment \\(i\\). Assuming \\(Y_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij}\\) with \\(\\varepsilon_{ij} \\sim N(0,\\sigma^2)\\):\n\nRandomization ensures that \\((\\varepsilon_{ij})\\) are exchangeable random variables.\nUnder \\(H_0: \\alpha_i=0\\), the test statistic (e.g., ANOVA F-statistic) follows an F-distribution derived from the ratio of quadratic forms in normal variables (Scheffé, 1959).\nWithout randomization, the assumptions that justify the standard F and t distributions could be invalid, making inferences incorrect.\n\nFor more technical details, see Kempthorne (1977, Ch. 2)."
  },
  {
    "objectID": "lectures/week-01_intro-design_part1.html#a-derivation-of-expected-mean-squares",
    "href": "lectures/week-01_intro-design_part1.html#a-derivation-of-expected-mean-squares",
    "title": "Fundamentals of Experimental Design",
    "section": "A Derivation of Expected Mean Squares",
    "text": "A Derivation of Expected Mean Squares\nFor the simple one-way ANOVA model:\n\\[\nY_{ij} = \\mu + \\alpha_i + \\varepsilon_{ij},\n\\]\nwith \\(\\sum \\alpha_i = 0\\), the expected mean squares are:\n\\[\nE(MS_{\\text{Treatment}}) = \\sigma^2 + n \\sum \\alpha_i^2/(g-1),\n\\]\n\\[\nE(MS_{\\text{Error}}) = \\sigma^2.\n\\]\nThese derivations rely on properties of orthogonal decompositions of sums of squares and can be found in Montgomery (2019) and Scheffé (1959)."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html",
    "href": "lectures/week-01_intro-design_part2.html",
    "title": "Planning Experiments",
    "section": "",
    "text": "Experimental design forms the foundation of scientific inquiry whenever the goal is to understand how certain controllable inputs (factors) influence outcomes (responses). By carefully planning how to collect data and assigning treatments to experimental units, we can draw valid causal inferences about these relationships. Unlike observational studies, experiments involve actively manipulating factors to reveal their effects while controlling for unwanted variability.\nThis set of lecture notes provides a comprehensive overview of experimental design concepts and applications. We will address how to define clear objectives, classify sources of variation, choose appropriate experimental designs, apply randomization, incorporate blocking, specify statistical models, and determine appropriate sample sizes. Real-world examples and R code snippets will illustrate key ideas. Additionally, we will integrate exercises, class activities, summaries, challenges, and visual aids to reinforce understanding."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#steps-in-planning-an-experiment",
    "href": "lectures/week-01_intro-design_part2.html#steps-in-planning-an-experiment",
    "title": "Planning Experiments",
    "section": "Steps in Planning an Experiment",
    "text": "Steps in Planning an Experiment\nIt is very tempting to jump into data collection without a clear plan. However, a well-thought-out experimental design is crucial for obtaining valid and interpretable results. Here is a checklist of steps to guide you through the planning process:\n\n1. Define the Objectives of the Experiment\n\nClearly state the goals (e.g., comparing treatments, optimizing factors, studying interactions).\nWrite objectives as specific questions or hypotheses to guide the design.\n\n\n\n2. Identify All Sources of Variation\n\nTreatment Factors and Their Levels\n\nFactors actively manipulated (e.g., temperature, dosage) and their specific levels.\nExample: Dosage levels: 10 mg, 20 mg, 30 mg.\n\nExperimental Units\n\nSmallest division of material receiving a treatment independently.\nExample: Individual plants in a field trial.\n\nBlocking Factors, Noise Factors, and Covariates\n\nBlocking: Group similar units (e.g., day of testing).\nNoise: Uncontrollable variability (e.g., weather conditions).\nCovariates: Measureable properties (e.g., baseline performance).\n\n\n\n\n\n3. Choose a Rule for Assigning Experimental Units to Treatments\n\nSelect randomization techniques:\n\nCompletely Randomized Design (CRD).\nRandomized Block Design (RBD).\nSplit-Plot Design.\n\n\n\n\n4. Specify the Measurements, Procedure, and Anticipated Difficulties\n\nMeasurements\n\nSpecify precision, units, and frequency of measurement.\nExample: Growth in cm, tensile strength in MPa.\n\nProcedure\n\nStep-by-step instructions for consistent data collection.\n\nAnticipated Difficulties\n\nIdentify challenges (e.g., equipment failure) and mitigation strategies.\n\n\n\n\n\n5. Run a Pilot Experiment\n\nTest feasibility, validate instruments, and refine factor levels or treatment combinations.\n\n\n\n6. Specify the Statistical Model\n\nExample for one-way ANOVA: \\[\nY_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n\\]\n\nwhere:\n\n\\(Y_{ij}\\): Observed response.\n\\(\\mu\\): Overall mean.\n\\(\\tau_i\\): Treatment effect.\n\\(\\varepsilon_{ij}\\): Random error term.\n\n\n\n\n7. Outline the Analysis\n\nPlan descriptive and inferential methods (e.g., ANOVA, regression).\nInclude diagnostic checks for model assumptions.\n\n\n\n8. Calculate Sample Size\n\nPerform power analysis based on variability estimates and desired precision.\n\n\n\n9. Review and Revise\n\nRevisit and refine all decisions based on pilot results and practical constraints."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#flowchart-of-experimental-design-steps",
    "href": "lectures/week-01_intro-design_part2.html#flowchart-of-experimental-design-steps",
    "title": "Planning Experiments",
    "section": "Flowchart of Experimental Design Steps",
    "text": "Flowchart of Experimental Design Steps\n\n\n\n\n\n\n\n\nExperimentalDesign\n\n\ncluster_planning\n\nPlanning\n\n\ncluster_implementation\n\nImplementation\n\n\ncluster_analysis\n\nAnalysis\n\n\ncluster_iteration\n\nIteration\n\n\n\nDefine Objectives\n\nDefine Objectives\n\n\n\nIdentify Sources of Variation\n\nIdentify Sources of Variation\n\n\n\nDefine Objectives-&gt;Identify Sources of Variation\n\n\n\n\n\nChoose Assignment Rules\n\nChoose Assignment Rules\n\n\n\nIdentify Sources of Variation-&gt;Choose Assignment Rules\n\n\n\n\n\nSpecify Measurements & Procedure\n\nSpecify Measurements & Procedure\n\n\n\nChoose Assignment Rules-&gt;Specify Measurements & Procedure\n\n\n\n\n\nIdentify Anticipated Difficulties\n\nIdentify Anticipated Difficulties\n\n\n\nSpecify Measurements & Procedure-&gt;Identify Anticipated Difficulties\n\n\n\n\n\nRun Pilot Experiment\n\nRun Pilot Experiment\n\n\n\nIdentify Anticipated Difficulties-&gt;Run Pilot Experiment\n\n\n\n\n\nSpecify Statistical Model\n\nSpecify Statistical Model\n\n\n\nRun Pilot Experiment-&gt;Specify Statistical Model\n\n\n\n\n\nOutline Analysis\n\nOutline Analysis\n\n\n\nSpecify Statistical Model-&gt;Outline Analysis\n\n\n\n\n\nCalculate Sample Size\n\nCalculate Sample Size\n\n\n\nOutline Analysis-&gt;Calculate Sample Size\n\n\n\n\n\nReview and Revise\n\nReview and Revise\n\n\n\nCalculate Sample Size-&gt;Review and Revise\n\n\n\n\n\nReview and Revise-&gt;Define Objectives\n\n\n\n\n\nReview and Revise-&gt;Identify Sources of Variation\n\n\n\n\n\n\n\n\nFigure 1: Experimental Design Planning Flowchart"
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#identifying-experiment-objectives-and-variation-sources",
    "href": "lectures/week-01_intro-design_part2.html#identifying-experiment-objectives-and-variation-sources",
    "title": "Planning Experiments",
    "section": "Identifying Experiment Objectives and Variation Sources",
    "text": "Identifying Experiment Objectives and Variation Sources\n\nSetting Objectives\nEvery experiment must start with a clear research objective. Ask: What do we hope to learn or achieve? Objectives may include:\n\nDetermining which manufacturing process leads to the highest product quality.\nComparing new teaching methods for improved student performance.\nFinding the optimal settings of machine parameters to reduce defects.\n\nWithout well-defined objectives, the experiment risks being unfocused, making the results difficult to interpret.\n\n\nVariation Sources\nVariation in responses can arise from multiple sources:\n\nMajor Variation (Treatment Effects): Due to the factors of interest (e.g., different fertilizers or drug dosages).\nNuisance Factors (noise): Variables that influence the response but are not of primary interest (e.g., room temperature, operator skill). Some nuisance factors can be controlled or incorporated as blocking factors, while others must be accepted as noise.\n\nBy identifying and classifying sources of variation, we can decide which factors to manipulate as treatments, which to fix or control, and which to block or treat as covariates.\n\n\nExample\nConsider an agricultural experiment testing different fertilizer types (A, B, C). The main factors are the fertilizer types. Soil heterogeneity could be a nuisance factor. If the field has varying soil quality, it might mask the fertilizer effect. Recognizing this early allows us to incorporate blocking or other strategies to handle this nuisance variation.\n\n\nActivity (Work Out Experiment): Think-Pair-Share\n\nThink: Imagine you are testing two workout plans (high-intensity vs. moderate-intensity) to improve muscle strength.\nPair: Discuss with a partner what the main objective is and what factors could cause unwanted variation (e.g., baseline fitness level, diet).\nShare: Volunteer pairs share their classification of factors and whether they could be controlled or blocked.\n\n\n\nChallenges and Common Mistakes\n\nStarting without a clear objective can lead to inconclusive results.\nConfusing major variation with nuisance variation can misdirect the design.\n\n\n\nExercises\n\nWhy must objectives be clear before starting an experiment?\nGiven three fertilizer types and two soil conditions, identify which factors should be considered treatments and which might be nuisance factors.\nExplain how failing to control a nuisance factor increases the variance of treatment effect estimates.\n\nKey Takeaways: - Well-defined objectives shape the experiment. - Classifying factors into treatment and nuisance categories enhances design efficiency."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#treatment-factors-and-levels",
    "href": "lectures/week-01_intro-design_part2.html#treatment-factors-and-levels",
    "title": "Planning Experiments",
    "section": "Treatment Factors and Levels",
    "text": "Treatment Factors and Levels\n\nDefining Treatment Factors\nA treatment factor is any variable deliberately manipulated by the experimenter to study its effect on the response.\nExample: In a detergent study, water temperature and detergent concentration are treatment factors.\n\n\nFactor Levels\nFactor levels are the specific settings or categories of a factor. For instance, twist levels in a cotton-spinning experiment might be 1.63, 1.69, 1.78, and 1.90 turns per inch. Chosen based on subject matter knowledge, factor levels must be realistic and relevant.\n\n\nActivity (Baking Experiment): Design Your Own Factorial Experiment\nIndividually pick a factor (e.g., cooking time) and propose three levels. Pair up with another student who has chosen a different factor (e.g., oven temperature), and combine them into a factorial design. Discuss the complexity and the number of total treatment combinations.\n\n\nChallenges and Common Mistakes\n\nChoosing impractical or irrelevant factor levels can undermine the experiment’s usefulness.\nToo many factor levels complicate analysis without necessarily adding insight.\n\n\n\nExercises\n\nWhat criteria guide the selection of factor levels?\nFor a factor with 4 levels and another with 3 levels, determine how many treatment combinations exist.\nExplain how replicates per factor level (with total resources fixed) decreases the variance of treatment effect estimates.\n\nKey Takeaways: - Treatments factors are deliberately manipulated variables. - Factor levels must be meaningful, informed by real-world considerations or pilot data."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#experimental-units-blocking-and-nuisance-factors",
    "href": "lectures/week-01_intro-design_part2.html#experimental-units-blocking-and-nuisance-factors",
    "title": "Planning Experiments",
    "section": "Experimental Units, Blocking, and Nuisance Factors",
    "text": "Experimental Units, Blocking, and Nuisance Factors\n\nExperimental Units\nThe experimental unit is the smallest entity to which treatments are independently assigned. This concept is crucial for valid statistical inference.\n\n\nExamples of Experimental Units\n\nPlants in a field trial if each plant gets a different fertilizer.\nEngine test benches if each engine is run under a unique setup.\nStudents in a classroom if each student receives a different teaching method.\nPatients in a clinical trial if each patient gets a different drug.\n\n\n\nBlocking Factors\nWhen a known nuisance factor could confound results, use blocking. Blocking forms groups of similar experimental units, each block receiving all treatments. This controls for block-to-block variation, increasing precision.\nEquation (Block Design):\n\\[\nY_{ij} = \\mu + \\tau_i + \\beta_j + \\varepsilon_{ij}\n\\] {#eq:block-design}\nHere, \\(\\tau_i\\) is the treatment effect and \\(\\beta_j\\) the block effect.\n\n\nNoise Factors and Covariates\n\nNoise Factors: Uncontrollable variations like daily temperature fluctuations.\nCovariates: Measured continuous variables (e.g., initial weight) that help explain variation when included in the model.\n\n\n\nFactor Classification Flowchart\n\n\n\n\n\n\nflowchart TD\n    A[All Factors]\n    B[Interested in studying the effect?]\n    C[Treatment Factors]\n    D[Nuisance Factors]\n    E[Can we group by factor levels?]\n    F[Block Factor]\n    G[Noise/Uncontrolled Factor]\n\n    A --&gt; B\n    B --&gt;|Yes| C\n    B --&gt;|No| D\n    D --&gt; E\n    E --&gt;|Yes| F\n    E --&gt;|No| G\n\n\n\n\nFigure 2: Factor Classification Flowchart\n\n\n\n\n\n\n\nActivity: Blocking Brainstorm\nConsider a baking experiment where ovens differ in temperature calibration. Could ovens form blocks? Discuss how blocking might improve detection of differences in recipes.\n\n\nChallenges and Common Mistakes\n\nConfusing the experimental unit with observational units leads to incorrect analysis.\nIgnoring a known source of variation that could be blocked may inflate residual error.\n\n\n\nExercises\n\nConceptual: Differentiate between experimental and observational units.\nNumerical: If you have 4 treatments and 6 blocks (each block gets all 4 treatments), how many total experimental units are there?\nProof/Derivation: Show that including a block factor in the model reduces the residual variance compared to an unblocked design.\n\nKey Takeaways:\n\nCorrectly identifying experimental units is essential.\nBlocking controls known nuisance factors, improving the experiment’s power to detect treatment effects."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#principles-of-randomization",
    "href": "lectures/week-01_intro-design_part2.html#principles-of-randomization",
    "title": "Planning Experiments",
    "section": "Principles of Randomization",
    "text": "Principles of Randomization\n\nRandomization Concept\nRandomization ensures that each experimental unit has an equal chance of receiving any treatment. This prevents systematic bias and justifies the assumptions underlying standard statistical tests.\nExample: Use a random number generator to assign treatments to plots, ensuring no predetermined pattern.\n\n\nHow to Perform Randomization\nRandomization ensures unbiased allocation of experimental units to treatments and reduces the risk of systematic error. Here’s how you can perform randomization in practice:\n\nProcedure\n\nList Experimental Units:\n\nCreate a list of all experimental units (e.g., subjects, plots of land).\n\nList Treatments:\n\nSpecify the treatments or factor levels to be assigned.\n\nDetermine Design Type:\n\nChoose a design type: Completely Randomized Design (CRD), Randomized Block Design (RBD), or Split-Plot Design.\n\nGenerate Random Numbers:\n\nUse random number generation to assign treatments to units.\n\nAssign Treatments:\n\nPair random numbers with treatments to allocate them to experimental units.\n\nVerify:\n\nEnsure assignments are correct and check for any unintended patterns.\n\n\n\n\n\nPseudocode for Randomization\n\nCompletely Randomized Design\nINPUT: List of experimental units, List of treatments\nOUTPUT: Randomized assignments of treatments to units\n\n1. Define N = Total number of experimental units\n2. Define T = List of treatments\n3. Repeat T to match the number of experimental units if unbalanced\n4. Shuffle the list of treatments randomly\n5. Assign shuffled treatments to the experimental units\n6. Return randomized assignments\n\n\nRandomized Block Design\nINPUT: List of blocks, List of treatments\nOUTPUT: Randomized assignments within each block\n\n1. For each block in blocks:\n    a. Shuffle the list of treatments randomly\n    b. Assign shuffled treatments to experimental units in the block\n2. Combine all block-level assignments\n3. Return randomized assignments\n\n\n\nR Implementation: Randomization Examples\n\n1. Completely Randomized Design\n\n\n\nn &lt;- 10  # Number of experimental units\n# Define experimental units and treatments\nexperimental_units &lt;- 1:n  # 10 experimental units\ntreatments &lt;- c(\"A\", \"B\", \"C\")  # 3 treatments\n\n# Repeat treatments to match the number of units\ntreatments &lt;- rep(treatments, length.out = n)\n\n# Shuffle treatments randomly\nset.seed(123)  # For reproducibility\nrandomized_treatments &lt;- sample(treatments)\n\n# Create a data frame with randomized assignments\nrandomization_crd &lt;- data.frame(Unit = experimental_units, Treatment = randomized_treatments)\nprint(randomization_crd)\n\n   Unit Treatment\n1     1         C\n2     2         A\n3     3         B\n4     4         B\n5     5         C\n6     6         C\n7     7         A\n8     8         A\n9     9         B\n10   10         A\n\n\n\nTable 1: Randomized Assignments in CRD\n\n\n\n\n\n2. Randomized Block Design\n\n# Define blocks and treatments\nblocks &lt;- 1:4 # 4 blocks\ntreatments &lt;- c(\"A\", \"B\", \"C\") # 3 treatments\n\n# Create a data frame to store results\nrandomization_rbd &lt;- data.frame(Block = integer(), Unit = integer(), Treatment = character())\n\n# Randomize within each block\nset.seed(123)\nfor (block in blocks) {\n    # Randomize treatments\n    randomized_treatments &lt;- sample(treatments)\n\n    # Assign treatments to units within the block\n    block_data &lt;- data.frame(\n        Block = block,\n        Unit = seq_len(length(treatments)),\n        Treatment = randomized_treatments\n    )\n    randomization_rbd &lt;- rbind(randomization_rbd, block_data)\n}\n\nprint(randomization_rbd)\n\n   Block Unit Treatment\n1      1    1         C\n2      1    2         A\n3      1    3         B\n4      2    1         B\n5      2    2         A\n6      2    3         C\n7      3    1         B\n8      3    2         C\n9      3    3         A\n10     4    1         A\n11     4    2         B\n12     4    3         C\n\n\n\n\n\nExplanation in R\n\nrep(): Repeats the treatment list to match the total number of experimental units.\nsample(): Randomizes the order of the treatments.\nset.seed(): Ensures reproducibility of the randomization process.\nfor loop (RBD): Iterates through blocks, randomizing treatments for each.\n\nThese R scripts provide easy-to-follow approaches to randomization, ensuring balanced and unbiased treatment assignment. Let me know if you’d like further assistance with advanced designs!\n\n\n\nSee the help page for rep() in the Appendix for more details.\nSee the help page for sample() in the Appendix for more details.\n\n\n\nR Example: Simple Random Assignment\n\nset.seed(123)\ntreatments &lt;- rep(c(\"A\",\"B\",\"C\"), each=4)\nassignments &lt;- sample(treatments)\nassignments\n\n [1] \"A\" \"C\" \"C\" \"A\" \"B\" \"C\" \"B\" \"A\" \"C\" \"B\" \"A\" \"B\"\n\n\n\n\nActivity: Randomization Drill\nWrite down three treatments on slips of paper and randomly assign them to hypothetical units. Discuss how randomization prevents biased placement of favorable treatments.\n\n\nChallenges and Common Mistakes\n\nAssigning treatments alphabetically or in a patterned manner is not randomization.\nRandomization must be deliberate, not haphazard.\n\n\n\nExercises\n\nWhy is randomization crucial for valid statistical inference?\nWith 12 units and 3 treatments, show one method of random assignment.\nUnder randomization and the null hypothesis, show that the expected difference between treatment means is zero.\n\nKey Takeaways:\n\nRandomization removes systematic bias.\nIt underpins the validity of ANOVA and other inferential methods."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#standard-experimental-designs",
    "href": "lectures/week-01_intro-design_part2.html#standard-experimental-designs",
    "title": "Planning Experiments",
    "section": "Standard Experimental Designs",
    "text": "Standard Experimental Designs\n\nCompletely Randomized Design (CRD)\n\nNo blocking, treatments assigned entirely at random.\nSuitable when units are homogeneous.\n\n\n\nRandomized Block Design (RBD)\n\nIntroduce a blocking factor to control known variation.\nEach block receives all treatments, improving the precision of treatment comparisons.\n\n\n\nRow-Column and Latin Squares\n\nControl for two perpendicular nuisance factors simultaneously.\n\n\n\nFactorial Designs\n\nStudy multiple factors and their interactions simultaneously.\nEfficiently explore how factors work together.\n\n\n\nSplit-Plot Designs\n\nUseful when some factors are harder to change than others.\nCreates a hierarchy of experimental units (e.g., fields as main plots and subplots within fields).\n\n\n\nActivity: Design Match-Up\nGiven three scenarios (no nuisance factor, known nuisance factor, and multiple factors with a complex structure), choose the appropriate design (CRD, RBD, Factorial, Latin Square).\n\n\nChallenges and Common Mistakes\n\nUsing a CRD when a known nuisance factor should be blocked.\nIgnoring interactions in factorial experiments.\n\n\n\nExercises\n\nConceptual: Under what conditions would you choose a CRD over an RBD?\nNumerical: Compute the number of runs in a 2x4 factorial with 3 replicates.\nProof/Derivation: Show how the variance decomposition changes from CRD to RBD, highlighting the block effect.\n\nKey Takeaways:\n\nDifferent designs address different research needs.\nFactorial designs detect interactions, blocking improves precision, and split-plots handle difficult-to-change factors."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#model-specification-and-effect-types",
    "href": "lectures/week-01_intro-design_part2.html#model-specification-and-effect-types",
    "title": "Planning Experiments",
    "section": "Model Specification and Effect Types",
    "text": "Model Specification and Effect Types\n\nLinear Models and ANOVA\nExperiments often use a linear model to relate responses to treatments and blocks:\n\\[\nY = \\mu + \\text{treatment effects} + \\text{block effects} + \\text{error}\n\\] {#eq:linear-model}\nANOVA decomposes total variation into components attributable to treatments, blocks, and error, enabling inference on treatment effects.\n\n\nFixed vs. Random Effects\n\nFixed Effects: Chosen levels of interest; inferences apply only to those tested levels.\nRandom Effects: Levels are a random sample from a broader population; inferences generalize beyond observed levels.\n\n\n\nActivity: Fixed or Random?\nConsider an experiment with 3 specific fertilizer brands (likely fixed) vs. an experiment using 3 randomly chosen brands from a large market (random). Discuss how interpretation changes.\n\n\nChallenges and Common Mistakes\n\nTreating random factors as fixed or vice versa leads to incorrect conclusions.\nConfusing the meaning of random effects with randomization.\n\n\n\nExercises\n\nConceptual: Why is it important to distinguish fixed and random effects?\nNumerical: In an RBD, show how expected mean squares differ for a random block factor vs. a fixed block factor.\nProof/Derivation: Derive the expected mean squares for a two-way ANOVA with one random factor.\n\nKey Takeaways:\n\nCorrect model specification ensures valid inference.\nFixed vs. random distinction affects interpretation and generalized conclusions."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#planning-data-collection-and-pilot-experiments",
    "href": "lectures/week-01_intro-design_part2.html#planning-data-collection-and-pilot-experiments",
    "title": "Planning Experiments",
    "section": "Planning Data Collection and Pilot Experiments",
    "text": "Planning Data Collection and Pilot Experiments\n\nMeasurement and Procedure\nCareful data collection planning ensures relevance and quality:\n\nChoose appropriate measurement scales and ensure instruments are calibrated.\nTrain personnel to reduce measurement errors.\n\n\n\nPilot Experiments\nA small-scale pilot run can identify unforeseen difficulties, refine factor levels, and confirm data collection procedures before launching the main study.\n\n\nActivity: Pilot Study Discussion\nDiscuss what a pilot experiment might reveal in a crop study (e.g., unexpected soil pests) and how that influences the main experiment’s design.\n\n\nChallenges and Common Mistakes\n\nOmitting a pilot study can lead to costly mistakes in the main experiment.\nRelying on untested procedures risks invalid data collection.\n\n\n\nExercises\n\nWhy are pilot experiments beneficial?\nIf pilot data suggest variance = 4 and you want a margin of error = 1, how many samples per treatment are needed (assuming normality)?\nShow how a pilot-based variance estimate informs sample size calculations.\n\nKey Takeaways:\n\nPlanning and pilot testing prevent wasted resources.\nWell-designed procedures and preliminary runs ensure high-quality, interpretable data."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#determining-the-number-of-observations",
    "href": "lectures/week-01_intro-design_part2.html#determining-the-number-of-observations",
    "title": "Planning Experiments",
    "section": "Determining the Number of Observations",
    "text": "Determining the Number of Observations\n\nSample Size and Power\nThe number of observations affects the experiment’s ability to detect true effects:\n\nPower: Probability of detecting a true effect if it exists.\nLarger sample size generally increases power but also increases cost.\n\nPower calculations balance desired precision, variance estimates, and available resources.\n\n\nActivity: Sample Size Calculation\nGiven an estimated variance and a desired effect size, estimate the required sample size. Discuss trade-offs between resource constraints and statistical power.\n\n\nChallenges and Common Mistakes\n\nChoosing sample size arbitrarily can lead to low power or wasted resources.\nOverly large samples may be unnecessary and expensive.\n\n\n\nExercises\n\nConceptual: Explain why sample size must be justified.\nNumerical: Given effect size = 2 (SD=3) and desired power = 0.8, calculate required sample size per group.\nProof/Derivation: Derive a basic formula relating power, effect size, and sample size for a one-way ANOVA.\n\nKey Takeaways:\n\nAdequate sample size ensures meaningful, reliable conclusions.\nPower analysis guides optimal resource allocation."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#practical-example-in-r",
    "href": "lectures/week-01_intro-design_part2.html#practical-example-in-r",
    "title": "Planning Experiments",
    "section": "Practical Example in R",
    "text": "Practical Example in R\n\nExample: A Simple CRD\nCompare three battery types (A, B, C) with 4 replicates each.\n\n\nR Code\n\n\n\nset.seed(123)\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), each = 4)\nassignments &lt;- sample(treatments)\n\n# Simulate some response data\nresponse &lt;- rnorm(12, mean = 50, sd = 5)\n\n# Fit ANOVA model\nmodel &lt;- aov(response ~ factor(assignments))\nsummary(model)\n\n                    Df Sum Sq Mean Sq F value Pr(&gt;F)\nfactor(assignments)  2   14.2    7.08   0.163  0.852\nResiduals            9  391.5   43.50               \n\n\n\nTable 2: ANOVA Table\n\n\n\nInterpret the ANOVA table:\n\nIf the p-value for factor(assignments) is small, it suggests a difference among battery types.\n\n\n\nActivity: Interpret R Output\nExamine the ANOVA table. Identify the F-statistic and p-value, and discuss whether treatments differ.\n\n\nChallenges and Common Mistakes\n\nMisinterpreting p-values without context.\nIgnoring model assumptions such as normality and equal variances.\n\n\n\nExercises\n\nConceptual: Explain why randomization is essential before running ANOVA.\nNumerical: Modify the code for 2 treatments with 5 replicates and interpret results.\nProof/Derivation: Show how the F-statistic relates to the ratio of variances in ANOVA.\n\nKey Takeaways:\n\nR facilitates practical application of design concepts.\nInterpreting software output requires understanding design principles and model assumptions."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#summary-of-key-takeaways",
    "href": "lectures/week-01_intro-design_part2.html#summary-of-key-takeaways",
    "title": "Planning Experiments",
    "section": "Summary of Key Takeaways",
    "text": "Summary of Key Takeaways\n\nObjectives: Clear objectives guide the entire design and analysis.\nFactors and Levels: Treatment factors and meaningful factor levels ensure relevance.\nExperimental Units and Blocking: Correct identification of units and use of blocking increases precision.\nRandomization: Ensures unbiased estimates and valid statistical inference.\nDesigns (CRD, RBD, Factorial, etc.): Different designs solve different problems; factorial designs explore interactions, blocking controls known variation.\nFixed vs. Random Effects: Proper classification determines the scope of inferences.\nPlanning and Pilots: Thoughtful planning and preliminary trials prevent costly errors.\nSample Size and Power: Adequate sample size ensures detectable effects without wasting resources.\nR Implementation: Practical coding examples reinforce theoretical principles."
  },
  {
    "objectID": "lectures/week-01_intro-design_part2.html#definitions-of-key-terms",
    "href": "lectures/week-01_intro-design_part2.html#definitions-of-key-terms",
    "title": "Planning Experiments",
    "section": "Definitions of Key Terms",
    "text": "Definitions of Key Terms\n\nObjective: The main research question or goal of the experiment.\nNuisance Factor: A variable that affects the response but is not of primary interest.\nTreatment Factor: A variable manipulated by the experimenter.\nFactor Levels: The specific settings or categories of a factor.\nExperimental Unit: The entity to which a treatment is applied independently.\nBlock Factor: A factor used to group units into homogeneous sets.\nRandomization: Assigning treatments randomly to avoid bias.\nCRD (Completely Randomized Design): A design without blocking; treatments are assigned randomly.\nRBD (Randomized Block Design): A design that uses blocks to control known nuisance factors.\nFactorial Design: A design that includes multiple factors simultaneously.\nFixed Effects: Effects of chosen factor levels of specific interest.\nRandom Effects: Effects of factor levels considered as a random sample from a population.\nPilot Experiment: A small preliminary study to refine methods before the main experiment.\nPower: The probability of detecting a true treatment effect.\nANOVA: A statistical method for comparing means across groups."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html",
    "href": "lectures/week-02_crd-overview.html",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "",
    "text": "In mathematical statistics, well-structured experimental designs and robust analytical methods are essential for drawing valid conclusions about treatment effects. One of the simplest yet fundamental designs is the completely randomized design (CRD). Under a CRD, each experimental unit is randomly assigned to one of several treatment conditions without any blocking factors. This approach ensures that all treatments have an equal chance of being applied to each unit, thereby mitigating systematic biases and facilitating straightforward inference.\nA key tool for analyzing CRDs is the one-way analysis of variance (ANOVA). The one-way ANOVA tests whether at least two of several treatment means differ significantly. Building upon principles of linear models, least squares estimation, and classical hypothesis testing, the one-way ANOVA provides a comprehensive framework to quantify uncertainty and make decisions about treatments based on observed data.\n\n\n\nUnderstand the structure and rationale behind the completely randomized design.\nLearn the randomization procedure and the role of coding treatments.\nFormulate the one-way ANOVA model and understand its assumptions.\nExplore concepts of estimability, least squares estimation, and the distribution of estimators.\nApply the ANOVA test to decide whether treatments differ significantly.\nCalculate sample size and power, and incorporate these insights when planning experiments.\nDemonstrate procedures using R, including randomization, data input, fitting models, and producing ANOVA tables.\n\n\n\n\nThis lecture uses several R packages for data manipulation, analysis, and plotting:\n\npacman::p_load(emmeans, pwr, here)\n\n\npacman: A package management tool for installing and loading packages.\nemmeans: For estimating treatment means and testing contrasts.\npwr: For power calculations in hypothesis testing.\nhere: For managing file paths in R projects."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#objectives",
    "href": "lectures/week-02_crd-overview.html#objectives",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "",
    "text": "Understand the structure and rationale behind the completely randomized design.\nLearn the randomization procedure and the role of coding treatments.\nFormulate the one-way ANOVA model and understand its assumptions.\nExplore concepts of estimability, least squares estimation, and the distribution of estimators.\nApply the ANOVA test to decide whether treatments differ significantly.\nCalculate sample size and power, and incorporate these insights when planning experiments.\nDemonstrate procedures using R, including randomization, data input, fitting models, and producing ANOVA tables."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#r-packages",
    "href": "lectures/week-02_crd-overview.html#r-packages",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "",
    "text": "This lecture uses several R packages for data manipulation, analysis, and plotting:\n\npacman::p_load(emmeans, pwr, here)\n\n\npacman: A package management tool for installing and loading packages.\nemmeans: For estimating treatment means and testing contrasts.\npwr: For power calculations in hypothesis testing.\nhere: For managing file paths in R projects."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#defining-experimental-design",
    "href": "lectures/week-02_crd-overview.html#defining-experimental-design",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Defining Experimental Design",
    "text": "Defining Experimental Design\nAn experimental design is a rule determining how experimental units are assigned to treatments. In a completely randomized design (CRD), no blocking factors are considered; all experimental units are assigned to treatments purely at random. This simplicity allows:\n\nEqual probability of treatment assignment.\nAvoidance of experimenter bias.\nStraightforward statistical analysis.\n\nExamples include testing different battery types or comparing rotary pump speeds to measure fluid flow. When no major nuisance factors (like day-to-day variation or spatial differences) need controlling, a CRD is often appropriate."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#randomization-procedure",
    "href": "lectures/week-02_crd-overview.html#randomization-procedure",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Randomization Procedure",
    "text": "Randomization Procedure\nRandomization ensures fairness and the validity of the statistical distributions underlying standard tests. Treatments, labeled 1 through \\(v\\), are assigned to experimental units by generating random numbers and sorting them. Modern software (R) or random number tables can be used. For instance, if testing three treatment levels (A, B, C) with equal sample sizes, we can:\n\nGenerate random numbers for each unit.\nRank units by their random numbers.\nAssign treatments in order based on the desired allocation.\n\nThis procedure ensures every arrangement is equally likely, aligning with the assumptions in ANOVA and related inference."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#model-formulation",
    "href": "lectures/week-02_crd-overview.html#model-formulation",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Model Formulation",
    "text": "Model Formulation\nConsider \\(v\\) treatments with \\(n\\) observations each (for simplicity). Let \\(Y_{ij}\\) denote the response from the \\(j\\)-th unit receiving treatment \\(i\\), with \\(i=1,\\dots,v\\) and \\(j=1,\\dots,n\\). A common model is:\n\\[\nY_{ij} = \\mu + \\tau_i + \\varepsilon_{ij},\n\\] {#eq:anova-model}\nwhere:\n\n\\(\\mu\\) is the overall mean response.\n\\(\\tau_i\\) is the effect of the \\(i\\)-th treatment.\n\\(\\varepsilon_{ij}\\) are independent, normally distributed errors with mean 0 and variance \\(\\sigma^2\\).\n\nThe assumptions are:\n\nErrors \\(\\varepsilon_{ij}\\) are i.i.d. \\(N(0,\\sigma^2)\\).\nTreatment effects sum to zero: \\(\\sum_{i=1}^v \\tau_i = 0\\) for identifiability.\n\nThis linear model model underpins the ANOVA. The null hypothesis for testing no treatment differences is \\(H_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_v = 0\\).\n\n\n\nidentifiability:"
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#estimability-and-least-squares-estimation",
    "href": "lectures/week-02_crd-overview.html#estimability-and-least-squares-estimation",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Estimability and Least Squares Estimation",
    "text": "Estimability and Least Squares Estimation\nNot all model parameters (\\(\\tau_i\\)) are independently estimable. However, estimable functions, linear combinations of parameters that correspond to expected values of observed data, can be estimated uniquely.\nThe least squares estimators (LSEs) solve the normal equations obtained by minimizing the sum of squared errors (SSE):\n\\[\n\\text{SSE} = \\sum_{i=1}^v \\sum_{j=1}^n (Y_{ij} - \\hat{\\mu}_i)^2,\n\\] {#eq:sse}\nwhere \\(\\hat{\\mu}_i\\) is the estimated mean for treatment \\(i\\). The LSE for each treatment mean \\(\\mu_i = \\mu + \\tau_i\\) is simply the sample mean \\(\\bar{Y}_i = \\frac{1}{n}\\sum_{j=1}^n Y_{ij}\\).\nBecause the normal equations are linearly dependent, an additional constraint is needed (e.g., \\(\\sum \\tau_i = 0\\)). Still, any estimable function (like differences between treatment means) can be uniquely estimated."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#distribution-of-estimators",
    "href": "lectures/week-02_crd-overview.html#distribution-of-estimators",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Distribution of Estimators",
    "text": "Distribution of Estimators\nUnder the normal error assumption, each estimator \\(\\bar{Y}_i\\) is normally distributed, with:\n\\[\nE(\\bar{Y}_i) = \\mu_i, \\quad \\text{Var}(\\bar{Y}_i) = \\frac{\\sigma^2}{n}.\n\\] {#eq:estimator-variance}\nThe error variance \\(\\sigma^2\\) is estimated by the mean square error (MSE):\n\\[\n\\hat{\\sigma}^2 = \\frac{\\text{SSE}}{N - v},\n\\] {#eq:mse}\nwhere \\(N = v \\cdot n\\) is the total number of observations."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#the-anova-f-test",
    "href": "lectures/week-02_crd-overview.html#the-anova-f-test",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "The ANOVA F-Test",
    "text": "The ANOVA F-Test\nTo test \\(H_0: \\tau_1 = \\cdots = \\tau_v = 0\\) against \\(H_a:\\) at least two \\(\\tau_i\\) differ, we use the ANOVA F-statistic:\n\\[\nF = \\frac{\\text{MST}}{\\text{MSE}},\n\\] {#eq:f-statistic}\nwhere:\n\n\\(\\text{SST}\\) = treatment sum of squares measures variation among treatment means.\n\\(\\text{MSE}\\) = error mean square, \\(\\text{MSE} = \\text{SSE}/(N-v)\\).\n\\(\\text{MST} = \\text{SST}/(v-1)\\).\n\nUnder \\(H_0\\), \\(F \\sim F_{v-1, N-v}\\). If \\(F\\)-value exceeds a critical value (or the p-value is below a chosen \\(\\alpha\\)), we reject \\(H_0\\)."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#anova-table",
    "href": "lectures/week-02_crd-overview.html#anova-table",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nThe ANOVA table summarizes the decomposition of total variability:\n\n\n\n\n\n\nSource\nDF\nSS\nMS\nF-ratio\n\n\n\n\nTreatment\n\\(v-1\\)\nSST\nMST=SST/(v-1)\nMST/MSE\n\n\nError\n\\(N - v\\)\nSSE\nMSE=SSE/(N-v)\n\n\n\nTotal\n\\(N-1\\)\nSSTotal\n\n\n\n\n\n\n\nTable 1: ANOVA Table for a Completely Randomized Design\n\n\n\nThe table provides a concise summary of the ANOVA results, including degrees of freedom (DF), sum of squares (SS), mean squares (MS), and the F-ratio.\nRejecting \\(H_0\\) indicates that not all treatment means are equal."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#determining-sample-size-and-power",
    "href": "lectures/week-02_crd-overview.html#determining-sample-size-and-power",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Determining Sample Size and Power",
    "text": "Determining Sample Size and Power\nTo ensure the experiment can detect meaningful differences, consider the power analysis. Power calculations (detailed in referenced texts) help determine the number of replicates \\(n\\) per treatment to achieve a desired probability of detecting a specified difference at a given significance level \\(\\alpha\\).\nUse methods from Montgomery, Peck & Vining (2020) or Dean, Voss & Draguljić (2017), along with pilot data, to estimate \\(\\sigma^2\\) and decide on sample size. If resources are limited, one may have to adjust objectives or accept lower power."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#confidence-limits-for-variance",
    "href": "lectures/week-02_crd-overview.html#confidence-limits-for-variance",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Confidence Limits for Variance",
    "text": "Confidence Limits for Variance\nA one-sided confidence interval on \\(\\sigma^2\\) prevents underestimating required observations. The upper confidence limit is derived from the chi-squared distribution:\n\\[\n\\frac{(N-v)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{N-v}.\n\\] {#eq:variance-ci}\nSuch intervals guide planning by addressing uncertainty in the variance estimate."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#randomization-in-r",
    "href": "lectures/week-02_crd-overview.html#randomization-in-r",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Randomization in R",
    "text": "Randomization in R\nSuppose we have 3 treatments (A, B, C) and total 9 units (3 per treatment):\n\nset.seed(123) # Total units\nn &lt;- 9 # Assign treatments to units, e.g., AAABBBCCC\ntreatments &lt;- rep(c(\"A\", \"B\", \"C\"), each = 3) # Assign random numbers\nrand_nums &lt;- runif(9) # Genrates random numbers\ndata_frame &lt;- data.frame(Unit = 1:9, Treatment = treatments, RN = rand_nums)\ndata_frame_sorted &lt;- data_frame[order(data_frame$RN), ] # Sort by RN\ndata_frame_sorted\n\n\n\n\n\n  \n\n\n\n\nTable 2: Randomized Treatment Allocation Data\n\n\n\n\nThis yields a random allocation order.\n\nExplanation of the Key Functions in R\n\nset.seed(123): Sets a seed for reproducibility.\nrep(c(\"A\", \"B\", \"C\"), each = 3): Creates a balanced treatment allocation, where each = 3 assigns 3 units to each treatment.\nrunif(9): Generates 9 random numbers from a uniform distribution.\norder: Sorts the data frame based on the random numbers. The result of the function is row indices that would sort the data frame."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#fitting-a-one-way-anova-model",
    "href": "lectures/week-02_crd-overview.html#fitting-a-one-way-anova-model",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Fitting a One-Way ANOVA Model",
    "text": "Fitting a One-Way ANOVA Model\nAssume we have data from a soap experiment (soap.txt) with three soap types (regular, deodorant, moisturizing):\n\npacman::p_load(here)\n# Read data\nsoap.data &lt;- read.table(here(\"data\",\"dean2017\", \"soap.txt\"), header=TRUE)\nsoap.data$fSoap &lt;- factor(soap.data$Soap)\n\n# Fit a one-way ANOVA\nfit &lt;- aov(WtLoss ~ fSoap, data=soap.data)\nsummary(fit)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfSoap        2 16.122   8.061   104.5 5.91e-07 ***\nResiduals    9  0.695   0.077                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA summary shows if soap type affects weight loss significantly."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#estimating-means-and-contrasts",
    "href": "lectures/week-02_crd-overview.html#estimating-means-and-contrasts",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Estimating Means and Contrasts",
    "text": "Estimating Means and Contrasts\nWe can estimate treatment means (least squares means) and test contrasts using emmeans or multcomp packages. For example:\n\n# install.packages(\"emmeans\") # Run once if not installed\npacman::p_load(emmeans)\nemmeans(fit, ~ fSoap)\n\n fSoap emmean    SE df lower.CL upper.CL\n 1     -0.035 0.139  9   -0.349    0.279\n 2      2.700 0.139  9    2.386    3.014\n 3      1.992 0.139  9    1.678    2.307\n\nConfidence level used: 0.95 \n\n\nThis provides estimates and confidence intervals for each soap type’s mean weight loss."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#conclusion",
    "href": "lectures/week-02_crd-overview.html#conclusion",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "Conclusion",
    "text": "Conclusion\nA completely randomized design and its corresponding one-way ANOVA analysis form the bedrock of experimental inference when blocking is unnecessary. By carefully randomizing treatments, modeling responses, and applying least squares estimation, one can confidently infer whether treatments differ.\nWe introduced the conceptual framework and key formulae, showed how to conduct hypothesis tests and interpret ANOVA results, discussed sample size and power considerations, and provided R code for practical execution. Mastery of these fundamentals empowers researchers to design efficient experiments, maximize the utility of collected data, and confidently draw conclusions about treatment effects."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#references",
    "href": "lectures/week-02_crd-overview.html#references",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "References",
    "text": "References\n\nChristensen, R. (2018). Analysis of variance, design, and regression: Linear modeling for unbalanced data (2nd Ed.). Chapman and Hall/CRC.\nDean, A., Voss, D., & Draguljić, D. (2017). Design and analysis of experiments. Springer. https://doi.org/10.1007/978-3-319-52250-0\nKempthorne, O. (1977). Design and Analysis of Experiments. Wiley.\nMontgomery, D. C., Peck, E. A., & Vining, G. G. (2020). Introduction to linear regression analysis (5th Ed.). Wiley."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#a.1-normal-equations-and-estimability",
    "href": "lectures/week-02_crd-overview.html#a.1-normal-equations-and-estimability",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "A.1 Normal Equations and Estimability",
    "text": "A.1 Normal Equations and Estimability\nThe one-way ANOVA model can be written as:\n\\[\nY_{ij} = \\mu_i + \\varepsilon_{ij}, \\quad \\text{with } \\mu_i = \\mu + \\tau_i.\n\\]\nThe least squares approach forms normal equations from partial derivatives of SSE. Because \\(\\sum \\tau_i = 0\\), only differences in means are estimable. The space of estimable functions includes contrasts like \\(\\mu_1 - \\mu_2\\), but not individual \\(\\tau_i\\) alone."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#a.2-distribution-of-the-f-statistic",
    "href": "lectures/week-02_crd-overview.html#a.2-distribution-of-the-f-statistic",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "A.2 Distribution of the F-Statistic",
    "text": "A.2 Distribution of the F-Statistic\nUnder \\(H_0\\):\n\\[\nF = \\frac{MST}{MSE} \\sim F_{v-1, N-v}.\n\\]\nIf treatments truly differ, F follows a noncentral F-distribution, enabling power calculations and minimum detectable differences."
  },
  {
    "objectID": "lectures/week-02_crd-overview.html#a.3-power-calculations",
    "href": "lectures/week-02_crd-overview.html#a.3-power-calculations",
    "title": "Completely Randomized Design and One-Way ANOVA",
    "section": "A.3 Power Calculations",
    "text": "A.3 Power Calculations\nPower for detecting a difference \\(\\delta\\) in means involves the noncentrality parameter of the F-distribution. By selecting \\(\\alpha, \\sigma^2, \\delta,\\) and desired power, tables or software (e.g., pwr package in R) guide sample size choices.\n\n# Example using pwr package\n# install.packages(\"pwr\") # if not installed\npacman::p_load(pwr)\npwr.anova.test(k = 3, f = 0.4, sig.level = 0.05, power = 0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 21.10364\n              f = 0.4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nThis computes required sample size per treatment to achieve 80% power with a given effect size \\(f\\)."
  },
  {
    "objectID": "home_contents.html",
    "href": "home_contents.html",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "This page organizes the key course materials, including lecture notes, assignments, supplemental readings, and important resources.\n\n\nThe course syllabus contains all essential details about the course structure, policies, assignments, and exam dates.\n\n📄 Download Full Course Syllabus (PDF)\nImportant: You must read the syllabus carefully, as it contains critical information not available elsewhere.\n\n\n\n\nLecture notes and slides will be posted on the Course GitHub Page in HTML format.\n\n📚 Access Lecture Notes\n\nNote: GitHub Pages may cache content. Refresh your browser if the notes appear outdated.\n\nTo refresh, use:\n\nCtrl + Shift + R (Windows/Linux)\n\nCmd + Shift + R (Mac)\n\n\n\n\n\n\n\nThe course schedule provides a week-by-week breakdown of topics, reading assignments, and due dates.\n\n📅 View Course Schedule\n\n\n\n\nAssignments will be posted on Canvas as well as on the course website on the Assignments page. All the assignments must be submitted via Canvas. All assignments require the use of the Quarto template provided.\n\n🔗 Quarto Template\n\n\n\n\n\nSupplemental readings will be provided as needed. Public domain readings will be posted on the Contents page.\n\n🔗 Access Supplemental Readings on Canvas\n\nNote: Some readings may not be shared due to copyright restrictions.\n\n\n\nTo complete assignments and analyses, you will need access to R and related tools.\n\nR Installation: Download R\n\nRStudio IDE: Download RStudio\n\nVS Code IDE: Download VS Code\n\n\n\n\nQuarto Documentation: Learn Quarto\n\nR Tutorials: R Basics for Beginners"
  },
  {
    "objectID": "home_contents.html#course-syllabus",
    "href": "home_contents.html#course-syllabus",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "The course syllabus contains all essential details about the course structure, policies, assignments, and exam dates.\n\n📄 Download Full Course Syllabus (PDF)\nImportant: You must read the syllabus carefully, as it contains critical information not available elsewhere."
  },
  {
    "objectID": "home_contents.html#lecture-notes",
    "href": "home_contents.html#lecture-notes",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "Lecture notes and slides will be posted on the Course GitHub Page in HTML format.\n\n📚 Access Lecture Notes\n\nNote: GitHub Pages may cache content. Refresh your browser if the notes appear outdated.\n\nTo refresh, use:\n\nCtrl + Shift + R (Windows/Linux)\n\nCmd + Shift + R (Mac)"
  },
  {
    "objectID": "home_contents.html#course-schedule",
    "href": "home_contents.html#course-schedule",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "The course schedule provides a week-by-week breakdown of topics, reading assignments, and due dates.\n\n📅 View Course Schedule"
  },
  {
    "objectID": "home_contents.html#assignments",
    "href": "home_contents.html#assignments",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "Assignments will be posted on Canvas as well as on the course website on the Assignments page. All the assignments must be submitted via Canvas. All assignments require the use of the Quarto template provided.\n\n🔗 Quarto Template"
  },
  {
    "objectID": "home_contents.html#supplemental-readings",
    "href": "home_contents.html#supplemental-readings",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "Supplemental readings will be provided as needed. Public domain readings will be posted on the Contents page.\n\n🔗 Access Supplemental Readings on Canvas\n\nNote: Some readings may not be shared due to copyright restrictions."
  },
  {
    "objectID": "home_contents.html#software-tools-and-resources",
    "href": "home_contents.html#software-tools-and-resources",
    "title": "Course Content - STAT 454/545",
    "section": "",
    "text": "To complete assignments and analyses, you will need access to R and related tools.\n\nR Installation: Download R\n\nRStudio IDE: Download RStudio\n\nVS Code IDE: Download VS Code\n\n\n\n\nQuarto Documentation: Learn Quarto\n\nR Tutorials: R Basics for Beginners"
  },
  {
    "objectID": "home_contents.html#r-code-and-data-additional-references",
    "href": "home_contents.html#r-code-and-data-additional-references",
    "title": "Course Content - STAT 454/545",
    "section": "R Code and Data-Additional References",
    "text": "R Code and Data-Additional References\n\n\n\nR Code\nData File\nChapter and Problems\n\n\n\n\nstartup.r\n\nChapter 3: Sect. 3.9, starting R code for basic setup\n\n\nsoap.r\nsoap.txt\nChapter 3: Sect. 3.9.2, soap randomization study\n\n\nsoap2.r\nsoap.txt\nChapter 3: Sect. 3.9.4, follow-up soap analysis\n\n\nbattery.r\nbattery.txt\nChapter 4: Sect. 4.7, battery example for confidence intervals\n\n\nmungbean.r\nmung.bean.txt\nChapter 5: Sect. 5.9.1, mung bean experiment\n\n\nmungbean2.r\nmung.bean.txt\nChapter 5: Sect. 5.9.2, continuation of mung bean analysis\n\n\ntrout.r\ntrout.txt\nChapter 5: Sect. 5.9.3, trout multiple comparisons\n\n\nGamesHowell.r\n\nChapter 5: Sect. 5.9.3.1, related to multiple comparisons\n\n\nreactiontime.r\nreaction.time.txt\nChapter 6: Sect. 6.9, reaction time contrasts\n\n\nreactiontime2.r\nreaction.time.txt\nChapter 6: Sect. 6.9.3, further reaction time analysis\n\n\nairvelocity.r\nair.velocity.txt\nChapter 6: Sect. 6.9.4, air velocity contrasts example\n\n\ndrilladvance.r\ndrill.advance.txt\nChapter 7: Sect. 7.7.1, drill advance example\n\n\ndrilladvance2.r\ndrill.advance.txt\nChapter 7: Sect. 7.7.2, follow-up to drill advance study\n\n\nrailweld.r\nrail.weld.txt\nChapter 7: Sect. 7.7.3, rail weld strength study\n\n\nbeansoaking.r\nbean.txt\nChapter 8: Sect. 8.10, bean soaking experiment\n\n\nballoon.r\nballoon.txt\nChapter 9: Sect. 9.7, balloon experiment to illustrate analysis of covariance\n\n\ncottonspinning.r\ncotton.spinning.txt\nChapter 10: Sect. 10.10, cotton spinning example for randomized block designs\n\n\nplasma-day1.r\nplasma.txt\nChapter 11: Sect. 11.9.2–3, plasma study for block designs\n\n\ndetergent.r\ndetergent.txt\nChapter 11: Sect. 11.9.2, detergent optimization experiment\n\n\nexercisebicycle.r\nexercise.bicycle.txt\nChapter 12: Sect. 12.9, exercise bicycle example for row–column designs\n\n\nexercisebicycle2.r\nexercise.bicycle.txt\nChapter 12: Sect. 12.9.2, advanced row–column design example\n\n\ncoil.r\ncoil.txt\nChapter 13: Sect. 13.12, coil example\n\n\ndye.r\ndye.txt\nChapter 14: Sect. 14.6, dye application study\n\n\nsludge.r\nsludge.txt\nChapter 15: Sect. 15.10.1, sludge experiment\n\n\ninclinometer.r\ninclinometer.product.txt\nChapter 15: Sect. 15.10.2, inclinometer product experiment\n\n\ncopper.r\ncopper.txt\nChapter 16: Sect. 16.8.1, acid copper pattern plating study\n\n\nPAH.r\npah.txt\nChapter 16: Sect. 16.8.2, PAH recovery example\n\n\nRSMdesigns.r\n\nChapter 16: Sect. 16.8.3, generating response surface designs\n\n\ncleanwool.r\nclean.wool.txt\nChapter 17: Sect. 17.11.1, clean wool study\n\n\ntempr.r\ntemperature.txt\nChapter 17: Sect. 17.11.2, temperature effects study\n\n\nicecream.r\nice.cream.txt\nChapter 17: Sect. 17.11.3, ice cream study\n\n\nvoltage.r\nvoltage.txt\nChapter 18: Sect. 18.6.2, voltage nested effects\n\n\nvoltage2.r\nvoltage.txt\nChapter 18: Sect. 18.6.3, further analysis of voltage nested effects\n\n\noats.r\noats.txt\nChapter 19: Sect. 19.9.1, oats experiment\n\n\nUAV.r\nuav.txt\nChapter 19: Sect. 19.9.2, unmanned aerial vehicle study\n\n\nUAV3.r\nuav3.txt\nChapter 19: Sect. 19.9.3, UAV switch study\n\n\noats2.r\noats.txt\nChapter 19: Sect. 19.9.4, follow-up oats analysis\n\n\nMCFS71.r\nMCFS71.txt\nChapter 19: Sect. 19.9.5, mobile computing field study\n\n\nmaximinLHD.r\n\nChapter 20: Sect. 20.7.1, finding approximate maximin Latin hypercube designs\n\n\nneuron.r\nneuron.txt\nChapter 20: Sect. 20.7.2, neuron experiment"
  }
]